{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "from ssast_model import ASTModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## folding and umfolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      "shape:  torch.Size([1, 1, 4, 14])\n",
      "tensor([[[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.],\n",
      "          [15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.],\n",
      "          [29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.],\n",
      "          [43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56.]]]])\n",
      "\n",
      "Input tensor unfolded:\n",
      "shape:  torch.Size([1, 8, 7])\n",
      "tensor([[[ 1.,  3.,  5.,  7.,  9., 11., 13.],\n",
      "         [ 2.,  4.,  6.,  8., 10., 12., 14.],\n",
      "         [15., 17., 19., 21., 23., 25., 27.],\n",
      "         [16., 18., 20., 22., 24., 26., 28.],\n",
      "         [29., 31., 33., 35., 37., 39., 41.],\n",
      "         [30., 32., 34., 36., 38., 40., 42.],\n",
      "         [43., 45., 47., 49., 51., 53., 55.],\n",
      "         [44., 46., 48., 50., 52., 54., 56.]]])\n",
      "\n",
      "Input tensor unfolded (transposed):\n",
      "shape:  torch.Size([1, 7, 8])\n",
      "tensor([[[ 1.,  2., 15., 16., 29., 30., 43., 44.],\n",
      "         [ 3.,  4., 17., 18., 31., 32., 45., 46.],\n",
      "         [ 5.,  6., 19., 20., 33., 34., 47., 48.],\n",
      "         [ 7.,  8., 21., 22., 35., 36., 49., 50.],\n",
      "         [ 9., 10., 23., 24., 37., 38., 51., 52.],\n",
      "         [11., 12., 25., 26., 39., 40., 53., 54.],\n",
      "         [13., 14., 27., 28., 41., 42., 55., 56.]]])\n",
      "\n",
      "Input tensor unfolded (transposed back):\n",
      "shape:  torch.Size([1, 8, 7])\n",
      "tensor([[[ 1.,  3.,  5.,  7.,  9., 11., 13.],\n",
      "         [ 2.,  4.,  6.,  8., 10., 12., 14.],\n",
      "         [15., 17., 19., 21., 23., 25., 27.],\n",
      "         [16., 18., 20., 22., 24., 26., 28.],\n",
      "         [29., 31., 33., 35., 37., 39., 41.],\n",
      "         [30., 32., 34., 36., 38., 40., 42.],\n",
      "         [43., 45., 47., 49., 51., 53., 55.],\n",
      "         [44., 46., 48., 50., 52., 54., 56.]]])\n",
      "\n",
      "Output tensor folded:\n",
      "shape:  torch.Size([1, 1, 4, 14])\n",
      "tensor([[[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.],\n",
      "          [15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.],\n",
      "          [29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.],\n",
      "          [43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56.]]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.Unfold(kernel_size=(4, 2), stride=(4, 2)) corresponds to torch.nn.Unfold(kernel_size=(128, 2), stride=(128, 2)) \n",
    "# input shape:  torch.Size([1, 1, 4, 14]) corresponds to torch.Size([1, 1, 128, 998])\n",
    "# unfolded input shape (after transposing dimensions 1 and 2):  torch.Size([1, 7, 8]) corresponds to torch.Size([1, 499, 256])\n",
    "\n",
    "unfold = torch.nn.Unfold(kernel_size=(4, 2), stride=(4, 2))\n",
    "\n",
    "# inverse operation of Unfold\n",
    "fold = torch.nn.Fold(output_size=(4, 14), kernel_size=(4, 2), stride=(4, 2)) # corresponds to torch.nn.Fold(output_size=(128, 998), kernel_size=(128, 2), stride=(128, 2))\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor([[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "                                [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
    "                                [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42],\n",
    "                                [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]]]], dtype=torch.float32)\n",
    "\n",
    "print(\"Input tensor:\")\n",
    "print(\"shape: \", input_tensor.shape)\n",
    "print(input_tensor)\n",
    "\n",
    "# Apply the Unfold operation\n",
    "input = unfold(input_tensor)\n",
    "\n",
    "print(\"\\nInput tensor unfolded:\")\n",
    "print(\"shape: \", input.shape)\n",
    "print(input)\n",
    "\n",
    "# Transpose the dimensions 1 and 2\n",
    "input = input.transpose(1, 2)\n",
    "\n",
    "print(\"\\nInput tensor unfolded (transposed):\")\n",
    "print(\"shape: \", input.shape)\n",
    "print(input)\n",
    "\n",
    "# Transpose back to original shape\n",
    "input = input.transpose(1, 2)\n",
    "\n",
    "print(\"\\nInput tensor unfolded (transposed back):\")\n",
    "print(\"shape: \", input.shape)\n",
    "print(input)\n",
    "\n",
    "# Apply the Fold operation\n",
    "output_tensor = fold(input)\n",
    "print(\"\\nOutput tensor folded:\")\n",
    "print(\"shape: \", output_tensor.shape)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of last sample: 9.9999375\n",
      "Number of samples: 160000\n",
      "Sample rate: 16000\n",
      "Duration of audio: 10.0\n",
      "Shape of audio signal: (160000,)\n",
      "Type of audio signal: <class 'numpy.ndarray'>\n",
      "Data type of audio signal: float64\n",
      "Type of audio tensor: <class 'torch.Tensor'>\n",
      "Data type of audio tensor: torch.float64\n",
      "Shape of audio tensor: torch.Size([160000])\n",
      "Data type of audio tensor: torch.float32\n",
      "Shape of audio tensor: torch.Size([1, 160000])\n",
      "Shape of fbank features: torch.Size([998, 128])\n",
      "Shape of fbank features: torch.Size([1, 998, 128])\n",
      "Shape of dublicated fbank features: torch.Size([2, 998, 128])\n"
     ]
    }
   ],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "test_input = fbank_features\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-3.6925695\n",
    "dataset_std=4.020388\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {test_input.shape}\")\n",
    "\n",
    "# duplicate input tensor to get a batch of 2\n",
    "test_input = torch.cat((test_input, test_input), 0)\n",
    "print(f\"Shape of dublicated fbank features: {test_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretraining patch split stride: frequency=128, time=2\n",
      "pretraining patch shape: frequency=128, time=2\n",
      "pretraining patch array dimension: frequency=1, time=499\n",
      "pretraining number of patches=499\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014/models/audio_model.120.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "print(next(model.parameters()).device)  # Should print 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now load a SSL pretrained models from /home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014/models/audio_model.73.pth\n",
      "pretraining patch split stride: frequency=128, time=2\n",
      "pretraining patch shape: frequency=128, time=2\n",
      "pretraining patch array dimension: frequency=1, time=499\n",
      "pretraining number of patches=499\n",
      "fine-tuning patch split stride: frequncey=128, time=2\n",
      "fine-tuning number of patches=499\n",
      "v.cls_token torch.Size([1, 1, 768])\n",
      "v.pos_embed torch.Size([1, 501, 768])\n",
      "v.dist_token torch.Size([1, 1, 768])\n",
      "v.patch_embed.proj.weight torch.Size([768, 1, 128, 2])\n",
      "v.patch_embed.proj.bias torch.Size([768])\n",
      "v.blocks.0.norm1.weight torch.Size([768])\n",
      "v.blocks.0.norm1.bias torch.Size([768])\n",
      "v.blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.0.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.0.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.0.attn.proj.bias torch.Size([768])\n",
      "v.blocks.0.norm2.weight torch.Size([768])\n",
      "v.blocks.0.norm2.bias torch.Size([768])\n",
      "v.blocks.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.0.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.0.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.1.norm1.weight torch.Size([768])\n",
      "v.blocks.1.norm1.bias torch.Size([768])\n",
      "v.blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.1.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.1.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.1.attn.proj.bias torch.Size([768])\n",
      "v.blocks.1.norm2.weight torch.Size([768])\n",
      "v.blocks.1.norm2.bias torch.Size([768])\n",
      "v.blocks.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.1.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.1.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.2.norm1.weight torch.Size([768])\n",
      "v.blocks.2.norm1.bias torch.Size([768])\n",
      "v.blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.2.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.2.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.2.attn.proj.bias torch.Size([768])\n",
      "v.blocks.2.norm2.weight torch.Size([768])\n",
      "v.blocks.2.norm2.bias torch.Size([768])\n",
      "v.blocks.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.2.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.2.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.3.norm1.weight torch.Size([768])\n",
      "v.blocks.3.norm1.bias torch.Size([768])\n",
      "v.blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.3.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.3.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.3.attn.proj.bias torch.Size([768])\n",
      "v.blocks.3.norm2.weight torch.Size([768])\n",
      "v.blocks.3.norm2.bias torch.Size([768])\n",
      "v.blocks.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.3.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.3.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.4.norm1.weight torch.Size([768])\n",
      "v.blocks.4.norm1.bias torch.Size([768])\n",
      "v.blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.4.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.4.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.4.attn.proj.bias torch.Size([768])\n",
      "v.blocks.4.norm2.weight torch.Size([768])\n",
      "v.blocks.4.norm2.bias torch.Size([768])\n",
      "v.blocks.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.4.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.4.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.5.norm1.weight torch.Size([768])\n",
      "v.blocks.5.norm1.bias torch.Size([768])\n",
      "v.blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.5.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.5.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.5.attn.proj.bias torch.Size([768])\n",
      "v.blocks.5.norm2.weight torch.Size([768])\n",
      "v.blocks.5.norm2.bias torch.Size([768])\n",
      "v.blocks.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.5.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.5.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.6.norm1.weight torch.Size([768])\n",
      "v.blocks.6.norm1.bias torch.Size([768])\n",
      "v.blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.6.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.6.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.6.attn.proj.bias torch.Size([768])\n",
      "v.blocks.6.norm2.weight torch.Size([768])\n",
      "v.blocks.6.norm2.bias torch.Size([768])\n",
      "v.blocks.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.6.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.6.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.7.norm1.weight torch.Size([768])\n",
      "v.blocks.7.norm1.bias torch.Size([768])\n",
      "v.blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.7.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.7.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.7.attn.proj.bias torch.Size([768])\n",
      "v.blocks.7.norm2.weight torch.Size([768])\n",
      "v.blocks.7.norm2.bias torch.Size([768])\n",
      "v.blocks.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.7.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.7.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.8.norm1.weight torch.Size([768])\n",
      "v.blocks.8.norm1.bias torch.Size([768])\n",
      "v.blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.8.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.8.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.8.attn.proj.bias torch.Size([768])\n",
      "v.blocks.8.norm2.weight torch.Size([768])\n",
      "v.blocks.8.norm2.bias torch.Size([768])\n",
      "v.blocks.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.8.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.8.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.9.norm1.weight torch.Size([768])\n",
      "v.blocks.9.norm1.bias torch.Size([768])\n",
      "v.blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.9.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.9.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.9.attn.proj.bias torch.Size([768])\n",
      "v.blocks.9.norm2.weight torch.Size([768])\n",
      "v.blocks.9.norm2.bias torch.Size([768])\n",
      "v.blocks.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.9.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.9.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.10.norm1.weight torch.Size([768])\n",
      "v.blocks.10.norm1.bias torch.Size([768])\n",
      "v.blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.10.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.10.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.10.attn.proj.bias torch.Size([768])\n",
      "v.blocks.10.norm2.weight torch.Size([768])\n",
      "v.blocks.10.norm2.bias torch.Size([768])\n",
      "v.blocks.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.10.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.10.mlp.fc2.bias torch.Size([768])\n",
      "v.blocks.11.norm1.weight torch.Size([768])\n",
      "v.blocks.11.norm1.bias torch.Size([768])\n",
      "v.blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "v.blocks.11.attn.qkv.bias torch.Size([2304])\n",
      "v.blocks.11.attn.proj.weight torch.Size([768, 768])\n",
      "v.blocks.11.attn.proj.bias torch.Size([768])\n",
      "v.blocks.11.norm2.weight torch.Size([768])\n",
      "v.blocks.11.norm2.bias torch.Size([768])\n",
      "v.blocks.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "v.blocks.11.mlp.fc1.bias torch.Size([3072])\n",
      "v.blocks.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "v.blocks.11.mlp.fc2.bias torch.Size([768])\n",
      "v.norm.weight torch.Size([768])\n",
      "v.norm.bias torch.Size([768])\n",
      "v.head.weight torch.Size([1000, 768])\n",
      "v.head.bias torch.Size([1000])\n",
      "v.head_dist.weight torch.Size([1000, 768])\n",
      "v.head_dist.bias torch.Size([1000])\n",
      "mlp_head.0.weight torch.Size([768])\n",
      "mlp_head.0.bias torch.Size([768])\n",
      "mlp_head.1.weight torch.Size([527, 768])\n",
      "mlp_head.1.bias torch.Size([527])\n"
     ]
    }
   ],
   "source": [
    "model50 = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=False, load_pretrained_mdl_path='/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014/models/audio_model.73.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finetuningavgtok()) function\n",
    "with torch.no_grad():\n",
    "    output = model50(test_input, task='ft_avgtok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches:  499 mask_patch:  400\n",
      "mask_index shape:  torch.Size([400])\n",
      "num_patches:  499 mask_patch:  400\n",
      "mask_index shape:  torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "# test mpg() function\n",
    "with torch.no_grad():\n",
    "    output = model(test_input, task='pretrain_mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 998, 128])\n",
      "torch.Size([])\n",
      "tensor(0.9921)\n",
      "tensor(-1.5235)\n",
      "tensor(0.0504)\n",
      "tensor(0.0504)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba500bb850>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare input and output\n",
    "print(test_input.shape)\n",
    "print(output.shape)\n",
    "print(torch.max(test_input))\n",
    "print(torch.min(test_input))\n",
    "print(torch.max(output))\n",
    "print(torch.min(output))\n",
    "# plot output\n",
    "plt.plot(output.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
