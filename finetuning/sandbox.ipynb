{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "from ssast_model import ASTModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## folding and umfolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Unfold(kernel_size=(4, 2), stride=(4, 2)) corresponds to torch.nn.Unfold(kernel_size=(128, 2), stride=(128, 2)) \n",
    "# input shape:  torch.Size([1, 1, 4, 14]) corresponds to torch.Size([1, 1, 128, 998])\n",
    "# unfolded input shape (after transposing dimensions 1 and 2):  torch.Size([1, 7, 8]) corresponds to torch.Size([1, 499, 256])\n",
    "\n",
    "unfold = torch.nn.Unfold(kernel_size=(4, 2), stride=(4, 2))\n",
    "\n",
    "# inverse operation of Unfold\n",
    "fold = torch.nn.Fold(output_size=(4, 14), kernel_size=(4, 2), stride=(4, 2)) # corresponds to torch.nn.Fold(output_size=(128, 998), kernel_size=(128, 2), stride=(128, 2))\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor([[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "                                [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
    "                                [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42],\n",
    "                                [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]]]], dtype=torch.float32)\n",
    "\n",
    "print(\"Input tensor:\")\n",
    "print(\"shape: \", input_tensor.shape)\n",
    "print(input_tensor)\n",
    "\n",
    "# Apply the Unfold operation\n",
    "input = unfold(input_tensor)\n",
    "\n",
    "print(\"\\nInput tensor unfolded:\")\n",
    "print(\"shape: \", input.shape)\n",
    "print(input)\n",
    "\n",
    "# Transpose the dimensions 1 and 2\n",
    "input = input.transpose(1, 2)\n",
    "\n",
    "print(\"\\nInput tensor unfolded (transposed):\")\n",
    "print(\"shape: \", input.shape)\n",
    "print(input)\n",
    "\n",
    "# Transpose back to original shape\n",
    "input = input.transpose(1, 2)\n",
    "\n",
    "print(\"\\nInput tensor unfolded (transposed back):\")\n",
    "print(\"shape: \", input.shape)\n",
    "print(input)\n",
    "\n",
    "# Apply the Fold operation\n",
    "output_tensor = fold(input)\n",
    "print(\"\\nOutput tensor folded:\")\n",
    "print(\"shape: \", output_tensor.shape)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "test_input = fbank_features\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-3.6925695\n",
    "dataset_std=4.020388\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {test_input.shape}\")\n",
    "\n",
    "# duplicate input tensor to get a batch of 2\n",
    "test_input = torch.cat((test_input, test_input), 0)\n",
    "print(f\"Shape of dublicated fbank features: {test_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014/models/audio_model.120.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "print(next(model.parameters()).device)  # Should print 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model50 = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=False, load_pretrained_mdl_path='/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014/models/audio_model.73.pth', label_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finetuningavgtok()) function\n",
    "with torch.no_grad():\n",
    "    output = model50(test_input, task='ft_avgtok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mpg() function\n",
    "with torch.no_grad():\n",
    "    output = model(test_input, task='pretrain_mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input and output\n",
    "print(test_input.shape)\n",
    "print(output.shape)\n",
    "print(torch.max(test_input))\n",
    "print(torch.min(test_input))\n",
    "print(torch.max(output))\n",
    "print(torch.min(output))\n",
    "# plot output\n",
    "plt.plot(output.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate 5 unique numbers in the range [10, 20]\n",
    "indices = np.random.choice(np.arange(10, 21), 5, replace=False)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
