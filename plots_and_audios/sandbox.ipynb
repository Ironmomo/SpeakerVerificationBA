{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "from ssast_model import ASTModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchaudio\n",
    "import pickle\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming audio_signal and sample_rate are defined previously\n",
    "mel_librosa = librosa.feature.melspectrogram(\n",
    "    y=audio_signal,  # specify the audio data with keyword argument\n",
    "    sr=sample_rate,\n",
    "    n_mels=128,\n",
    "    hop_length=160,  # 10 ms = 160 * 1/16000\n",
    "    n_fft=400,       # 25 ms = 400 * 1/16000\n",
    "    center=False,\n",
    "    htk=True,\n",
    "    fmin=0.0,\n",
    "    fmax=None        # default Nyquist frequency\n",
    ")\n",
    "\n",
    "# Print shape, minimum, and maximum value of the mel spectrogram\n",
    "print(f\"Shape of mel spectrogram computed by librosa: {mel_librosa.shape}\")\n",
    "print(f\"Minimum value of mel spectrogram computed by librosa: {mel_librosa.min()}\")\n",
    "print(f\"Maximum value of mel spectrogram computed by librosa: {mel_librosa.max()}\")\n",
    "\n",
    "# Parameters for inversion to closely match feature extraction settings\n",
    "sr = sample_rate\n",
    "n_fft = 400\n",
    "hop_length = int(0.01 * sr)  # 10 ms\n",
    "win_length = int(0.025 * sr)  # 25 ms\n",
    "window = 'hanning'\n",
    "\n",
    "# Reconstruct the audio without explicitly specifying the length\n",
    "y = librosa.feature.inverse.mel_to_audio(mel_librosa, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "                                         window=window, n_iter=32, htk=True)\n",
    "\n",
    "# Check the length of the reconstructed audio\n",
    "print(\"\\nLength of reconstructed audio (samples) using librosa:\", len(y))\n",
    "print(\"Duration of reconstructed audio (seconds) using librosa:\", len(y) / sr)\n",
    "\n",
    "# save the audio signal\n",
    "#sf.write('reconstructed_audio_librosa.wav', y, sr)\n",
    "\n",
    "\n",
    "# Convert tensor fbank_features back to numpy array and transpose it\n",
    "M = fbank_features.cpu().numpy().T\n",
    "print(\"\\n\\nshape of mel spectrogram computed by torchaudio:\", M.shape)\n",
    "print(\"min of mel spectrogram computed by torchaudio:\", M.min())\n",
    "print(\"max of mel spectrogram computed by torchaudio:\", M.max())\n",
    "\n",
    "\n",
    "# Convert tensor fbank_features back to numpy array, transpose, and convert from dB to power\n",
    "M = fbank_features.cpu().numpy().T\n",
    "print(\"min and max of mel spectrogram computed by torchaudio (dB):\", M.min(), M.max())\n",
    "#M_linear = np.exp(M)/np.exp(1)\n",
    "M_linear = np.exp(M)\n",
    "print(\"min and max of mel spectrogram computed by torchaudio (power):\", M_linear.min(), M_linear.max())\n",
    "\n",
    "print(\"shape of mel spectrogram computed by torchaudio (adjusted to power):\", M_linear.shape)\n",
    "print(\"min of mel spectrogram computed by torchaudio (adjusted to power):\", M_linear.min())\n",
    "print(\"max of mel spectrogram computed by torchaudio (adjusted to power):\", M_linear.max())\n",
    "\n",
    "M = M_linear\n",
    "\n",
    "\n",
    "# Parameters for inversion to closely match feature extraction settings\n",
    "sr = sample_rate\n",
    "n_fft = 400\n",
    "hop_length = int(0.01 * sr)  # 10 ms\n",
    "win_length = int(0.025 * sr)  # 25 ms\n",
    "window = 'hanning'\n",
    "\n",
    "# Reconstruct the audio without explicitly specifying the length\n",
    "y = librosa.feature.inverse.mel_to_audio(M, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "                                         window=window, n_iter=50)\n",
    "\n",
    "# Check the length of the reconstructed audio\n",
    "print(\"\\nLength of reconstructed audio (samples):\", len(y))\n",
    "print(\"Duration of reconstructed audio (seconds):\", len(y) / sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrograms in db\n",
    "M_librosa_db = librosa.power_to_db(mel_librosa)\n",
    "print(\"min and max of mel spectrogram computed by librosa (in dB):\", M_librosa_db.min(), M_librosa_db.max())\n",
    "M_torchaudio_db = librosa.power_to_db(M)\n",
    "print(\"min and max of mel spectrogram computed by torchaudio (in dB):\", M_torchaudio_db.min(), M_torchaudio_db.max())\n",
    "\n",
    "print(\"shape of mel spectrogram computed by librosa (in dB):\", M_librosa_db.shape)\n",
    "print(\"shape of mel spectrogram computed by torchaudio (in dB):\", M_torchaudio_db.shape)\n",
    "\n",
    "# type of the spectrograms\n",
    "print(\"type of mel spectrogram computed by librosa (in dB):\", type(M_librosa_db))\n",
    "print(\"type of mel spectrogram computed by torchaudio (in dB):\", type(M_torchaudio_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mel spectrogram computed by librosa on dB scale\n",
    "plt.figure(figsize=(12, 1.6))\n",
    "plt.title('Mel Spectrogram (librosa)')\n",
    "plt.imshow(M_librosa_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "\n",
    "# plot the mel spectrogram computed by torchaudio on dB scale\n",
    "plt.figure(figsize=(12, 1.6))\n",
    "plt.title('Mel Spectrogram (torchaudio)')\n",
    "plt.imshow(M_torchaudio_db, aspect='auto', origin='lower', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compare the specta at the 350th frame\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(M_librosa_db[:, 350], label='librosa')\n",
    "plt.plot(M_torchaudio_db[:, 350], label='torchaudio')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Mel Spectra at Frame 350')\n",
    "plt.xlabel('Mel Bin')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to map the torchaudio spectrogram to the same scale as the librosa spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_spectrogram(librosa_spectrogram, torchaudio_spectrogram_to_fit, torchaudio_spectrogram_to_transform):\n",
    "    def fit_linear_transform(M1, M2):\n",
    "        b = M1 - M2\n",
    "        f_bins, t_frames = M1.shape\n",
    "        f = np.repeat(np.arange(f_bins), t_frames)\n",
    "        ones = np.ones(f_bins * t_frames)\n",
    "        A = np.column_stack((f, ones))\n",
    "        b = b.flatten()\n",
    "        A_transpose = A.T\n",
    "        A_transpose_A = np.dot(A_transpose, A)\n",
    "        A_transpose_b = np.dot(A_transpose, b)\n",
    "        x = np.linalg.solve(A_transpose_A, A_transpose_b)\n",
    "        return x[0], x[1]  # m, q\n",
    "\n",
    "    # Calculate the transformation parameters m and q using the first torchaudio and librosa spectrograms\n",
    "    m, q = fit_linear_transform(np.array(librosa_spectrogram), np.array(torchaudio_spectrogram_to_fit))\n",
    "\n",
    "    # Apply the linear transformation to each frequency bin across all time frames in the third torchaudio spectrogram\n",
    "    frequency_indices = np.arange(torchaudio_spectrogram_to_transform.shape[0]).reshape(-1, 1)\n",
    "    adjustments = m * frequency_indices + q  # Calculate adjustments for each frequency bin\n",
    "    \n",
    "    # Extend the adjustments across all time frames by broadcasting\n",
    "    adjustments = np.tile(adjustments, (1, torchaudio_spectrogram_to_transform.shape[1]))\n",
    "    \n",
    "    # Add the adjustments to the original torchaudio spectrogram\n",
    "    transformed_spectrogram = torchaudio_spectrogram_to_transform + adjustments\n",
    "    \n",
    "    return transformed_spectrogram\n",
    "\n",
    "# test the function\n",
    "M_torchaudio_db_transformed = transform_spectrogram(M_librosa_db, M_torchaudio_db, M_torchaudio_db)\n",
    "\n",
    "# plot the whole spectrograms of librosa, torchaudio, and adjusted torchaudio\n",
    "# plot the whole spectrograms of librosa, torchaudio, and adjusted torchaudio\n",
    "plt.figure(figsize=(12, 1.6))\n",
    "plt.title('Mel Spectrogram (librosa)')\n",
    "plt.imshow(M_librosa_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "\n",
    "plt.figure(figsize=(12, 1.6))\n",
    "plt.title('Mel Spectrogram (torchaudio)')\n",
    "plt.imshow(M_torchaudio_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "\n",
    "plt.figure(figsize=(12, 1.6))\n",
    "plt.title('Mel Spectrogram (torchaudio adjusted)')\n",
    "plt.imshow(M_torchaudio_db_transformed, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the spectra at a specific frame\n",
    "frame_index = 250\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(M_librosa_db[:, frame_index], label='librosa')\n",
    "plt.plot(M_torchaudio_db[:, frame_index], label='torchaudio')\n",
    "plt.plot(M_torchaudio_db_transformed[:, frame_index], label='torchaudio adjusted')\n",
    "plt.legend()\n",
    "plt.title(f'Comparison of Mel Spectra at Frame {frame_index}')\n",
    "plt.xlabel('Mel Bin')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()\n",
    "\n",
    "# calculate mean of original torchaudio and adjusted torchaudio and librosa\n",
    "print(\"Mean of mel spectrogram computed by librosa (in dB):\", M_librosa_db.mean())\n",
    "print(\"Mean of mel spectrogram computed by torchaudio (in dB):\", M_torchaudio_db.mean())\n",
    "print(\"Mean of mel spectrogram computed by torchaudio (adjusted) (in dB):\", M_torchaudio_db_transformed.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resynthesize to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that resynthesizes audio from a mel spectrogram and takes into account the difference in the mel spectrogram between two different libraries\n",
    "def adjust_and_synthesize_audio(librosa_audiosignal, torchaudio_spectrogram_to_fit, torchaudio_spectrogram_to_adjust):\n",
    "    def fit_linear_transform(M1, M2):\n",
    "        b = M1 - M2\n",
    "        f_bins, t_frames = M1.shape\n",
    "        f = np.repeat(np.arange(f_bins), t_frames)\n",
    "        ones = np.ones(f_bins * t_frames)\n",
    "        A = np.column_stack((f, ones))\n",
    "        b = b.flatten()\n",
    "        A_transpose = A.T\n",
    "        A_transpose_A = np.dot(A_transpose, A)\n",
    "        A_transpose_b = np.dot(A_transpose, b)\n",
    "        x = np.linalg.solve(A_transpose_A, A_transpose_b)\n",
    "        return x[0], x[1]  # m, q\n",
    "    \n",
    "    def mel_2_audio(M):\n",
    "        sr = 16000\n",
    "        n_fft = 400\n",
    "        hop_length = int(0.01 * sr)  # 10 ms\n",
    "        win_length = int(0.025 * sr)  # 25 ms (= n_fft) \n",
    "        window = 'hanning'\n",
    "        linear_M = np.exp(M)/np.exp(1)\n",
    "        audio = librosa.feature.inverse.mel_to_audio(linear_M, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, n_iter=50, htk=True) \n",
    "        return audio\n",
    "    \n",
    "    librosa_spectrogram_linear = librosa.feature.melspectrogram(\n",
    "        y=librosa_audiosignal,\n",
    "        sr=sample_rate,\n",
    "        n_mels=128,\n",
    "        hop_length=160,  # 10 ms = 160 * 1/16000\n",
    "        n_fft=400,       # 25 ms = 400 * 1/16000\n",
    "        center=False,\n",
    "        htk=True,\n",
    "        fmin=0.0,\n",
    "        fmax=None        # default Nyquist frequency\n",
    "    )\n",
    "\n",
    "    # convert the librosa spectrogram to dB\n",
    "    librosa_spectrogram = librosa.power_to_db(librosa_spectrogram_linear)\n",
    "    \n",
    "    # Calculate the transformation parameters m and q\n",
    "    m, q = fit_linear_transform(np.array(librosa_spectrogram), np.array(torchaudio_spectrogram_to_fit))\n",
    "\n",
    "    # Apply the linear transformation to each frequency bin across all time frames in the torchaudio spectrogram\n",
    "    frequency_indices = np.arange(torchaudio_spectrogram_to_adjust.shape[0]).reshape(-1, 1)\n",
    "    adjustments = m * frequency_indices + q  # Calculate adjustments for each frequency bin\n",
    "    adjustments = np.tile(adjustments, (1, torchaudio_spectrogram_to_adjust.shape[1]))\n",
    "    adjusted_spectrogram = torchaudio_spectrogram_to_adjust + adjustments\n",
    "\n",
    "    # resynthesize the audio\n",
    "    audio = mel_2_audio(adjusted_spectrogram)\n",
    "\n",
    "    return audio, adjusted_spectrogram\n",
    "\n",
    "# test the function\n",
    "audio_adjusted, M_torchaudio_db_transformed = adjust_and_synthesize_audio(audio_signal, M_torchaudio_db, M_torchaudio_db)\n",
    "\n",
    "# save the adjusted audio\n",
    "#sf.write('audio_adjusted.wav', audio_adjusted, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def adjust_and_synthesize_audio(audiosignal, torchaudio_spectrogram_to_fit, torchaudio_spectrogram_to_adjust, sample_rate):\n",
    "    \n",
    "    def fit_polynomial_transform(M1, M2, order=5):\n",
    "        b = M1 - M2\n",
    "        f_bins, t_frames = M1.shape\n",
    "        f = np.repeat(np.arange(f_bins), t_frames)\n",
    "        # Create polynomial features up to the given order\n",
    "        A = np.column_stack([f**i for i in range(order + 1)])\n",
    "        b = b.flatten()\n",
    "        A_transpose = A.T\n",
    "        A_transpose_A = np.dot(A_transpose, A)\n",
    "        A_transpose_b = np.dot(A_transpose, b)\n",
    "        x = np.linalg.solve(A_transpose_A, A_transpose_b)\n",
    "        return x  # coefficients of the polynomial\n",
    "\n",
    "    def mel_2_audio(M):\n",
    "        sr = sample_rate\n",
    "        n_fft = 400\n",
    "        hop_length = int(0.01 * sr)  # 10 ms\n",
    "        win_length = int(0.025 * sr)  # 25 ms (= n_fft)\n",
    "        window = 'hanning'\n",
    "        linear_M = librosa.db_to_power(M)\n",
    "        audio = librosa.feature.inverse.mel_to_audio(linear_M, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, n_iter=50, htk=True)\n",
    "        return audio\n",
    "\n",
    "    librosa_spectrogram_linear = librosa.feature.melspectrogram(\n",
    "        y=audiosignal,\n",
    "        sr=sample_rate,\n",
    "        n_mels=128,\n",
    "        hop_length=160,  # 10 ms = 160 * 1/16000\n",
    "        n_fft=400,       # 25 ms = 400 * 1/16000\n",
    "        center=False,\n",
    "        htk=True,\n",
    "        fmin=0.0,\n",
    "        fmax=sample_rate / 2  # default Nyquist frequency\n",
    "    )\n",
    "\n",
    "    # torchaudio_spectrogram_to_fit_linear = np.exp(torchaudio_spectrogram_to_fit)\n",
    "    # torchaudio_spectrogram_to_adjust_linear = np.exp(torchaudio_spectrogram_to_adjust)\n",
    "\n",
    "    torchaudio_spectrogram_to_fit_db = torchaudio_spectrogram_to_fit\n",
    "    torchaudio_spectrogram_to_adjust_db = torchaudio_spectrogram_to_fit\n",
    "    librosa_spectrogram_linear_db = librosa.power_to_db(librosa_spectrogram_linear)\n",
    "\n",
    "    # Calculate the polynomial transformation parameters\n",
    "    coefficients = fit_polynomial_transform(np.array(librosa_spectrogram_linear_db), np.array(torchaudio_spectrogram_to_fit_db))\n",
    "\n",
    "    # Apply the polynomial transformation to each frequency bin across all time frames\n",
    "    frequency_indices = np.arange(torchaudio_spectrogram_to_adjust_db.shape[0]).reshape(-1, 1)\n",
    "    adjustments = sum([coefficients[i] * frequency_indices**i for i in range(len(coefficients))])\n",
    "    adjustments = np.tile(adjustments, (1, torchaudio_spectrogram_to_adjust_db.shape[1]))\n",
    "    adjusted_spectrogram_db = torchaudio_spectrogram_to_adjust_db + adjustments\n",
    "\n",
    "    # Resynthesize the audio from the adjusted spectrogram\n",
    "    audio = mel_2_audio(adjusted_spectrogram_db)\n",
    "\n",
    "    return audio, adjusted_spectrogram_db\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have defined 'audio_signal', 'M_torchaudio_db', and 'sample_rate' appropriately\n",
    "audio_adjusted, M_torchaudio_db_transformed = adjust_and_synthesize_audio(audio_signal, M_torchaudio_db, M_torchaudio_db, sample_rate=16000)\n",
    "\n",
    "# Save the adjusted audio\n",
    "#sf.write('audio_adjusted_poly.wav', audio_adjusted, sample_rate)\n",
    "\n",
    "# Plot the adjusted mel spectrogram\n",
    "plt.figure(figsize=(12, 1.6))\n",
    "plt.title('Mel Spectrogram (torchaudio adjusted)')\n",
    "plt.imshow(M_torchaudio_db_transformed, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# Plot the spectra at a specific frame\n",
    "frame_index = 250\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(M_librosa_db[:, frame_index], label='librosa')\n",
    "plt.plot(M_torchaudio_db[:, frame_index], label='torchaudio')\n",
    "plt.plot(M_torchaudio_db_transformed[:, frame_index], label='torchaudio adjusted')\n",
    "plt.legend()\n",
    "plt.title(f'Comparison of Mel Spectra at Frame {frame_index}')\n",
    "plt.xlabel('Mel Bin')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histogram to get a sense of the correct scale (linear vs log, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the spectrogram arrays to 1D\n",
    "mel_librosa_flat = mel_librosa.flatten()\n",
    "M_flat = M.flatten()\n",
    "M_torchaudio_linear_flat = (np.exp(M_torchaudio_db_transformed)).flatten()\n",
    "\n",
    "\n",
    "# Calculate histogram bin edges\n",
    "bins = np.histogram(np.hstack((mel_librosa_flat, M_flat)), bins=400)[1]  # Get only the bin edges\n",
    "\n",
    "# Exclude the first bin by starting from the second bin's minimum edge\n",
    "custom_bins = bins[0:]  # Start from the second bin\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(mel_librosa_flat, bins=custom_bins, alpha=0.5, label='Librosa', color='blue', log=True)\n",
    "plt.hist(M_flat, bins=custom_bins, alpha=0.5, label='Torchaudio', color='orange', log=True)\n",
    "plt.hist(M_torchaudio_linear_flat, bins=custom_bins, alpha=0.5, label='Torchaudio Adjusted', color='green', log=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Mel Spectrogram Magnitudes (First Bin Omitted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Parameters\n",
    "sr = 16000  # Sample rate\n",
    "n_fft = 400  # FFT window size\n",
    "hop_length = 160  # Hop length\n",
    "win_length = 400  # Window length\n",
    "\n",
    "# Generate a test signal consisting of 5 frequencies in the range audible to humans\n",
    "freqs = [200, 400, 900, 1800, 3500]  # Hz\n",
    "duration = 2.0  # seconds\n",
    "t = np.linspace(0., duration, int(sr*duration), endpoint=False)\n",
    "y = np.sum([0.5*np.sin(f*2*np.pi*t) for f in freqs], axis=0)\n",
    "\n",
    "# Librosa Mel Spectrogram\n",
    "S_librosa = librosa.feature.melspectrogram(\n",
    "    y=y,\n",
    "    sr=sample_rate,\n",
    "    n_mels=128,\n",
    "    hop_length=160,  # 10 ms = 160 * 1/16000\n",
    "    n_fft=400,       # 25 ms = 400 * 1/16000\n",
    "    center=False,\n",
    "    htk=True,\n",
    "    fmin=0.0,\n",
    "    fmax=None        # default Nyquist frequency\n",
    ")\n",
    "\n",
    "# Torchaudio Mel Spectrogram\n",
    "waveform = torch.tensor(y).unsqueeze(0)\n",
    "S_torchaudio = torchaudio.compliance.kaldi.fbank(\n",
    "    waveform, \n",
    "    sample_frequency=sr, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Plot the Mel spectrograms computed by both libraries using subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5))\n",
    "plt.suptitle('Comparison of Mel Spectrograms (Librosa vs Torchaudio) of a 440 Hz Sine Wave', fontsize=16)\n",
    "\n",
    "# Librosa Mel Spectrogram\n",
    "axs[0].imshow(S_librosa, aspect='auto', origin='lower', cmap='viridis')\n",
    "axs[0].set_title('Mel Spectrogram (Librosa)')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Mel bins')\n",
    "\n",
    "# Torchaudio Mel Spectrogram\n",
    "print('shape of torchaudio mel spectrogram:', S_torchaudio.shape)\n",
    "S_torchaudio_np = S_torchaudio.numpy().T\n",
    "print('shape of torchaudio mel spectrogram:', S_torchaudio_np.shape)\n",
    "axs[1].imshow(np.exp(S_torchaudio_np), aspect='auto', origin='lower', cmap='viridis')\n",
    "axs[1].set_title('Mel Spectrogram (Torchaudio)')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Mel bins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of spectrograms of audios/madked_music.wav, audios/reconstructed_music_194.wav, audios/original_music.wav\n",
    "\n",
    "# Load the audio files\n",
    "masked_audio, _ = sf.read('audios/masked_music.wav')\n",
    "reconstructed_audio_194, _ = sf.read('audios/reconstructed_music_108.wav')\n",
    "original_audio, _ = sf.read('audios/original_music.wav')\n",
    "raw_audio, _ = sf.read('audios/raw_music.wav')\n",
    "\n",
    "# calculate the mel spectrograms USING LIBROSA\n",
    "masked_mel = librosa.feature.melspectrogram(y=masked_audio, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "reconstructed_mel_194 = librosa.feature.melspectrogram(y=reconstructed_audio_194, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "original_mel = librosa.feature.melspectrogram(y=original_audio, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "raw_mel = librosa.feature.melspectrogram(y=raw_audio, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "\n",
    "# print shapes\n",
    "print(masked_mel.shape)\n",
    "print(reconstructed_mel_194.shape)\n",
    "print(original_mel.shape)\n",
    "print(raw_mel.shape)\n",
    "\n",
    "# turn into log scale\n",
    "masked_mel = librosa.power_to_db(masked_mel)\n",
    "reconstructed_mel_194 = librosa.power_to_db(reconstructed_mel_194)\n",
    "original_mel = librosa.power_to_db(original_mel)\n",
    "raw_mel = librosa.power_to_db(raw_mel)\n",
    "\n",
    "# plot the mel spectrograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.imshow(masked_mel, aspect='auto', origin='lower')\n",
    "plt.title('Masked Mel Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.imshow(reconstructed_mel_194, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Mel Spectrogram (after 194k Iterations)')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.imshow(original_mel, aspect='auto', origin='lower')\n",
    "plt.title('Original Mel Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.imshow(raw_mel, aspect='auto', origin='lower')\n",
    "plt.title('Raw Mel Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_speech, _ = sf.read('audios/masked_speech.wav')\n",
    "reconstructed_speech, _ = sf.read('audios/reconstructed_speech.wav')\n",
    "original_speech, _ = sf.read('audio_adjusted_poly.wav')\n",
    "raw_speech, _ = sf.read('audios/raw_speech.wav')\n",
    "\n",
    "# calculate the mel spectrograms USING LIBROSA\n",
    "masked_mel = librosa.feature.melspectrogram(y=masked_speech, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "reconstructed_mel = librosa.feature.melspectrogram(y=reconstructed_speech, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "original_mel = librosa.feature.melspectrogram(y=original_speech, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "raw_mel = librosa.feature.melspectrogram(y=raw_speech, sr=16000, n_mels=128, hop_length=160, n_fft=400, htk=True, fmin=0.0, fmax=8000)\n",
    "\n",
    "# turn into log scale\n",
    "masked_mel = librosa.power_to_db(masked_mel)\n",
    "reconstructed_mel = librosa.power_to_db(reconstructed_mel)\n",
    "original_mel = librosa.power_to_db(original_mel)\n",
    "raw_mel = librosa.power_to_db(raw_mel)\n",
    "\n",
    "# plot the mel spectrograms\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.imshow(masked_mel, aspect='auto', origin='lower')\n",
    "plt.title('Masked Mel Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.imshow(reconstructed_mel, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Mel Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.imshow(original_mel, aspect='auto', origin='lower')\n",
    "plt.title('Original Mel Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.imshow(raw_mel, aspect='auto', origin='lower')\n",
    "plt.title('Raw Mel Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# generate 370 random indices in the range [0, 499], but using a seed for reproducibility\n",
    "np.random.seed(15)\n",
    "mask_indices = np.random.choice(499, 370, replace=False)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram = reconstructed_spectrogram[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram = reconstructed_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "# print shapes\n",
    "print(\"masked_spectrogram shape:\", masked_spectrogram.shape)\n",
    "print(\"reconstructed_spectrogram shape:\", reconstructed_spectrogram.shape)\n",
    "print(\"fbank_features shape:\", fbank_features.shape)\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "plt.imshow(masked_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "plt.imshow(fbank_features, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def synthesize_audio_from_spectrogram(torchaudio_spectrogram_to_synthesize, reference_audio_signal=None, sample_rate=16000):\n",
    "    \n",
    "    def fit_polynomial_transform(M1, M2, order=5):\n",
    "        b = M1 - M2\n",
    "        f_bins, t_frames = M1.shape\n",
    "        f = np.repeat(np.arange(f_bins), t_frames)\n",
    "        # Create polynomial features up to the given order\n",
    "        A = np.column_stack([f**i for i in range(order + 1)])\n",
    "        b = b.flatten()\n",
    "        A_transpose = A.T\n",
    "        A_transpose_A = np.dot(A_transpose, A)\n",
    "        A_transpose_b = np.dot(A_transpose, b)\n",
    "        x = np.linalg.solve(A_transpose_A, A_transpose_b)\n",
    "        return x  # coefficients of the polynomial\n",
    "\n",
    "    def mel_2_audio(M):\n",
    "        sr = sample_rate\n",
    "        n_fft = 400\n",
    "        hop_length = int(0.01 * sr)  # 10 ms\n",
    "        win_length = int(0.025 * sr)  # 25 ms (= n_fft)\n",
    "        window = 'hanning'\n",
    "        linear_M = librosa.db_to_power(M)\n",
    "        audio = librosa.feature.inverse.mel_to_audio(linear_M, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, n_iter=50, htk=True)\n",
    "        return audio\n",
    "    \n",
    "    if reference_audio_signal is not None:\n",
    "\n",
    "        librosa_spectrogram_linear = librosa.feature.melspectrogram(\n",
    "            y=reference_audio_signal,\n",
    "            sr=sample_rate,\n",
    "            n_mels=128,\n",
    "            hop_length=160,  # 10 ms = 160 * 1/16000\n",
    "            n_fft=400,       # 25 ms = 400 * 1/16000\n",
    "            center=False,\n",
    "            htk=True,\n",
    "            fmin=0.0,\n",
    "            fmax=sample_rate / 2  # default Nyquist frequency\n",
    "        )\n",
    "\n",
    "        reference_audio_tensor = torch.from_numpy(reference_audio_signal).float().unsqueeze(0)\n",
    "\n",
    "        fbanks_to_fit = torchaudio.compliance.kaldi.fbank(\n",
    "            reference_audio_tensor,\n",
    "            sample_frequency=sample_rate,\n",
    "            htk_compat=True,\n",
    "            use_energy=False,\n",
    "            window_type='hanning',\n",
    "            num_mel_bins=128,\n",
    "            dither=0.0,\n",
    "            frame_shift=10\n",
    "        )\n",
    "        # now shape is torch.Size([time_frames, mel_frequency_bins])\n",
    "\n",
    "        torchaudio_spectrogram_to_fit = fbanks_to_fit.cpu().numpy().T\n",
    "        # now shape is (mel_frequency_bins, time_frames)\n",
    "\n",
    "        torchaudio_spectrogram_to_fit_linear = np.exp(torchaudio_spectrogram_to_fit) # torchaudio uses a natural logarithm\n",
    "        torchaudio_spectrogram_to_synthesize_linear = np.exp(torchaudio_spectrogram_to_synthesize) # torchaudio uses a natural logarithm\n",
    "\n",
    "        torchaudio_spectrogram_to_fit_db = librosa.power_to_db(torchaudio_spectrogram_to_fit_linear)\n",
    "        torchaudio_spectrogram_to_synthesize_db = librosa.power_to_db(torchaudio_spectrogram_to_synthesize_linear)\n",
    "        librosa_spectrogram_linear_db = librosa.power_to_db(librosa_spectrogram_linear)\n",
    "\n",
    "        # Calculate the polynomial transformation parameters\n",
    "        coefficients = fit_polynomial_transform(np.array(librosa_spectrogram_linear_db), np.array(torchaudio_spectrogram_to_fit_db))\n",
    "\n",
    "        # Apply the polynomial transformation to each frequency bin across all time frames\n",
    "        frequency_indices = np.arange(torchaudio_spectrogram_to_synthesize_db.shape[0]).reshape(-1, 1)\n",
    "        adjustments = sum([coefficients[i] * frequency_indices**i for i in range(len(coefficients))])\n",
    "        adjustments = np.tile(adjustments, (1, torchaudio_spectrogram_to_synthesize_db.shape[1]))\n",
    "        adjusted_spectrogram_db = torchaudio_spectrogram_to_synthesize_db + adjustments\n",
    "\n",
    "        # Resynthesize the audio from the adjusted spectrogram\n",
    "        audio = mel_2_audio(adjusted_spectrogram_db)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        torchaudio_spectrogram_to_synthesize_linear = np.exp(torchaudio_spectrogram_to_synthesize) # torchaudio uses a natural logarithm\n",
    "        torchaudio_spectrogram_to_synthesize_db = librosa.power_to_db(torchaudio_spectrogram_to_synthesize_linear)\n",
    "        audio = mel_2_audio(torchaudio_spectrogram_to_synthesize_db)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "# convert back to waveform\n",
    "y_masked = synthesize_audio_from_spectrogram(masked_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_reconstructed = synthesize_audio_from_spectrogram(reconstructed_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_original = synthesize_audio_from_spectrogram(fbank_features, reference_audio_signal=audio_signal)\n",
    "\n",
    "\n",
    "# save audio\n",
    "directory = 'audios_sandbox'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_masked_audio = 'audios_sandbox/masked_speech_random_masks.wav'\n",
    "output_path_reconstructed_audio = 'audios_sandbox/reconstructed_speech_random_masks.wav' \n",
    "output_path_original_audio = 'audios_sandbox/original_speech_random_masks.wav'\n",
    "sr = 16000\n",
    "sf.write(output_path_masked_audio, y_masked, sr)\n",
    "sf.write(output_path_reconstructed_audio, y_reconstructed, sr)\n",
    "sf.write(output_path_original_audio, y_original, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
