{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "from ssast_model import ASTModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchaudio\n",
    "import pickle\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruct spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to reconstruct an audio signal from a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_audio_from_spectrogram(torchaudio_spectrogram_to_synthesize, reference_audio_signal=None, sample_rate=16000):\n",
    "    \n",
    "    def fit_polynomial_transform(M1, M2, order=5):\n",
    "        b = M1 - M2\n",
    "        f_bins, t_frames = M1.shape\n",
    "        f = np.repeat(np.arange(f_bins), t_frames)\n",
    "        # Create polynomial features up to the given order\n",
    "        A = np.column_stack([f**i for i in range(order + 1)])\n",
    "        b = b.flatten()\n",
    "        A_transpose = A.T\n",
    "        A_transpose_A = np.dot(A_transpose, A)\n",
    "        A_transpose_b = np.dot(A_transpose, b)\n",
    "        x = np.linalg.solve(A_transpose_A, A_transpose_b)\n",
    "        return x  # coefficients of the polynomial\n",
    "\n",
    "    def mel_2_audio(M):\n",
    "        sr = sample_rate\n",
    "        fmin = 20\n",
    "        n_fft = 512\n",
    "        hop_length = int(0.01 * sr)  # 10 ms\n",
    "        win_length = int(0.025 * sr)  # 25 ms \n",
    "        window = 'hanning'\n",
    "        linear_M = librosa.db_to_power(M)\n",
    "        audio = librosa.feature.inverse.mel_to_audio(linear_M, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, n_iter=50, htk=True, fmin=fmin)\n",
    "        return audio\n",
    "    \n",
    "    if reference_audio_signal is not None:\n",
    "\n",
    "        librosa_spectrogram_linear = librosa.feature.melspectrogram(\n",
    "            y=reference_audio_signal,\n",
    "            sr=sample_rate,\n",
    "            n_mels=128,\n",
    "            hop_length=160,  # 10 ms = 160 * 1/16000\n",
    "            win_length=400,  # 25 ms = 400 * 1/16000\n",
    "            n_fft=512,\n",
    "            center=False,\n",
    "            htk=True,\n",
    "            fmin=20,\n",
    "            fmax=sample_rate / 2  # default Nyquist frequency\n",
    "        )\n",
    "\n",
    "        reference_audio_tensor = torch.from_numpy(reference_audio_signal).float().unsqueeze(0)\n",
    "\n",
    "        fbanks_to_fit = torchaudio.compliance.kaldi.fbank(\n",
    "            reference_audio_tensor,\n",
    "            sample_frequency=sample_rate,\n",
    "            htk_compat=True,\n",
    "            use_energy=False,\n",
    "            window_type='hanning',\n",
    "            num_mel_bins=128,\n",
    "            dither=0.0,\n",
    "            frame_shift=10\n",
    "        )\n",
    "        # now shape is torch.Size([time_frames, mel_frequency_bins])\n",
    "\n",
    "        torchaudio_spectrogram_to_fit = fbanks_to_fit.cpu().numpy().T\n",
    "        # now shape is (mel_frequency_bins, time_frames)\n",
    "\n",
    "        torchaudio_spectrogram_to_fit_linear = np.exp(torchaudio_spectrogram_to_fit) # torchaudio uses a natural logarithm\n",
    "        torchaudio_spectrogram_to_synthesize_linear = np.exp(torchaudio_spectrogram_to_synthesize) # torchaudio uses a natural logarithm\n",
    "\n",
    "        torchaudio_spectrogram_to_fit_db = librosa.power_to_db(torchaudio_spectrogram_to_fit_linear)\n",
    "        torchaudio_spectrogram_to_synthesize_db = librosa.power_to_db(torchaudio_spectrogram_to_synthesize_linear)\n",
    "        librosa_spectrogram_linear_db = librosa.power_to_db(librosa_spectrogram_linear)\n",
    "\n",
    "        # Calculate the polynomial transformation parameters\n",
    "        coefficients = fit_polynomial_transform(np.array(librosa_spectrogram_linear_db), np.array(torchaudio_spectrogram_to_fit_db[:, :-1]))\n",
    "\n",
    "        # Apply the polynomial transformation to each frequency bin across all time frames\n",
    "        frequency_indices = np.arange(torchaudio_spectrogram_to_synthesize_db.shape[0]).reshape(-1, 1)\n",
    "        adjustments = sum([coefficients[i] * frequency_indices**i for i in range(len(coefficients))])\n",
    "        adjustments = np.tile(adjustments, (1, torchaudio_spectrogram_to_synthesize_db.shape[1]))\n",
    "        adjusted_spectrogram_db = torchaudio_spectrogram_to_synthesize_db + adjustments\n",
    "\n",
    "        # Resynthesize the audio from the adjusted spectrogram\n",
    "        audio = mel_2_audio(adjusted_spectrogram_db)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        torchaudio_spectrogram_to_synthesize_linear = np.exp(torchaudio_spectrogram_to_synthesize) # torchaudio uses a natural logarithm\n",
    "        torchaudio_spectrogram_to_synthesize_db = librosa.power_to_db(torchaudio_spectrogram_to_synthesize_linear)\n",
    "        audio = mel_2_audio(torchaudio_spectrogram_to_synthesize_db)\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save original audio to a WAV file\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_raw_audio = 'audios/raw_speech.wav'\n",
    "sf.write(output_path_raw_audio, audio_signal, sample_rate)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "print(next(model.parameters()).device)  # Should print 'cpu'\n",
    "\n",
    "\n",
    "hop_width = 20\n",
    "hop_length = 50 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - hop_width//2, hop_length)\n",
    "print(\"hops: \", hops) # range(50, 489, 50)\n",
    "mask_indices = [range(i-hop_width//2, i + hop_width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(\"mask_indices: \", mask_indices) # [40, 41, 42, ..., 59, 90, 91, 92, ..., 109, 140, 141, 142, ..., 159, 190, 191, 192, ......, 459]\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram = reconstructed_spectrogram[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram = reconstructed_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "# print shapes\n",
    "print(\"masked_spectrogram shape:\", masked_spectrogram.shape)\n",
    "print(\"reconstructed_spectrogram shape:\", reconstructed_spectrogram.shape)\n",
    "print(\"fbank_features shape:\", fbank_features.shape)\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "plt.imshow(masked_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "plt.imshow(fbank_features, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# convert back to waveform\n",
    "y_masked = synthesize_audio_from_spectrogram(masked_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_reconstructed = synthesize_audio_from_spectrogram(reconstructed_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_original = synthesize_audio_from_spectrogram(fbank_features, reference_audio_signal=audio_signal)\n",
    "\n",
    "\n",
    "# save audio\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_masked_audio = 'audios/masked_speech.wav'\n",
    "output_path_reconstructed_audio = 'audios/reconstructed_speech.wav' \n",
    "output_path_original_audio = 'audios/original_speech.wav'\n",
    "sr = 16000\n",
    "sf.write(output_path_masked_audio, y_masked, sr)\n",
    "sf.write(output_path_reconstructed_audio, y_reconstructed, sr)\n",
    "sf.write(output_path_original_audio, y_original, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare raw and reconstructed audio spectrograms\n",
    "\n",
    "signal_raw, sr = sf.read(output_path_raw_audio)\n",
    "signal_masked, sr = sf.read(output_path_masked_audio)\n",
    "signal_reconstructed, sr = sf.read(output_path_reconstructed_audio)\n",
    "signal_original, sr = sf.read(output_path_original_audio)\n",
    "\n",
    "M_raw = librosa.feature.melspectrogram(y=signal_raw, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "M_masked = librosa.feature.melspectrogram(y=signal_masked, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "M_reconstructed = librosa.feature.melspectrogram(y=signal_reconstructed, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "M_original = librosa.feature.melspectrogram(y=signal_original, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "\n",
    "M_raw = librosa.power_to_db(M_raw)\n",
    "M_masked = librosa.power_to_db(M_masked)\n",
    "M_reconstructed = librosa.power_to_db(M_reconstructed)\n",
    "M_original = librosa.power_to_db(M_original)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.imshow(M_masked, aspect='auto', origin='lower')\n",
    "plt.title('Masked Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.imshow(M_reconstructed, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.imshow(M_original, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.imshow(M_raw, aspect='auto', origin='lower')\n",
    "plt.title('Raw Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### long mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save original audio to a WAV file\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_raw_audio = 'audios/raw_speech.wav'\n",
    "sf.write(output_path_raw_audio, audio_signal, sample_rate)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "\n",
    "\n",
    "model1 = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model2 = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model1 = torch.nn.DataParallel(model1)\n",
    "model2 = torch.nn.DataParallel(model2)\n",
    "model1.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model2.load_state_dict(torch.load('../pretraining/exp/pretrained-20240429-112534-shuffled-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.54.pth'))\n",
    "model1 = model1.module\n",
    "model2 = model2.module\n",
    "model1.to('cpu')\n",
    "model2.to('cpu')\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "hop_width = 80\n",
    "hop_length = 250 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - hop_width//2, hop_length)\n",
    "print(\"hops: \", hops) # range(50, 489, 50)\n",
    "mask_indices = [range(i-hop_width//2, i + hop_width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(\"mask_indices: \", mask_indices) # [40, 41, 42, ..., 59, 90, 91, 92, ..., 109, 140, 141, 142, ..., 159, 190, 191, 192, ......, 459]\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram1 = model1(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "    reconstructed_spectrogram2 = model2(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram1 = reconstructed_spectrogram1[0].cpu().numpy().T\n",
    "reconstructed_spectrogram2 = reconstructed_spectrogram2[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram1 = reconstructed_spectrogram1 * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram2 = reconstructed_spectrogram2 * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.imshow(masked_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram2, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram from Shuffled Model')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.imshow(reconstructed_spectrogram1, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram from Original Model')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.imshow(fbank_features, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# convert back to waveform\n",
    "y_masked = synthesize_audio_from_spectrogram(masked_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_reconstructed1 = synthesize_audio_from_spectrogram(reconstructed_spectrogram1, reference_audio_signal=audio_signal)\n",
    "y_reconstructed2 = synthesize_audio_from_spectrogram(reconstructed_spectrogram2, reference_audio_signal=audio_signal)\n",
    "y_original = synthesize_audio_from_spectrogram(fbank_features, reference_audio_signal=audio_signal)\n",
    "\n",
    "\n",
    "# save audio\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_masked_audio = 'audios/masked_speech_longmask.wav'\n",
    "output_path_reconstructed_audio1 = 'audios/reconstructed_speech_OSModel_longmask.wav'\n",
    "output_path_reconstructed_audio2 = 'audios/reconstructed_speech_SUModel_longmask.wav' \n",
    "output_path_original_audio = 'audios/original_speech.wav'\n",
    "sr = 16000\n",
    "sf.write(output_path_masked_audio, y_masked, sr)\n",
    "sf.write(output_path_reconstructed_audio1, y_reconstructed1, sr)\n",
    "sf.write(output_path_reconstructed_audio2, y_reconstructed2, sr)\n",
    "sf.write(output_path_original_audio, y_original, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "#file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed_eval/1a281f7d-bde7-4744-9220-7ea599b6e093/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save original audio to a WAV file\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_raw_audio = 'audios/raw_music.wav'\n",
    "sf.write(output_path_raw_audio, audio_signal, sample_rate)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "test_input = fbank_features\n",
    "\n",
    "# keep numpy copy for audio reconstruction\n",
    "test_input_numpy = test_input.clone().detach().numpy().T\n",
    "print(f\"Shape of numpy fbank features: {test_input_numpy.shape}\")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "\n",
    "width = 20\n",
    "hop_length = 50 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - width//2, hop_length)\n",
    "mask_indices = [range(i-width//2, i + width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(mask_indices)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(test_input)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = test_input * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Assuming 'test_input' is your spectrogram tensor\n",
    "n_timesteps = test_input.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    return ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "\n",
    "def load_model(checkpoint_id):\n",
    "    model = initialize_model()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(f'../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.{checkpoint_id}.pth'))\n",
    "    model = model.module\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "models = {i: load_model(i) for i in [1, 2, 6, 40, 108]}\n",
    "\n",
    "# Generate spectrograms\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrograms = {i: model(test_input, task='visualize_mask', mask_indices=mask_indices) for i, model in models.items()}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "def plot_spectrogram(data, title, position):         \n",
    "    plt.subplot(7, 1, position)\n",
    "    plt.imshow(data[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_ticks, x_labels)\n",
    "    plt.yticks(y_ticks)\n",
    "\n",
    "def convert_and_save_audio(data, output_path, M_for_fitting=test_input_numpy):\n",
    "    M_normalized = data[0].cpu().numpy().T\n",
    "    M_unnormalized = M_normalized * (2 * dataset_std) + dataset_mean\n",
    "    y = synthesize_audio_from_spectrogram(M_unnormalized, reference_audio_signal=audio_signal)\n",
    "    sr = 16000\n",
    "    sf.write(output_path, y, sr)\n",
    "\n",
    "# masked spectrogram\n",
    "plot_spectrogram(masked_spectrogram, 'Masked Spectrogram', 1)\n",
    "convert_and_save_audio(masked_spectrogram, 'audios/masked_music.wav')\n",
    "\n",
    "# reconstructed spectrograms\n",
    "for index, checkpoint in enumerate(sorted(reconstructed_spectrograms.keys())):\n",
    "    plot_spectrogram(reconstructed_spectrograms[checkpoint], f'Reconstructed Spectrogram (after {(checkpoint-1)*4}k Iterations)', index + 2)\n",
    "    convert_and_save_audio(reconstructed_spectrograms[checkpoint], f'audios/reconstructed_music_{checkpoint}.wav')\n",
    "\n",
    "# original spectrogram\n",
    "plot_spectrogram(test_input, 'Original Spectrogram', 7)\n",
    "convert_and_save_audio(test_input, 'audios/original_music.wav')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare raw and reconstructed audio spectrograms\n",
    "\n",
    "signal_raw, sr = sf.read(\"audios/raw_music.wav\")\n",
    "signal_original, sr = sf.read(\"audios/original_music.wav\")\n",
    "signal_reconstructed_194, sr = sf.read(\"audios/reconstructed_music_108.wav\")\n",
    "\n",
    "M_raw = librosa.feature.melspectrogram(y=signal_raw, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "M_original = librosa.feature.melspectrogram(y=signal_original, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "M_reconstructed_194 = librosa.feature.melspectrogram(y=signal_reconstructed_194, sr=sr, n_mels=128, hop_length=160, n_fft=400, center=False, htk=True, fmin=0.0, fmax=sr/2)\n",
    "\n",
    "M_raw = librosa.power_to_db(M_raw)\n",
    "M_original = librosa.power_to_db(M_original)\n",
    "M_reconstructed_194 = librosa.power_to_db(M_reconstructed_194)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(M_raw, aspect='auto', origin='lower')\n",
    "plt.title('Raw Spectrogram')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(M_original, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(M_reconstructed_194, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram (after 108k Iterations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Speech but with randomly distributed masked frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model trained on original spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# make 250 random indices in the range [0, 499], but using a seed for reproducibility\n",
    "np.random.seed(15)\n",
    "mask_indices = np.random.choice(499, 370, replace=False)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram = reconstructed_spectrogram[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram = reconstructed_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "# print shapes\n",
    "print(\"masked_spectrogram shape:\", masked_spectrogram.shape)\n",
    "print(\"reconstructed_spectrogram shape:\", reconstructed_spectrogram.shape)\n",
    "print(\"fbank_features shape:\", fbank_features.shape)\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "plt.imshow(masked_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "plt.imshow(fbank_features, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# convert back to waveform\n",
    "y_masked = synthesize_audio_from_spectrogram(masked_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_reconstructed = synthesize_audio_from_spectrogram(reconstructed_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_original = synthesize_audio_from_spectrogram(fbank_features, reference_audio_signal=audio_signal)\n",
    "\n",
    "\n",
    "# save audio\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_masked_audio = 'audios/masked_speech_random_masks_originalModel.wav'\n",
    "output_path_reconstructed_audio = 'audios/reconstructed_speech_random_masks_originalModel.wav' \n",
    "output_path_original_audio = 'audios/original_speech_random_masks_originalModel.wav'\n",
    "sr = 16000\n",
    "sf.write(output_path_masked_audio, y_masked, sr)\n",
    "sf.write(output_path_reconstructed_audio, y_reconstructed, sr)\n",
    "sf.write(output_path_original_audio, y_original, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model trained on shuffled spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240429-112534-shuffled-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.54.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# make 250 random indices in the range [0, 499], but using a seed for reproducibility\n",
    "np.random.seed(15)\n",
    "mask_indices = np.random.choice(499, 370, replace=False)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram = reconstructed_spectrogram[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram = reconstructed_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "# print shapes\n",
    "print(\"masked_spectrogram shape:\", masked_spectrogram.shape)\n",
    "print(\"reconstructed_spectrogram shape:\", reconstructed_spectrogram.shape)\n",
    "print(\"fbank_features shape:\", fbank_features.shape)\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "plt.imshow(masked_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "plt.imshow(fbank_features, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# convert back to waveform\n",
    "y_masked = synthesize_audio_from_spectrogram(masked_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_reconstructed = synthesize_audio_from_spectrogram(reconstructed_spectrogram, reference_audio_signal=audio_signal)\n",
    "y_original = synthesize_audio_from_spectrogram(fbank_features, reference_audio_signal=audio_signal)\n",
    "\n",
    "\n",
    "# save audio\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_path_masked_audio = 'audios/masked_speech_random_masks_shuffledModel.wav'\n",
    "output_path_reconstructed_audio = 'audios/reconstructed_speech_random_masks_shuffledModel.wav' \n",
    "output_path_original_audio = 'audios/original_speech_random_masks_shuffledModel.wav'\n",
    "sr = 16000\n",
    "sf.write(output_path_masked_audio, y_masked, sr)\n",
    "sf.write(output_path_reconstructed_audio, y_reconstructed, sr)\n",
    "sf.write(output_path_original_audio, y_original, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
