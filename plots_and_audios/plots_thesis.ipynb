{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "from ssast_model import ASTModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from PIL import Image\n",
    "import qrcode\n",
    "import torchaudio\n",
    "import pickle\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting global parameters for Matplotlib to use LaTeX rendering\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,  # Enable LaTeX rendering\n",
    "    'text.latex.preamble': r'\\usepackage{lmodern}\\usepackage{amsmath}',  # Use Latin Modern font and include amsmath\n",
    "    'font.family': 'serif',  # Use serif font for consistency with LaTeX document\n",
    "    'font.serif': ['Latin Modern Roman'],  # Specify Latin Modern Roman\n",
    "    'pdf.fonttype': 42,  # Ensures fonts are embedded as TrueType\n",
    "    'savefig.dpi': 400,  # Lower DPI setting for non-text elements\n",
    "    'font.size': 8.5,  # Adjust font size to match document (you may need to tweak this)\n",
    "    'axes.labelsize': 8.5,  # Size of the x and y labels\n",
    "    'axes.titlesize': 10.0,  # Size of the plot title\n",
    "    'xtick.labelsize': 7.0,  # Size of the x-axis tick labels\n",
    "    'ytick.labelsize': 7.0,  # Size of the y-axis tick labels\n",
    "    'legend.fontsize': 8.5,  # Size of the legend font\n",
    "    'figure.titlesize': 12.0  # Size of the figure's main title if any\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "\n",
    "# Function to plot spectrogram\n",
    "def plot_spectrogram(spectrogram, ax, title=\"Mel Spectrogram\"):\n",
    "    # Assuming spectrogram data is on a tensor and needs to be moved to CPU and converted to numpy array\n",
    "    ax.imshow(spectrogram.T.cpu().numpy(), aspect='auto', origin='lower', cmap='viridis', interpolation='none')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(np.concatenate((np.arange(0, 1000, 100), [spectrogram.shape[0]])))\n",
    "    ax.set_yticks(np.arange(0, 150, 25))\n",
    "    ax.set_xlabel('Time Frames')\n",
    "    ax.set_ylabel('Mel Frequency Bins')\n",
    "\n",
    "def plot_spectrogram_mesh(spectrogram, ax, title=\"Spectrogram\"):\n",
    "    # Convert tensor to numpy if necessary\n",
    "    data = spectrogram.T.cpu().numpy()\n",
    "    # Create a meshgrid for the x and y dimensions\n",
    "    x = np.linspace(0, data.shape[1], data.shape[1])\n",
    "    y = np.linspace(0, data.shape[0], data.shape[0])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    # Use pcolormesh to plot data\n",
    "    pcm = ax.pcolormesh(X, Y, data, shading='auto', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(np.concatenate((np.arange(0, 1000, 100), [spectrogram.shape[0]])))\n",
    "    ax.set_yticks(np.arange(0, 150, 25))\n",
    "    ax.set_xlabel('Time Frames')\n",
    "    ax.set_ylabel('Mel Frequency Bins')\n",
    "    # Optionally add a colorbar\n",
    "    #plt.colorbar(pcm, ax=ax)\n",
    "\n",
    "# Create a figure with two subplots, one above the other\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6.2, 2.8))  # Adjust total figure size as needed\n",
    "\n",
    "# Plot the audio signal\n",
    "ax1.plot(time, audio_signal, linewidth=0.2)\n",
    "ax1.set_xlabel('Time [s]')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.set_xticks(np.arange(0, 11, 1))\n",
    "ax1.set_yticks(np.arange(-0.2, 0.3, 0.1))\n",
    "ax1.set_xlim([0, 10])\n",
    "ax1.set_title('Waveform')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot the fbank features using the defined function\n",
    "plot_spectrogram(fbank_features, ax2)\n",
    "\n",
    "# Adjust layout so plots do not overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig(\"plots/waveform_spectrogram.pdf\")\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffled frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the progress and result data\n",
    "path_shuffled = '../pretraining/exp/pretrained-20240429-112534-shuffled-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli'\n",
    "with open(os.path.join(path_shuffled, 'progress.pkl'), 'rb') as f:\n",
    "    progress_shuffled = pickle.load(f)\n",
    "iteration_shuffled = np.array([x[1] for x in progress_shuffled])\n",
    "epoch_shuffled = np.linspace(0, 5, len(iteration_shuffled))\n",
    "result_shuffled = np.genfromtxt(os.path.join(path_shuffled, 'result.csv'), delimiter=',')\n",
    "\n",
    "def get_column(array):\n",
    "    for i in range(array.shape[1]):\n",
    "        yield array[:, i]\n",
    "\n",
    "acc_tr_shuffled, loss1_tr_shuffled, loss2_tr_shuffled, acc_ev_shuffled, loss1_ev_shuffled, loss2_ev_shuffled, lr_shuffled = get_column(result_shuffled)\n",
    "\n",
    "# Define expected values\n",
    "expected_mse = 0.129726\n",
    "expected_infoNCE = 5.966147\n",
    "\n",
    "# Setup the plot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(6.2, 3.7))\n",
    "color_shuffled = 'tab:green'\n",
    "\n",
    "# Plot training and evaluation data with horizontal lines for expected values\n",
    "axs[0, 0].axhline(y=expected_mse, color='r', linestyle='--', label=f'Expected Value')\n",
    "axs[0, 0].plot(epoch_shuffled, loss2_tr_shuffled, color=color_shuffled, label='True Value')\n",
    "axs[0, 0].set_xlabel('Epoch')\n",
    "axs[0, 0].set_ylabel('MSE (Train)')\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].grid()\n",
    "\n",
    "axs[1, 0].plot(epoch_shuffled, loss2_ev_shuffled, color=color_shuffled, linestyle='dotted')\n",
    "axs[1, 0].set_xlabel('Epoch')\n",
    "axs[1, 0].set_ylabel('MSE (Eval)')\n",
    "axs[1, 0].grid()\n",
    "\n",
    "axs[0, 1].axhline(y=expected_infoNCE, color='r', linestyle='--', label=f'Expected Value')\n",
    "axs[0, 1].plot(epoch_shuffled, loss1_tr_shuffled, color=color_shuffled, label='True Value')\n",
    "axs[0, 1].set_xlabel('Epoch')\n",
    "axs[0, 1].set_ylabel('InfoNCE (Train)')\n",
    "axs[0, 1].legend()\n",
    "axs[0, 1].grid()\n",
    "\n",
    "# Plot for learning rate\n",
    "axs[1, 1].plot(epoch_shuffled, loss1_ev_shuffled, color=color_shuffled, linestyle='dotted')\n",
    "axs[1, 1].set_xlabel('Epoch')\n",
    "axs[1, 1].set_ylabel('InfoNCE (Eval)')\n",
    "axs[1, 1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig('plots/pretraining_shuffledModel.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original frames compared to gong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the progress and result data\n",
    "path_original = '../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli'\n",
    "with open(os.path.join(path_original, 'progress.pkl'), 'rb') as f:\n",
    "    progress_original = pickle.load(f)\n",
    "iteration_original = np.array([x[1] for x in progress_original])\n",
    "epoch_original = np.linspace(0, 10, len(iteration_original))\n",
    "result_original = np.genfromtxt(os.path.join(path_original, 'result.csv'), delimiter=',')\n",
    "\n",
    "\n",
    "# Load Gong's model\n",
    "relative_path_gong = 'pretraining/result_gong.csv'\n",
    "path_gong = os.path.join(parent_directory, relative_path_gong)\n",
    "result_gong = np.genfromtxt(path_gong, delimiter=',')\n",
    "\n",
    "print(\"rows orig:\\t\", result_original.shape[0], \";\\t\\tcolumns orig:\\t\", result_original.shape[1])\n",
    "print(\"rows gong:\\t\", result_gong.shape[0], \";\\t\\tcolumns gong:\\t\", result_gong.shape[1])\n",
    "\n",
    "def get_column(array):\n",
    "    for i in range(array.shape[1]):\n",
    "        yield array[:, i]\n",
    "\n",
    "acc_tr_original, loss1_tr_original, loss2_tr_original, acc_ev_original, loss1_ev_original, loss2_ev_original, lr_original = get_column(result_original)\n",
    "acc_tr_gong, loss_tr_gong, acc_ev_gong, mse_ev_gong, lr_gong = get_column(result_gong)\n",
    "iteration_gong = np.arange(1, len(acc_tr_gong)+1) * 4000\n",
    "epoch_gong = np.linspace(0, 8.5, len(iteration_gong))\n",
    "\n",
    "# Setup the plot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(6.2, 3.7))\n",
    "color_original = 'tab:green'\n",
    "color_gong = 'tab:orange'\n",
    "\n",
    "axs[0, 0].plot(epoch_original, loss1_tr_original + 10*loss2_tr_original, color=color_original, label='Ours')\n",
    "axs[0, 0].plot(epoch_gong, loss_tr_gong, color=color_gong, label=r'\\textsc{Gong}s')\n",
    "axs[0, 0].set_xlabel('Epoch')\n",
    "axs[0, 0].set_ylabel('Loss (Train)')\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].grid()\n",
    "\n",
    "axs[1, 0].plot(epoch_original, loss2_ev_original, color=color_original, linestyle='dotted', label='Ours')\n",
    "axs[1, 0].plot(epoch_gong, mse_ev_gong, color=color_gong, linestyle='dotted', label=r'\\textsc{Gong}s')\n",
    "axs[1, 0].set_xlabel('Epoch')\n",
    "axs[1, 0].set_ylabel('MSE (Eval)')\n",
    "axs[1, 0].legend()\n",
    "axs[1, 0].grid()\n",
    "\n",
    "axs[0, 1].plot(epoch_original, acc_tr_original, color=color_original, label='Ours')\n",
    "axs[0, 1].plot(epoch_gong, acc_tr_gong, color=color_gong, label=r'\\textsc{Gong}s')\n",
    "axs[0, 1].set_xlabel('Epoch')\n",
    "axs[0, 1].set_ylabel('Accuracy (Train)')\n",
    "axs[0, 1].legend()\n",
    "axs[0, 1].grid()\n",
    "\n",
    "axs[1, 1].plot(epoch_original, acc_ev_original, color=color_original, linestyle='dotted', label='Ours')\n",
    "axs[1, 1].plot(epoch_gong, acc_ev_gong, color=color_gong, linestyle='dotted', label=r'\\textsc{Gong}s')\n",
    "axs[1, 1].set_xlabel('Epoch')\n",
    "axs[1, 1].set_ylabel('Accuracy (Eval)')\n",
    "axs[1, 1].legend()\n",
    "axs[1, 1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig('plots/pretraining_comparison_Gong.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training progress\n",
    "fig, ax1 = plt.subplots(figsize=(6.2, 3.5))\n",
    "\n",
    "# set title\n",
    "#fig.suptitle('Pretraining Progress')\n",
    "\n",
    "color_acc = 'tab:green'\n",
    "ax1.set_xlabel('Iteration [k]')\n",
    "ax1.set_ylabel('Accuracy', color=color_acc)\n",
    "line1, = ax1.plot(iteration_original / 1e3, acc_tr_original, color=color_acc, label='Train Accuracy')\n",
    "line2, = ax1.plot(iteration_original / 1e3, acc_ev_original, color=color_acc, linestyle='dotted', label='Eval Accuracy')\n",
    "ax1.tick_params(axis='y', labelcolor=color_acc)\n",
    "#ax1.set_ylim(0, 1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color_InfoNCE = 'tab:red'\n",
    "ax2.set_ylabel('Loss: InfoNCE', color=color_InfoNCE)\n",
    "line3, = ax2.plot(iteration_original / 1e3, loss1_tr_original, color=color_InfoNCE, label='Train InfoNCE')\n",
    "line4, = ax2.plot(iteration_original / 1e3, loss1_ev_original, color=color_InfoNCE, linestyle='dotted', label='Eval InfoNCE')\n",
    "ax2.tick_params(axis='y', labelcolor=color_InfoNCE)\n",
    "ax2.set_ylim(0, ax2.get_ylim()[1])\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 40))\n",
    "color_10MSE = 'tab:orange'\n",
    "ax3.set_ylabel('Loss: 10*MSE', color=color_10MSE)\n",
    "line5, = ax3.plot(iteration_original / 1e3, loss2_tr_original*10, color=color_10MSE, label='Train 10*MSE')\n",
    "line6, = ax3.plot(iteration_original / 1e3, loss2_ev_original*10, color=color_10MSE, linestyle='dotted', label='Eval 10*MSE')\n",
    "ax3.tick_params(axis='y', labelcolor=color_10MSE)\n",
    "ax3.set_ylim(0, ax3.get_ylim()[1])\n",
    "\n",
    "# Adding grid to the primary axis (ax1)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Collect all lines and labels for the legend\n",
    "lines = [line1, line2, line3, line4, line5, line6]\n",
    "labels = [line.get_label() for line in lines]\n",
    "\n",
    "# After your plotting code, to adjust the legend:\n",
    "all_axes = fig.get_axes()\n",
    "for axis in all_axes:\n",
    "    legend = axis.get_legend()\n",
    "    if legend is not None:\n",
    "        legend.remove()\n",
    "        all_axes[-1].add_artist(legend)\n",
    "\n",
    "# Place a single legend on the plot\n",
    "ax3.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "#plt.savefig('plots/pretraining_progress.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the masked spectrogram, the reconstructed spectrogram, and the original spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "test_input = fbank_features\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {test_input.shape}\")\n",
    "\n",
    "# # duplicate input tensor to get a batch of 2\n",
    "# test_input = torch.cat((test_input, test_input), 0)\n",
    "# print(f\"Shape of dublicated fbank features: {test_input.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "print(next(model.parameters()).device)  # Should print 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "hop_width = 20\n",
    "hop_length = 50 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - hop_width//2, hop_length)\n",
    "print(hops)\n",
    "mask_indices = [range(i-hop_width//2, i + hop_width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(mask_indices)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(test_input)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = test_input * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(test_input, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(test_input.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "# Assuming 'test_input' is your spectrogram tensor\n",
    "n_timesteps = test_input.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(6.2, 3.3))\n",
    "\n",
    "urls = [\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/masked_speech.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_speech.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/original_speech.wav'\n",
    "]\n",
    "\n",
    "# Generate QR codes and convert them to numpy arrays\n",
    "qr_codes = []\n",
    "for url in urls:\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "        box_size=10,  # Reduced box size for fitting within plot\n",
    "        border=1,  # Minimal border\n",
    "    )\n",
    "    qr.add_data(url)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image(fill_color='black', back_color='white').convert('L')\n",
    "    qr_codes.append(np.array(img))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = test_input.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# Function to add rectangles to the axes\n",
    "def add_rectangles(ax, hops, patch_width, patch_height):\n",
    "    for mid_index in hops:\n",
    "        # Calculate the left corner of the rectangle\n",
    "        left_corner = 2*mid_index - patch_width\n",
    "        # Create a rectangle patch with gray color\n",
    "        rect = patches.Rectangle((left_corner, 0), 2*patch_width, patch_height, linewidth=1, edgecolor='lightgray', facecolor='lightgray')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        # Add 'MASK' text at the center of the rectangle\n",
    "        ax.text(left_corner + patch_width, patch_height / 2, r'\\textbf{MASK}', horizontalalignment='center', verticalalignment='center', color='black', fontsize=6, rotation=90)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "plt.imshow(masked_spectrogram[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "add_rectangles(ax1, hops, hop_width, patch_height)\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "inset_ax1 = ax1.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax1.transAxes)  # Adjust placement and size of the inset axes (where the QR code is placed) relative to the main axes (`ax`). They are normalized coordinates with values ranging from 0 to 1: [x-coordinate of lower left corner of inset axes, y-coordinate of lower left corner of inset axes, width of inset axes, height of inset axes]\n",
    "inset_ax1.imshow(qr_codes[0], aspect='equal', cmap='gray')  # 'equal' keeps the aspect ratio of the QR code\n",
    "inset_ax1.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "inset_ax2 = ax2.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax2.transAxes)\n",
    "inset_ax2.imshow(qr_codes[1], aspect='equal', cmap='gray')\n",
    "inset_ax2.axis('off')\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "plt.imshow(test_input[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "inset_ax3 = ax3.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax3.transAxes)\n",
    "inset_ax3.imshow(qr_codes[2], aspect='equal', cmap='gray')\n",
    "inset_ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig('plots/reconstructed_spectrogram_speech.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Print information about the audio\n",
    "print(\"Time of last sample:\", time[-1])\n",
    "print(\"Number of samples:\", len(audio_signal))\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(\"Duration of audio:\", len(audio_signal) / sample_rate)\n",
    "print(\"Shape of audio signal:\", audio_signal.shape)\n",
    "print(\"Type of audio signal:\", type(audio_signal))\n",
    "print(\"Data type of audio signal:\", audio_signal.dtype)\n",
    "\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Type of audio tensor:\", type(audio_tensor))\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "test_input = fbank_features\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {test_input.shape}\")\n",
    "\n",
    "# # duplicate input tensor to get a batch of 2\n",
    "# test_input = torch.cat((test_input, test_input), 0)\n",
    "# print(f\"Shape of dublicated fbank features: {test_input.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "model1 = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model2 = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model1 = torch.nn.DataParallel(model1)\n",
    "model2 = torch.nn.DataParallel(model2)\n",
    "model1.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model2.load_state_dict(torch.load('../pretraining/exp/pretrained-20240429-112534-shuffled-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.54.pth'))\n",
    "model1 = model1.module\n",
    "model2 = model2.module\n",
    "model1.to('cpu')\n",
    "model2.to('cpu')\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "\n",
    "hop_width = 80\n",
    "hop_length = 250 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - hop_width//2, hop_length)\n",
    "print(hops)\n",
    "mask_indices = [range(i-hop_width//2, i + hop_width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(mask_indices)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(test_input)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = test_input * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram1 = model1(test_input, task='visualize_mask', mask_indices=mask_indices)\n",
    "    reconstructed_spectrogram2 = model2(test_input, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# Assuming 'test_input' is your spectrogram tensor\n",
    "n_timesteps = test_input.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Now plotting all three: original input, masked input, and reconstructed output\n",
    "plt.figure(figsize=(6.2, 4.4))\n",
    "\n",
    "urls = [\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/masked_speech_longmask.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_speech_OSModel_longmask.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_speech_SUModel_longmask.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/original_speech.wav'\n",
    "]\n",
    "\n",
    "# Generate QR codes and convert them to numpy arrays\n",
    "qr_codes = []\n",
    "for url in urls:\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "        box_size=10,  # Reduced box size for fitting within plot\n",
    "        border=1,  # Minimal border\n",
    "    )\n",
    "    qr.add_data(url)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image(fill_color='black', back_color='white').convert('L')\n",
    "    qr_codes.append(np.array(img))\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = test_input.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# Function to add rectangles to the axes\n",
    "def add_rectangles(ax, hops, patch_width, patch_height):\n",
    "    for mid_index in hops:\n",
    "        # Calculate the left corner of the rectangle\n",
    "        left_corner = 2*mid_index - patch_width\n",
    "        # Create a rectangle patch with gray color\n",
    "        rect = patches.Rectangle((left_corner, 0), 2*patch_width, patch_height, linewidth=1, edgecolor='lightgray', facecolor='lightgray')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        # Add 'MASK' text at the center of the rectangle\n",
    "        ax.text(left_corner + patch_width, patch_height / 2, r'\\textbf{MASK}', horizontalalignment='center', verticalalignment='center', color='black', fontsize=6, rotation=0)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(4, 1, 1)\n",
    "plt.imshow(masked_spectrogram[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "add_rectangles(ax1, hops, hop_width, patch_height)\n",
    "plt.title('Masked Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "inset_ax1 = ax1.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax1.transAxes)  # Adjust placement and size of the inset axes (where the QR code is placed) relative to the main axes (`ax`). They are normalized coordinates with values ranging from 0 to 1: [x-coordinate of lower left corner of inset axes, y-coordinate of lower left corner of inset axes, width of inset axes, height of inset axes]\n",
    "inset_ax1.imshow(qr_codes[0], aspect='equal', cmap='gray')  # 'equal' keeps the aspect ratio of the QR code\n",
    "inset_ax1.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "ax2 = plt.subplot(4, 1, 2)\n",
    "plt.imshow(reconstructed_spectrogram2[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram from Shuffled Model')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "inset_ax2 = ax2.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax2.transAxes)\n",
    "inset_ax2.imshow(qr_codes[1], aspect='equal', cmap='gray')\n",
    "inset_ax2.axis('off')\n",
    "\n",
    "ax3 = plt.subplot(4, 1, 3)\n",
    "plt.imshow(reconstructed_spectrogram1[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Spectrogram from Original Model')\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.yticks(y_ticks)\n",
    "inset_ax3 = ax3.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax3.transAxes)\n",
    "inset_ax3.imshow(qr_codes[1], aspect='equal', cmap='gray')\n",
    "inset_ax3.axis('off')\n",
    "\n",
    "ax4 = plt.subplot(4, 1, 4)\n",
    "plt.imshow(test_input[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.xticks(x_ticks, x_labels) \n",
    "plt.yticks(y_ticks)\n",
    "inset_ax4 = ax4.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax4.transAxes)\n",
    "inset_ax4.imshow(qr_codes[2], aspect='equal', cmap='gray')\n",
    "inset_ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig('plots/reconstructed_spectrogram_speech_longmask.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the reconstruction at various stages of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "#file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed_eval/1a281f7d-bde7-4744-9220-7ea599b6e093/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "test_input = fbank_features\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "\n",
    "width = 20\n",
    "hop_length = 50 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - width//2, hop_length)\n",
    "mask_indices = [range(i-width//2, i + width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(\"length of mask indices:\", len(mask_indices))\n",
    "print(f\"this corresponds to a time duration of {len(mask_indices)*2*10/1000} seconds\")\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(test_input)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = test_input * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Assuming 'test_input' is your spectrogram tensor\n",
    "n_timesteps = test_input.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    return ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "\n",
    "def load_model(checkpoint_id):\n",
    "    model = initialize_model()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(f'../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.{checkpoint_id}.pth'))\n",
    "    model = model.module\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "models = {i: load_model(i) for i in [1, 2, 6, 40, 108]}\n",
    "\n",
    "# Generate spectrograms\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrograms = {i: model(test_input, task='visualize_mask', mask_indices=mask_indices) for i, model in models.items()}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6.2, 7.7))\n",
    "\n",
    "urls = [\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/masked_music.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_music_1.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_music_2.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_music_6.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_music_40.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_music_108.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/original_music.wav'\n",
    "]\n",
    "\n",
    "# Generate QR codes and convert them to numpy arrays\n",
    "qr_codes = []\n",
    "for url in urls:\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "        box_size=10,  # Reduced box size for fitting within plot\n",
    "        border=1,  # Minimal border\n",
    "    )\n",
    "    qr.add_data(url)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image(fill_color='black', back_color='white').convert('L')\n",
    "    qr_codes.append(np.array(img))\n",
    " \n",
    "\n",
    "def plot_spectrogram(data, title, position):         \n",
    "    plt.subplot(7, 1, position)\n",
    "    plt.imshow(data[0].cpu().numpy().T, aspect='auto', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_ticks, x_labels)\n",
    "    plt.yticks(y_ticks)\n",
    "    #plt.xlabel('Time')\n",
    "    #plt.ylabel('Mel Frequency Bins')\n",
    "\n",
    "    # Place QR code within the same subplot on the right side\n",
    "    # Inset axes: position, size of the inset axes within the main axes\n",
    "    ax = plt.gca()\n",
    "    inset_ax = ax.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax.transAxes)  # Adjust placement and size of the inset axes (where the QR code is placed) relative to the main axes (`ax`). They are normalized coordinates with values ranging from 0 to 1: [x-coordinate of lower left corner of inset axes, y-coordinate of lower left corner of inset axes, width of inset axes, height of inset axes]\n",
    "    inset_ax.imshow(qr_codes[position-1], aspect='equal', cmap='gray')  # 'equal' keeps the aspect ratio of the QR code\n",
    "    inset_ax.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "def add_red_rectangles(ax, hops, patch_width, patch_height):\n",
    "    for mid_index in hops:\n",
    "        # Calculate the left corner of the rectangle\n",
    "        left_corner = 2*mid_index - patch_width\n",
    "        # Create a rectangle with no fill\n",
    "        rect = patches.Rectangle((left_corner, 0), 2*patch_width, patch_height, linewidth=0.6, edgecolor='red', facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "def add_gray_mask(ax, hops, patch_width, patch_height):\n",
    "    for mid_index in hops:\n",
    "        # Calculate the left corner of the rectangle\n",
    "        left_corner = 2*mid_index - patch_width\n",
    "        # Create a rectangle patch with gray color\n",
    "        rect = patches.Rectangle((left_corner, 0), 2*patch_width, patch_height, linewidth=1, edgecolor='lightgray', facecolor='lightgray')  # semi-transparent\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add 'MASK' text at the center of the rectangle\n",
    "        ax.text(left_corner + patch_width, patch_height / 2, r'\\textbf{MASK}', horizontalalignment='center', verticalalignment='center', color='black', fontsize=6, rotation=90)\n",
    "\n",
    "\n",
    "# masked spectrogram\n",
    "plot_spectrogram(masked_spectrogram, 'Masked Spectrogram', 1)\n",
    "add_gray_mask(plt.gca(), hops, width, test_input.shape[2])\n",
    "\n",
    "# reconstructed spectrograms\n",
    "for index, checkpoint in enumerate(sorted(reconstructed_spectrograms.keys())):\n",
    "    plot_spectrogram(reconstructed_spectrograms[checkpoint], f'Reconstructed Spectrogram (after {(checkpoint-1)*4}k Iterations)', index + 2)\n",
    "    add_red_rectangles(plt.gca(), hops, width, test_input.shape[2])\n",
    "\n",
    "# original spectrogram\n",
    "plot_spectrogram(test_input, 'Original Spectrogram', 7)\n",
    "#add_red_rectangles(plt.gca(), hops, width, test_input.shape[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "directory = 'plots'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig('plots/reconstructed_spectrogram_music.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize masked speech with qr codes to listen to the audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model trained on original spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.108.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# use random indices in the range [0, 498], but using a seed for reproducibility\n",
    "np.random.seed(15)\n",
    "mask_indices = np.random.choice(499, 370, replace=False)\n",
    "\n",
    "# turn indices from model basis [0, 498] to spectrogram basis [0, 997]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# save tensor for later\n",
    "fbank_features_torch = fbank_features\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram = reconstructed_spectrogram[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram = reconstructed_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "# print shapes\n",
    "print(\"masked_spectrogram shape:\", masked_spectrogram.shape)\n",
    "print(\"reconstructed_spectrogram shape:\", reconstructed_spectrogram.shape)\n",
    "print(\"fbank_features shape:\", fbank_features.shape)\n",
    "\n",
    "def add_gray_masks(ax, expanded_mask_indices, patch_height):\n",
    "    print(\"expanded_mask_indices:\", expanded_mask_indices)\n",
    "\n",
    "    # Group adjacent indices\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for index in sorted(expanded_mask_indices):\n",
    "        if not current_group or index == current_group[-1] + 1:\n",
    "            current_group.append(index)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [index]\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "\n",
    "    # For each group of indices that are adjacent, create a rectangle patch\n",
    "    for group in groups:\n",
    "        left_corner = group[0] - 0.5\n",
    "        width = len(group)\n",
    "        rect = patches.Rectangle((left_corner, 0), width=width, height=1.0*patch_height, linewidth=0.0, edgecolor='lightgray', facecolor='lightgray', alpha=1.0)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "# Example URLs for audio files (replace with actual URLs)\n",
    "urls = [\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/masked_speech_random_masks_originalModel.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_speech_random_masks_originalModel.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/original_speech_random_masks_originalModel.wav'\n",
    "]\n",
    "\n",
    "# Generate QR codes and convert them to numpy arrays\n",
    "qr_codes = []\n",
    "for url in urls:\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "        box_size=10,  # Reduced box size for fitting within plot\n",
    "        border=1,  # Minimal border\n",
    "    )\n",
    "    qr.add_data(url)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image(fill_color='black', back_color='white').convert('L')\n",
    "    qr_codes.append(np.array(img))\n",
    "\n",
    "# Titles for each subplot\n",
    "titles = ['Masked Spectrogram', 'Reconstructed Spectrogram', 'Original Spectrogram']\n",
    "\n",
    "# Plotting spectrograms with QR codes\n",
    "fig, axs = plt.subplots(3, 1, figsize=(6.2, 3.3))\n",
    "\n",
    "spectrograms = [masked_spectrogram, reconstructed_spectrogram, fbank_features]\n",
    "\n",
    "for i, (ax, spectrogram) in enumerate(zip(axs, spectrograms)):\n",
    "    im = ax.imshow(spectrogram, aspect='auto', origin='lower')\n",
    "    if i == 0:\n",
    "        add_gray_masks(ax, expanded_mask_indices, fbank_features.shape[0])\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    \n",
    "    # Place QR code within the same subplot on the right side\n",
    "    # Inset axes: position, size of the inset axes within the main axes\n",
    "    inset_ax = ax.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax.transAxes)  # Adjust placement and size of the inset axes (where the QR code is placed) relative to the main axes (`ax`). They are normalized coordinates with values ranging from 0 to 1: [x-coordinate of lower left corner of inset axes, y-coordinate of lower left corner of inset axes, width of inset axes, height of inset axes]\n",
    "    inset_ax.imshow(qr_codes[i], aspect='equal', cmap='gray')  # 'equal' keeps the aspect ratio of the QR code\n",
    "    inset_ax.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "    # # Add text to the right of the spectrogram\n",
    "    # inset_ax_text = ax.inset_axes([1.0, 0.0, 0.07, 0.65], transform=ax.transAxes)\n",
    "    # inset_ax_text.text(0.5, 0.5, r'\\noindent $\\leftarrow$ Listen\\\\to it here', fontsize=10, rotation=-90, transform=inset_ax_text.transAxes, verticalalignment='center', horizontalalignment='center', color='black')\n",
    "    # inset_ax_text.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])  # Adjust the rect to prevent QR codes from being cut off: [left boundary of layout box as fraction of figure width, bottom boundary of \", right boundary of \", top boundary of \"]\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "plt.savefig('plots/reconstructed_spectrogram_speech_originalModel_with_qr.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model trained on shuffled spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "directory = 'audios'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-5.0716844 \n",
    "dataset_std=4.386603\n",
    "fbank_features = (fbank_features - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "fbank_features = fbank_features.unsqueeze(0)\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('../pretraining/exp/pretrained-20240429-112534-shuffled-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.54.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# make 250 random indices in the range [0, 499], but using a seed for reproducibility\n",
    "np.random.seed(15)\n",
    "mask_indices = np.random.choice(499, 370, replace=False)\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(fbank_features)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = fbank_features * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    reconstructed_spectrogram = model(fbank_features, task='visualize_mask', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(fbank_features.shape)\n",
    "print(reconstructed_spectrogram.shape)\n",
    "\n",
    "n_timesteps = fbank_features.shape[1]  # 998\n",
    "time_per_step = 10 / 1000  # Example: if each step represents 10 ms (adjust based on your actual data setup)\n",
    "\n",
    "# Creating time labels for every 100 steps (1 second if each step is 10 ms)\n",
    "# round timesteps to next 100 and get the range\n",
    "x_ticks = np.arange(0, n_timesteps, 100)\n",
    "last_tick = int(np.ceil(n_timesteps / 100) * 100)\n",
    "x_ticks = np.append(x_ticks, last_tick)\n",
    " # formatting time labels as strings in seconds\n",
    "x_labels = [f\"{x * time_per_step:.1f}s\" for x in x_ticks]\n",
    "x_ticks[-1] = n_timesteps\n",
    "\n",
    "print(\"x_ticks:\", x_ticks)\n",
    "print(\"x_labels:\", x_labels)\n",
    "\n",
    "\n",
    "# y-ticks for the frequency axis\n",
    "y_ticks = np.arange(0, 128, 25)\n",
    "\n",
    "# Define the patch width and height\n",
    "patch_height = fbank_features.shape[2]  # This is the vertical height of your spectrogram\n",
    "\n",
    "# save tensor for later\n",
    "fbank_features_torch = fbank_features\n",
    "\n",
    "# create numpy arrays\n",
    "masked_spectrogram = masked_spectrogram[0].cpu().numpy().T\n",
    "reconstructed_spectrogram = reconstructed_spectrogram[0].cpu().numpy().T\n",
    "fbank_features = fbank_features[0].cpu().numpy().T\n",
    "\n",
    "# unnormalize\n",
    "masked_spectrogram = masked_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "reconstructed_spectrogram = reconstructed_spectrogram * (2 * dataset_std) + dataset_mean\n",
    "fbank_features = fbank_features * (2 * dataset_std) + dataset_mean\n",
    "\n",
    "# print shapes\n",
    "print(\"masked_spectrogram shape:\", masked_spectrogram.shape)\n",
    "print(\"reconstructed_spectrogram shape:\", reconstructed_spectrogram.shape)\n",
    "print(\"fbank_features shape:\", fbank_features.shape)\n",
    "\n",
    "def add_gray_masks(ax, expanded_mask_indices, patch_height):\n",
    "    print(\"expanded_mask_indices:\", expanded_mask_indices)\n",
    "\n",
    "    # Group adjacent indices\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for index in sorted(expanded_mask_indices):\n",
    "        if not current_group or index == current_group[-1] + 1:\n",
    "            current_group.append(index)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [index]\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "\n",
    "    # For each group of indices that are adjacent, create a rectangle patch\n",
    "    for group in groups:\n",
    "        left_corner = group[0] - 0.5\n",
    "        width = len(group)\n",
    "        rect = patches.Rectangle((left_corner, 0), width=width, height=1.0*patch_height, linewidth=0.0, edgecolor='lightgray', facecolor='lightgray', alpha=1.0)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "\n",
    "# Example URLs for audio files (replace with actual URLs)\n",
    "urls = [\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/masked_speech_random_masks_shuffledModel.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/reconstructed_speech_random_masks_shuffledModel.wav',\n",
    "    'https://github.com/Ironmomo/SpeakerVerificationBA/raw/master/plots_and_audios/audios/original_speech_random_masks_shuffledModel.wav'\n",
    "]\n",
    "\n",
    "# Generate QR codes and convert them to numpy arrays\n",
    "qr_codes = []\n",
    "for url in urls:\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "        box_size=10,  # Reduced box size for fitting within plot\n",
    "        border=1,  # Minimal border\n",
    "    )\n",
    "    qr.add_data(url)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image(fill_color='black', back_color='white').convert('L')\n",
    "    qr_codes.append(np.array(img))\n",
    "\n",
    "# Titles for each subplot\n",
    "titles = ['Masked Spectrogram', 'Reconstructed Spectrogram', 'Original Spectrogram']\n",
    "\n",
    "# Plotting spectrograms with QR codes\n",
    "fig, axs = plt.subplots(3, 1, figsize=(6.2, 3.3))\n",
    "\n",
    "spectrograms = [masked_spectrogram, reconstructed_spectrogram, fbank_features]\n",
    "\n",
    "for i, (ax, spectrogram) in enumerate(zip(axs, spectrograms)):\n",
    "    im = ax.imshow(spectrogram, aspect='auto', origin='lower')\n",
    "    if i == 0:\n",
    "        add_gray_masks(ax, expanded_mask_indices, fbank_features.shape[0])\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    \n",
    "    # Place QR code within the same subplot on the right side\n",
    "    # Inset axes: position, size of the inset axes within the main axes\n",
    "    inset_ax = ax.inset_axes([1.01, 0.0, 0.13, 1.0], transform=ax.transAxes)  # Adjust placement and size of the inset axes (where the QR code is placed) relative to the main axes (`ax`). They are normalized coordinates with values ranging from 0 to 1: [x-coordinate of lower left corner of inset axes, y-coordinate of lower left corner of inset axes, width of inset axes, height of inset axes]\n",
    "    inset_ax.imshow(qr_codes[i], aspect='equal', cmap='gray')  # 'equal' keeps the aspect ratio of the QR code\n",
    "    inset_ax.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "    # # Add text to the right of the spectrogram\n",
    "    # inset_ax_text = ax.inset_axes([1.0, 0.0, 0.07, 0.65], transform=ax.transAxes)\n",
    "    # inset_ax_text.text(0.5, 0.5, r'\\noindent $\\leftarrow$ Listen\\\\to it here', fontsize=10, rotation=-90, transform=inset_ax_text.transAxes, verticalalignment='center', horizontalalignment='center', color='black')\n",
    "    # inset_ax_text.axis('off')  # Hide axis for cleaner look\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])  # Adjust the rect to prevent QR codes from being cut off: [left boundary of layout box as fraction of figure width, bottom boundary of \", right boundary of \", top boundary of \"]\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "plt.savefig('plots/reconstructed_spectrogram_speech_shuffledModel_with_qr.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if it is actually the mean of the unmasked frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "expanded_unmasked_indices = []\n",
    "for idx in range(499):\n",
    "    if idx not in mask_indices:\n",
    "        expanded_unmasked_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "    \n",
    "print(\"expanded_mask_indices length:\", len(expanded_mask_indices))\n",
    "print(\"expanded_mask_indices:\", expanded_mask_indices)\n",
    "print(\"expanded_unmasked_indices length:\", len(expanded_unmasked_indices))\n",
    "print(\"expanded_unmasked_indices:\", expanded_unmasked_indices)\n",
    "\n",
    "# create spectrogram with only the unmasked frames\n",
    "unmasked_frames = np.zeros((128, len(expanded_unmasked_indices)))\n",
    "print(unmasked_frames.shape)\n",
    "\n",
    "for i, idx in enumerate(expanded_unmasked_indices):\n",
    "    unmasked_frames[:, i] = fbank_features[:, idx]\n",
    "\n",
    "# plot the unmasked frames\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.imshow(unmasked_frames, aspect='auto', origin='lower')\n",
    "plt.title('Unmasked Frames')\n",
    "plt.show()\n",
    "\n",
    "# calculate mean of frames and aggreagate into one 128x1 vector\n",
    "mean_frames = np.mean(unmasked_frames, axis=1)\n",
    "print(mean_frames.shape)\n",
    "\n",
    "# randomly select 5 of the expanded mask indices with a seed for reproducibility\n",
    "np.random.seed(25)\n",
    "random_indices = np.random.choice(expanded_mask_indices, 6, replace=False)\n",
    "print(\"random_indices:\", random_indices)\n",
    "\n",
    "plt.figure(figsize=(6.2, 3))\n",
    "plt.plot(mean_frames, label='Mean of Unmasked Frames', linewidth=2)\n",
    "\n",
    "for idx in random_indices:\n",
    "    plt.plot(reconstructed_spectrogram[:, idx], label=f'Predicted Frame {idx}', linestyle='dotted', linewidth=1.0)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Predictions of the Model Trained on Shuffled Spectrograms')\n",
    "plt.xlabel('Mel Frequency Bins')\n",
    "plt.ylabel(r'$\\log_e(\\text{Fbank})$')  # Use raw string for LaTeX\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('plots/predictedFrames_shuffled_spectrogram.pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect the attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
