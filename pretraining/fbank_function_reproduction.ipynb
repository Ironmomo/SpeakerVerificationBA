{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## own function for fbank (to ensure I understand the process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# call with fewer arguments\n",
    "fbank_features_few = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "\n",
    "# compare the two\n",
    "print(f\"Are the two fbank features equal? {torch.allclose(fbank_features, fbank_features_few)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### own function for fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# numeric_limits<float>::epsilon() 1.1920928955078125e-07\n",
    "EPSILON = torch.tensor(torch.finfo(torch.float).eps)\n",
    "# 1 milliseconds = 0.001 seconds\n",
    "MILLISECONDS_TO_SECONDS = 0.001\n",
    "\n",
    "\n",
    "def _get_epsilon(device, dtype):\n",
    "    return EPSILON.to(device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def _next_power_of_2(x: int) -> int:\n",
    "    r\"\"\"Returns the smallest power of 2 that is greater than x\"\"\"\n",
    "    return 1 if x == 0 else 2 ** (x - 1).bit_length()\n",
    "\n",
    "\n",
    "def _get_strided(waveform: Tensor, window_size: int, window_shift: int, snip_edges: bool) -> Tensor:\n",
    "    r\"\"\"Given a waveform (1D tensor of size ``num_samples``), it returns a 2D tensor (m, ``window_size``)\n",
    "    representing how the window is shifted along the waveform. Each row is a frame.\n",
    "\n",
    "    Args:\n",
    "        waveform (Tensor): Tensor of size ``num_samples``\n",
    "        window_size (int): Frame length\n",
    "        window_shift (int): Frame shift\n",
    "        snip_edges (bool): If True, end effects will be handled by outputting only frames that completely fit\n",
    "            in the file, and the number of frames depends on the frame_length.  If False, the number of frames\n",
    "            depends only on the frame_shift, and we reflect the data at the ends.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: 2D tensor of size (m, ``window_size``) where each row is a frame\n",
    "    \"\"\"\n",
    "    assert waveform.dim() == 1\n",
    "    num_samples = waveform.size(0)\n",
    "    strides = (window_shift * waveform.stride(0), waveform.stride(0))\n",
    "\n",
    "    if snip_edges:\n",
    "        if num_samples < window_size:\n",
    "            return torch.empty((0, 0), dtype=waveform.dtype, device=waveform.device)\n",
    "        else:\n",
    "            m = 1 + (num_samples - window_size) // window_shift\n",
    "    else:\n",
    "        reversed_waveform = torch.flip(waveform, [0])\n",
    "        m = (num_samples + (window_shift // 2)) // window_shift\n",
    "        pad = window_size // 2 - window_shift // 2\n",
    "        pad_right = reversed_waveform\n",
    "        if pad > 0:\n",
    "            # torch.nn.functional.pad returns [2,1,0,1,2] for 'reflect'\n",
    "            # but we want [2, 1, 0, 0, 1, 2]\n",
    "            pad_left = reversed_waveform[-pad:]\n",
    "            waveform = torch.cat((pad_left, waveform, pad_right), dim=0)\n",
    "        else:\n",
    "            # pad is negative so we want to trim the waveform at the front\n",
    "            waveform = torch.cat((waveform[-pad:], pad_right), dim=0)\n",
    "\n",
    "    sizes = (m, window_size)\n",
    "    return waveform.as_strided(sizes, strides)\n",
    "\n",
    "\n",
    "def _feature_window_function(\n",
    "    window_size: int,\n",
    "    device: torch.device,\n",
    "    dtype: int,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Returns a window function with the given type and size\"\"\"\n",
    "    return torch.hann_window(window_size, periodic=False, device=device, dtype=dtype)\n",
    "\n",
    "def _get_log_energy(strided_input: Tensor, epsilon: Tensor, energy_floor: float) -> Tensor:\n",
    "    r\"\"\"Returns the log energy of size (m) for a strided_input (m,*)\"\"\"\n",
    "    device, dtype = strided_input.device, strided_input.dtype\n",
    "    log_energy = torch.max(strided_input.pow(2).sum(1), epsilon).log()  # size (m)\n",
    "    if energy_floor == 0.0:\n",
    "        return log_energy\n",
    "    return torch.max(log_energy, torch.tensor(math.log(energy_floor), device=device, dtype=dtype))\n",
    "\n",
    "\n",
    "def _get_waveform_and_window_properties(\n",
    "    waveform: Tensor,\n",
    "    channel: int,\n",
    "    sample_frequency: float,\n",
    "    frame_shift: float,\n",
    "    frame_length: float,\n",
    "    round_to_power_of_two: bool,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, int, int, int]:\n",
    "    r\"\"\"Gets the waveform and window properties\"\"\"\n",
    "    channel = max(channel, 0)\n",
    "    assert channel < waveform.size(0), \"Invalid channel {} for size {}\".format(channel, waveform.size(0))\n",
    "    waveform = waveform[channel, :]  # size (n)\n",
    "    window_shift = int(sample_frequency * frame_shift * MILLISECONDS_TO_SECONDS)\n",
    "    window_size = int(sample_frequency * frame_length * MILLISECONDS_TO_SECONDS)\n",
    "    padded_window_size = _next_power_of_2(window_size) if round_to_power_of_two else window_size\n",
    "\n",
    "    assert 2 <= window_size <= len(waveform), \"choose a window size {} that is [2, {}]\".format(\n",
    "        window_size, len(waveform)\n",
    "    )\n",
    "    assert 0 < window_shift, \"`window_shift` must be greater than 0\"\n",
    "    assert padded_window_size % 2 == 0, (\n",
    "        \"the padded `window_size` must be divisible by two.\" \" use `round_to_power_of_two` or change `frame_length`\"\n",
    "    )\n",
    "    assert 0.0 <= preemphasis_coefficient <= 1.0, \"`preemphasis_coefficient` must be between [0,1]\"\n",
    "    assert sample_frequency > 0, \"`sample_frequency` must be greater than zero\"\n",
    "    return waveform, window_shift, window_size, padded_window_size\n",
    "\n",
    "\n",
    "def _get_window(\n",
    "    waveform: Tensor,\n",
    "    padded_window_size: int,\n",
    "    window_size: int,\n",
    "    window_shift: int,\n",
    "    snip_edges: bool,\n",
    "    raw_energy: bool,\n",
    "    energy_floor: float,\n",
    "    remove_dc_offset: bool,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Gets a window and its log energy\n",
    "\n",
    "    Returns:\n",
    "        (Tensor, Tensor): strided_input of size (m, ``padded_window_size``) and signal_log_energy of size (m)\n",
    "    \"\"\"\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "    epsilon = _get_epsilon(device, dtype)\n",
    "\n",
    "    # size (m, window_size)\n",
    "    strided_input = _get_strided(waveform, window_size, window_shift, snip_edges)\n",
    "\n",
    "    if remove_dc_offset:\n",
    "        # Subtract each row/frame by its mean\n",
    "        row_means = torch.mean(strided_input, dim=1).unsqueeze(1)  # size (m, 1)\n",
    "        strided_input = strided_input - row_means\n",
    "\n",
    "    if raw_energy:\n",
    "        # Compute the log energy of each row/frame before applying preemphasis and\n",
    "        # window function\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    if preemphasis_coefficient != 0.0:\n",
    "        # strided_input[i,j] -= preemphasis_coefficient * strided_input[i, max(0, j-1)] for all i,j\n",
    "        offset_strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (1, 0), mode=\"replicate\").squeeze(\n",
    "            0\n",
    "        )  # size (m, window_size + 1)\n",
    "        strided_input = strided_input - preemphasis_coefficient * offset_strided_input[:, :-1]\n",
    "\n",
    "    # Apply window_function to each row/frame\n",
    "    window_function = _feature_window_function(window_size, device, dtype).unsqueeze(\n",
    "        0\n",
    "    )  # size (1, window_size)\n",
    "    strided_input = strided_input * window_function  # size (m, window_size)\n",
    "\n",
    "    # Pad columns with zero until we reach size (m, padded_window_size)\n",
    "    if padded_window_size != window_size:\n",
    "        padding_right = padded_window_size - window_size\n",
    "        strided_input = torch.nn.functional.pad(\n",
    "            strided_input.unsqueeze(0), (0, padding_right), mode=\"constant\", value=0\n",
    "        ).squeeze(0)\n",
    "\n",
    "    # Compute energy after window function (not the raw one)\n",
    "    if not raw_energy:\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    return strided_input, signal_log_energy\n",
    "\n",
    "\n",
    "def _subtract_column_mean(tensor: Tensor, subtract_mean: bool) -> Tensor:\n",
    "    # subtracts the column mean of the tensor size (m, n) if subtract_mean=True\n",
    "    # it returns size (m, n)\n",
    "    if subtract_mean:\n",
    "        col_means = torch.mean(tensor, dim=0).unsqueeze(0)\n",
    "        tensor = tensor - col_means\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\n",
    "def inverse_mel_scale_scalar(mel_freq: float) -> float:\n",
    "    return 700.0 * (math.exp(mel_freq / 1127.0) - 1.0)\n",
    "\n",
    "\n",
    "def inverse_mel_scale(mel_freq: Tensor) -> Tensor:\n",
    "    return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)\n",
    "\n",
    "\n",
    "def mel_scale_scalar(freq: float) -> float:\n",
    "    return 1127.0 * math.log(1.0 + freq / 700.0)\n",
    "\n",
    "\n",
    "def mel_scale(freq: Tensor) -> Tensor:\n",
    "    return 1127.0 * (1.0 + freq / 700.0).log()\n",
    "\n",
    "\n",
    "def get_mel_banks(\n",
    "    num_bins: int,\n",
    "    window_length_padded: int,\n",
    "    sample_freq: float,\n",
    "    low_freq: float,\n",
    "    high_freq: float,\n",
    "    vtln_low: float,\n",
    "    vtln_high: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        (Tensor, Tensor): The tuple consists of ``bins`` (which is\n",
    "        melbank of size (``num_bins``, ``num_fft_bins``)) and ``center_freqs`` (which is\n",
    "        center frequencies of bins of size (``num_bins``)).\n",
    "    \"\"\"\n",
    "    assert num_bins > 3, \"Must have at least 3 mel bins\"\n",
    "    assert window_length_padded % 2 == 0\n",
    "    num_fft_bins = window_length_padded / 2\n",
    "    nyquist = 0.5 * sample_freq\n",
    "\n",
    "    if high_freq <= 0.0:\n",
    "        high_freq += nyquist\n",
    "\n",
    "    assert (\n",
    "        (0.0 <= low_freq < nyquist) and (0.0 < high_freq <= nyquist) and (low_freq < high_freq)\n",
    "    ), \"Bad values in options: low-freq {} and high-freq {} vs. nyquist {}\".format(low_freq, high_freq, nyquist)\n",
    "\n",
    "    # fft-bin width [think of it as Nyquist-freq / half-window-length]\n",
    "    fft_bin_width = sample_freq / window_length_padded\n",
    "    mel_low_freq = mel_scale_scalar(low_freq)\n",
    "    mel_high_freq = mel_scale_scalar(high_freq)\n",
    "\n",
    "    # divide by num_bins+1 in next line because of end-effects where the bins\n",
    "    # spread out to the sides.\n",
    "    mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "\n",
    "    if vtln_high < 0.0:\n",
    "        vtln_high += nyquist\n",
    "\n",
    "    bin = torch.arange(num_bins).unsqueeze(1)\n",
    "    left_mel = mel_low_freq + bin * mel_freq_delta  # size(num_bins, 1)\n",
    "    center_mel = mel_low_freq + (bin + 1.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "    right_mel = mel_low_freq + (bin + 2.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "\n",
    "    center_freqs = inverse_mel_scale(center_mel)  # size (num_bins)\n",
    "    # size(1, num_fft_bins)\n",
    "    mel = mel_scale(fft_bin_width * torch.arange(num_fft_bins)).unsqueeze(0)\n",
    "\n",
    "    # size (num_bins, num_fft_bins)\n",
    "    up_slope = (mel - left_mel) / (center_mel - left_mel)\n",
    "    down_slope = (right_mel - mel) / (right_mel - center_mel)\n",
    "\n",
    "    # left_mel < center_mel < right_mel so we can min the two slopes and clamp negative values\n",
    "    bins = torch.max(torch.zeros(1), torch.min(up_slope, down_slope))\n",
    "    \n",
    "    return bins, center_freqs\n",
    "\n",
    "\n",
    "def fbank_own(\n",
    "    waveform: Tensor,\n",
    "    channel: int = -1,\n",
    "    energy_floor: float = 1.0,\n",
    "    frame_length: float = 25.0,\n",
    "    frame_shift: float = 10.0,\n",
    "    high_freq: float = 0.0,\n",
    "    low_freq: float = 20.0,\n",
    "    min_duration: float = 0.0,\n",
    "    num_mel_bins: int = 23,\n",
    "    preemphasis_coefficient: float = 0.97,\n",
    "    raw_energy: bool = True,\n",
    "    remove_dc_offset: bool = True,\n",
    "    round_to_power_of_two: bool = True,\n",
    "    sample_frequency: float = 16000.0,\n",
    "    snip_edges: bool = True,\n",
    "    subtract_mean: bool = False,\n",
    "    use_log_fbank: bool = True,\n",
    "    use_power: bool = True,\n",
    "    vtln_high: float = -500.0,\n",
    "    vtln_low: float = 100.0,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Create a fbank from a raw audio signal. This matches the input/output of Kaldi's\n",
    "    compute-fbank-feats.\n",
    "\n",
    "    Args:\n",
    "        waveform (Tensor): Tensor of audio of size (c, n) where c is in the range [0,2)\n",
    "        blackman_coeff (float, optional): Constant coefficient for generalized Blackman window. (Default: ``0.42``)\n",
    "        channel (int, optional): Channel to extract (-1 -> expect mono, 0 -> left, 1 -> right) (Default: ``-1``)\n",
    "        dither (float, optional): Dithering constant (0.0 means no dither). If you turn this off, you should set\n",
    "            the energy_floor option, e.g. to 1.0 or 0.1 (Default: ``0.0``)\n",
    "        energy_floor (float, optional): Floor on energy (absolute, not relative) in Spectrogram computation.  Caution:\n",
    "            this floor is applied to the zeroth component, representing the total signal energy.  The floor on the\n",
    "            individual spectrogram elements is fixed at std::numeric_limits<float>::epsilon(). (Default: ``1.0``)\n",
    "        frame_length (float, optional): Frame length in milliseconds (Default: ``25.0``)\n",
    "        frame_shift (float, optional): Frame shift in milliseconds (Default: ``10.0``)\n",
    "        high_freq (float, optional): High cutoff frequency for mel bins (if <= 0, offset from Nyquist)\n",
    "         (Default: ``0.0``)\n",
    "        htk_compat (bool, optional): If true, put energy last.  Warning: not sufficient to get HTK compatible features\n",
    "         (need to change other parameters). (Default: ``False``)\n",
    "        low_freq (float, optional): Low cutoff frequency for mel bins (Default: ``20.0``)\n",
    "        min_duration (float, optional): Minimum duration of segments to process (in seconds). (Default: ``0.0``)\n",
    "        num_mel_bins (int, optional): Number of triangular mel-frequency bins (Default: ``23``)\n",
    "        preemphasis_coefficient (float, optional): Coefficient for use in signal preemphasis (Default: ``0.97``)\n",
    "        raw_energy (bool, optional): If True, compute energy before preemphasis and windowing (Default: ``True``)\n",
    "        remove_dc_offset (bool, optional): Subtract mean from waveform on each frame (Default: ``True``)\n",
    "        round_to_power_of_two (bool, optional): If True, round window size to power of two by zero-padding input\n",
    "            to FFT. (Default: ``True``)\n",
    "        sample_frequency (float, optional): Waveform data sample frequency (must match the waveform file, if\n",
    "            specified there) (Default: ``16000.0``)\n",
    "        snip_edges (bool, optional): If True, end effects will be handled by outputting only frames that completely fit\n",
    "            in the file, and the number of frames depends on the frame_length.  If False, the number of frames\n",
    "            depends only on the frame_shift, and we reflect the data at the ends. (Default: ``True``)\n",
    "        subtract_mean (bool, optional): Subtract mean of each feature file [CMS]; not recommended to do\n",
    "            it this way.  (Default: ``False``)\n",
    "        use_energy (bool, optional): Add an extra dimension with energy to the FBANK output. (Default: ``False``)\n",
    "        use_log_fbank (bool, optional):If true, produce log-filterbank, else produce linear. (Default: ``True``)\n",
    "        use_power (bool, optional): If true, use power, else use magnitude. (Default: ``True``)\n",
    "        vtln_high (float, optional): High inflection point in piecewise linear VTLN warping function (if\n",
    "            negative, offset from high-mel-freq (Default: ``-500.0``)\n",
    "        vtln_low (float, optional): Low inflection point in piecewise linear VTLN warping function (Default: ``100.0``)\n",
    "        vtln_warp (float, optional): Vtln warp factor (only applicable if vtln_map not specified) (Default: ``1.0``)\n",
    "        window_type (str, optional): Type of window ('hamming'|'hanning'|'povey'|'rectangular'|'blackman')\n",
    "         (Default: ``'povey'``)\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A fbank identical to what Kaldi would output. The shape is (m, ``num_mel_bins + use_energy``)\n",
    "        where m is calculated in _get_strided\n",
    "    \"\"\"\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "    waveform, window_shift, window_size, padded_window_size = _get_waveform_and_window_properties(\n",
    "        waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient\n",
    "    )\n",
    "\n",
    "    if len(waveform) < min_duration * sample_frequency:\n",
    "        # signal is too short\n",
    "        return torch.empty(0, device=device, dtype=dtype)\n",
    "\n",
    "    # strided_input, size (m, padded_window_size) and signal_log_energy, size (m)\n",
    "    strided_input, signal_log_energy = _get_window(\n",
    "        waveform,\n",
    "        padded_window_size,\n",
    "        window_size,\n",
    "        window_shift,\n",
    "        snip_edges,\n",
    "        raw_energy,\n",
    "        energy_floor,\n",
    "        remove_dc_offset,\n",
    "        preemphasis_coefficient,\n",
    "    )\n",
    "\n",
    "    # size (m, padded_window_size // 2 + 1)\n",
    "    spectrum = torch.fft.rfft(strided_input).abs()\n",
    "    if use_power:\n",
    "        spectrum = spectrum.pow(2.0)\n",
    "\n",
    "    # size (num_mel_bins, padded_window_size // 2)\n",
    "    mel_energies, _ = get_mel_banks(\n",
    "        num_mel_bins, padded_window_size, sample_frequency, low_freq, high_freq, vtln_low, vtln_high\n",
    "    )\n",
    "    mel_energies = mel_energies.to(device=device, dtype=dtype)\n",
    "\n",
    "    # pad right column with zeros and add dimension, size (num_mel_bins, padded_window_size // 2 + 1)\n",
    "    mel_energies = torch.nn.functional.pad(mel_energies, (0, 1), mode=\"constant\", value=0)\n",
    "\n",
    "    # sum with mel fiterbanks over the power spectrum, size (m, num_mel_bins)\n",
    "    mel_energies = torch.mm(spectrum, mel_energies.T)\n",
    "    if use_log_fbank:\n",
    "        # avoid log of zero (which should be prevented anyway by dithering)\n",
    "        mel_energies = torch.max(mel_energies, _get_epsilon(device, dtype)).log()\n",
    "\n",
    "    mel_energies = _subtract_column_mean(mel_energies, subtract_mean)\n",
    "    return mel_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call fbank_own\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features_own = fbank_own(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    num_mel_bins=128, \n",
    "    frame_shift=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both\n",
    "\n",
    "# plot the fbank features from the own implementation\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_own.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Own fbank Features')\n",
    "\n",
    "# plot the fbank features\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Kaldi fbank Features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the 90th frame\n",
    "plt.plot(fbank_features[90, :], label='Kaldi')\n",
    "plt.plot(fbank_features_own[90, :], label='Own')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# numeric_limits<float>::epsilon() 1.1920928955078125e-07\n",
    "EPSILON = torch.tensor(torch.finfo(torch.float).eps)\n",
    "# 1 milliseconds = 0.001 seconds\n",
    "MILLISECONDS_TO_SECONDS = 0.001\n",
    "\n",
    "\n",
    "def _get_epsilon(device, dtype):\n",
    "    return EPSILON.to(device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def _next_power_of_2(x: int) -> int:\n",
    "    return 1 if x == 0 else 2 ** (x - 1).bit_length()\n",
    "\n",
    "\n",
    "def _get_strided(waveform: Tensor, window_size: int, window_shift: int, snip_edges: bool) -> Tensor:\n",
    "    assert waveform.dim() == 1\n",
    "    num_samples = waveform.size(0)\n",
    "    strides = (window_shift * waveform.stride(0), waveform.stride(0))\n",
    "\n",
    "    if snip_edges:\n",
    "        if num_samples < window_size:\n",
    "            return torch.empty((0, 0), dtype=waveform.dtype, device=waveform.device)\n",
    "        else:\n",
    "            m = 1 + (num_samples - window_size) // window_shift\n",
    "    else:\n",
    "        reversed_waveform = torch.flip(waveform, [0])\n",
    "        m = (num_samples + (window_shift // 2)) // window_shift\n",
    "        pad = window_size // 2 - window_shift // 2\n",
    "        pad_right = reversed_waveform\n",
    "        if pad > 0:\n",
    "            # torch.nn.functional.pad returns [2,1,0,1,2] for 'reflect'\n",
    "            # but we want [2, 1, 0, 0, 1, 2]\n",
    "            pad_left = reversed_waveform[-pad:]\n",
    "            waveform = torch.cat((pad_left, waveform, pad_right), dim=0)\n",
    "        else:\n",
    "            # pad is negative so we want to trim the waveform at the front\n",
    "            waveform = torch.cat((waveform[-pad:], pad_right), dim=0)\n",
    "\n",
    "    sizes = (m, window_size)\n",
    "    return waveform.as_strided(sizes, strides)\n",
    "\n",
    "\n",
    "def _feature_window_function(\n",
    "    window_size: int,\n",
    "    device: torch.device,\n",
    "    dtype: int,\n",
    ") -> Tensor:\n",
    "    return torch.hann_window(window_size, periodic=False, device=device, dtype=dtype)\n",
    "\n",
    "def _get_log_energy(strided_input: Tensor, epsilon: Tensor, energy_floor: float) -> Tensor:\n",
    "    device, dtype = strided_input.device, strided_input.dtype\n",
    "    log_energy = torch.max(strided_input.pow(2).sum(1), epsilon).log()  # size (m)\n",
    "    if energy_floor == 0.0:\n",
    "        return log_energy\n",
    "    return torch.max(log_energy, torch.tensor(math.log(energy_floor), device=device, dtype=dtype))\n",
    "\n",
    "\n",
    "def _get_waveform_and_window_properties(\n",
    "    waveform: Tensor,\n",
    "    channel: int,\n",
    "    sample_frequency: float,\n",
    "    frame_shift: float,\n",
    "    frame_length: float,\n",
    "    round_to_power_of_two: bool,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, int, int, int]:\n",
    "    r\"\"\"Gets the waveform and window properties\"\"\"\n",
    "    channel = max(channel, 0)\n",
    "    assert channel < waveform.size(0), \"Invalid channel {} for size {}\".format(channel, waveform.size(0))\n",
    "    waveform = waveform[channel, :]  # size (n)\n",
    "    window_shift = int(sample_frequency * frame_shift * MILLISECONDS_TO_SECONDS)\n",
    "    window_size = int(sample_frequency * frame_length * MILLISECONDS_TO_SECONDS)\n",
    "    padded_window_size = _next_power_of_2(window_size) if round_to_power_of_two else window_size\n",
    "\n",
    "    assert 2 <= window_size <= len(waveform), \"choose a window size {} that is [2, {}]\".format(\n",
    "        window_size, len(waveform)\n",
    "    )\n",
    "    assert 0 < window_shift, \"`window_shift` must be greater than 0\"\n",
    "    assert padded_window_size % 2 == 0, (\n",
    "        \"the padded `window_size` must be divisible by two.\" \" use `round_to_power_of_two` or change `frame_length`\"\n",
    "    )\n",
    "    assert 0.0 <= preemphasis_coefficient <= 1.0, \"`preemphasis_coefficient` must be between [0,1]\"\n",
    "    assert sample_frequency > 0, \"`sample_frequency` must be greater than zero\"\n",
    "    return waveform, window_shift, window_size, padded_window_size\n",
    "\n",
    "\n",
    "def _get_window(\n",
    "    waveform: Tensor,\n",
    "    padded_window_size: int,\n",
    "    window_size: int,\n",
    "    window_shift: int,\n",
    "    snip_edges: bool,\n",
    "    raw_energy: bool,\n",
    "    energy_floor: float,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "    epsilon = _get_epsilon(device, dtype)\n",
    "\n",
    "    # size (m, window_size)\n",
    "    strided_input = _get_strided(waveform, window_size, window_shift, snip_edges)\n",
    "\n",
    "    # Subtract each row/frame by its mean\n",
    "    row_means = torch.mean(strided_input, dim=1).unsqueeze(1)  # size (m, 1)\n",
    "    strided_input = strided_input - row_means\n",
    "\n",
    "    if raw_energy:\n",
    "        # Compute the log energy of each row/frame before applying preemphasis and window function\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    if preemphasis_coefficient != 0.0:\n",
    "        # strided_input[i,j] -= preemphasis_coefficient * strided_input[i, max(0, j-1)] for all i,j\n",
    "        offset_strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (1, 0), mode=\"replicate\").squeeze(\n",
    "            0\n",
    "        )  # size (m, window_size + 1)\n",
    "        strided_input = strided_input - preemphasis_coefficient * offset_strided_input[:, :-1]\n",
    "\n",
    "    # Apply window_function to each row/frame\n",
    "    window_function = _feature_window_function(window_size, device, dtype).unsqueeze(\n",
    "        0\n",
    "    )  # size (1, window_size)\n",
    "    strided_input = strided_input * window_function  # size (m, window_size)\n",
    "\n",
    "    # Pad columns with zero until we reach size (m, padded_window_size)\n",
    "    if padded_window_size != window_size:\n",
    "        padding_right = padded_window_size - window_size\n",
    "        strided_input = torch.nn.functional.pad(\n",
    "            strided_input.unsqueeze(0), (0, padding_right), mode=\"constant\", value=0\n",
    "        ).squeeze(0)\n",
    "\n",
    "    # Compute energy after window function (not the raw one)\n",
    "    if not raw_energy:\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    return strided_input, signal_log_energy\n",
    "\n",
    "\n",
    "def _subtract_column_mean(tensor: Tensor, subtract_mean: bool) -> Tensor:\n",
    "    # subtracts the column mean of the tensor size (m, n) if subtract_mean=True\n",
    "    # it returns size (m, n)\n",
    "    if subtract_mean:\n",
    "        col_means = torch.mean(tensor, dim=0).unsqueeze(0)\n",
    "        tensor = tensor - col_means\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\n",
    "def inverse_mel_scale_scalar(mel_freq: float) -> float:\n",
    "    return 700.0 * (math.exp(mel_freq / 1127.0) - 1.0)\n",
    "\n",
    "\n",
    "def inverse_mel_scale(mel_freq: Tensor) -> Tensor:\n",
    "    return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)\n",
    "\n",
    "\n",
    "def mel_scale_scalar(freq: float) -> float:\n",
    "    return 1127.0 * math.log(1.0 + freq / 700.0)\n",
    "\n",
    "\n",
    "def mel_scale(freq: Tensor) -> Tensor:\n",
    "    return 1127.0 * (1.0 + freq / 700.0).log()\n",
    "\n",
    "\n",
    "def get_mel_banks(\n",
    "    num_bins: int,\n",
    "    window_length_padded: int,\n",
    "    sample_freq: float,\n",
    "    low_freq: float,\n",
    "    high_freq: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \n",
    "    num_fft_bins = window_length_padded / 2\n",
    "    nyquist = 0.5 * sample_freq\n",
    "\n",
    "    if high_freq <= 0.0:\n",
    "        high_freq += nyquist\n",
    "\n",
    "    # fft-bin width [think of it as Nyquist-freq / half-window-length]\n",
    "    fft_bin_width = sample_freq / window_length_padded\n",
    "    mel_low_freq = mel_scale_scalar(low_freq)\n",
    "    mel_high_freq = mel_scale_scalar(high_freq)\n",
    "\n",
    "    # divide by num_bins+1 in next line because of end-effects where the bins\n",
    "    # spread out to the sides.\n",
    "    mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "\n",
    "    bin = torch.arange(num_bins).unsqueeze(1)\n",
    "    left_mel = mel_low_freq + bin * mel_freq_delta  # size(num_bins, 1)\n",
    "    center_mel = mel_low_freq + (bin + 1.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "    right_mel = mel_low_freq + (bin + 2.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "\n",
    "    center_freqs = inverse_mel_scale(center_mel)  # size (num_bins)\n",
    "    # size(1, num_fft_bins)\n",
    "    mel = mel_scale(fft_bin_width * torch.arange(num_fft_bins)).unsqueeze(0)\n",
    "\n",
    "    # size (num_bins, num_fft_bins)\n",
    "    up_slope = (mel - left_mel) / (center_mel - left_mel)\n",
    "    down_slope = (right_mel - mel) / (right_mel - center_mel)\n",
    "\n",
    "    # left_mel < center_mel < right_mel so we can min the two slopes and clamp negative values\n",
    "    bins = torch.max(torch.zeros(1), torch.min(up_slope, down_slope))\n",
    "    \n",
    "    return bins, center_freqs\n",
    "\n",
    "\n",
    "def fbank_own(\n",
    "    waveform: Tensor,\n",
    "    channel: int = -1,\n",
    "    energy_floor: float = 1.0,\n",
    "    frame_length: float = 25.0,\n",
    "    frame_shift: float = 10.0,\n",
    "    high_freq: float = 0.0,\n",
    "    low_freq: float = 20.0,\n",
    "    min_duration: float = 0.0,\n",
    "    num_mel_bins: int = 128,\n",
    "    preemphasis_coefficient: float = 0.97,\n",
    "    raw_energy: bool = True,\n",
    "    round_to_power_of_two: bool = True,\n",
    "    sample_frequency: float = 16000.0,\n",
    "    snip_edges: bool = True,\n",
    "    subtract_mean: bool = False,\n",
    "    use_log_fbank: bool = True,\n",
    "    use_power: bool = True,\n",
    ") -> Tensor:\n",
    "\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "    waveform, window_shift, window_size, padded_window_size = _get_waveform_and_window_properties(\n",
    "        waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient\n",
    "    )\n",
    "\n",
    "    if len(waveform) < min_duration * sample_frequency:\n",
    "        # signal is too short\n",
    "        return torch.empty(0, device=device, dtype=dtype)\n",
    "\n",
    "    # strided_input, size (m, padded_window_size) and signal_log_energy, size (m)\n",
    "    strided_input, signal_log_energy = _get_window(\n",
    "        waveform,\n",
    "        padded_window_size,\n",
    "        window_size,\n",
    "        window_shift,\n",
    "        snip_edges,\n",
    "        raw_energy,\n",
    "        energy_floor,\n",
    "        preemphasis_coefficient,\n",
    "    )\n",
    "\n",
    "    # size (m, padded_window_size // 2 + 1)\n",
    "    spectrum = torch.fft.rfft(strided_input).abs()\n",
    "    if use_power:\n",
    "        spectrum = spectrum.pow(2.0)\n",
    "\n",
    "    # size (num_mel_bins, padded_window_size // 2)\n",
    "    mel_energies, _ = get_mel_banks(\n",
    "        num_mel_bins, padded_window_size, sample_frequency, low_freq, high_freq\n",
    "    )\n",
    "    mel_energies = mel_energies.to(device=device, dtype=dtype)\n",
    "\n",
    "    # pad right column with zeros and add dimension, size (num_mel_bins, padded_window_size // 2 + 1)\n",
    "    mel_energies = torch.nn.functional.pad(mel_energies, (0, 1), mode=\"constant\", value=0)\n",
    "\n",
    "    # sum with mel fiterbanks over the power spectrum, size (m, num_mel_bins)\n",
    "    mel_energies = torch.mm(spectrum, mel_energies.T)\n",
    "    if use_log_fbank:\n",
    "        # avoid log of zero (which should be prevented anyway by dithering)\n",
    "        mel_energies = torch.max(mel_energies, _get_epsilon(device, dtype)).log()\n",
    "\n",
    "    mel_energies = _subtract_column_mean(mel_energies, subtract_mean)\n",
    "    return mel_energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def fbank_own(\n",
    "    waveform: Tensor,\n",
    ") -> Tensor:\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "    # shape is [c, n] (=[1, n] in case of mono) = [1, 160000] in our case\n",
    "    waveform = torch.squeeze(waveform)\n",
    "    # now shape is [n] = [160000] in our case\n",
    "\n",
    "\n",
    "    def get_window(\n",
    "        waveform: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "        strides = (160, 1)\n",
    "        sizes = (998, 400)\n",
    "\n",
    "        strided_input = waveform.as_strided(sizes, strides) # size (998, 400)\n",
    "\n",
    "        # Subtract each row/frame by its mean\n",
    "        row_means = torch.mean(strided_input, dim=1).unsqueeze(1)  # size (998, 1)\n",
    "        strided_input = strided_input - row_means # size (998, 400)\n",
    "\n",
    "        # strided_input[i,j] -= preemphasis_coefficient * strided_input[i, max(0, j-1)] for all i,j\n",
    "        offset_strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (1, 0), mode=\"replicate\").squeeze(0)  # size (998, 400 + 1)\n",
    "        strided_input = strided_input - 0.97 * offset_strided_input[:, :-1] # size (998, 400)\n",
    "\n",
    "        # Apply window_function to each row/frame\n",
    "        window_function = torch.hann_window(400, periodic=False, device=device, dtype=dtype).unsqueeze(0)  # size (1, 400)\n",
    "        strided_input = strided_input * window_function  # size (998, 400)\n",
    "\n",
    "        strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (0, 112), mode=\"constant\", value=0).squeeze(0) # 512 - 400 = 112\n",
    "\n",
    "        return strided_input\n",
    "\n",
    "    # strided_input, size (m, padded_window_size) and signal_log_energy, size (m)\n",
    "    strided_input = get_window(waveform) # size (998, 512)\n",
    "\n",
    "    # size (m, padded_window_size // 2 + 1)\n",
    "    spectrum = torch.fft.rfft(strided_input).abs() # size (998, 256 + 1)\n",
    "\n",
    "    spectrum = spectrum.pow(2.0)\n",
    "\n",
    "\n",
    "    def get_mel_banks(\n",
    "        num_bins: int\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "        def inverse_mel_scale_scalar(mel_freq: float) -> float:\n",
    "            return 700.0 * (math.exp(mel_freq / 1127.0) - 1.0)\n",
    "\n",
    "        def inverse_mel_scale(mel_freq: Tensor) -> Tensor:\n",
    "            return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)\n",
    "\n",
    "        def mel_scale_scalar(freq: float) -> float:\n",
    "            return 1127.0 * math.log(1.0 + freq / 700.0)\n",
    "\n",
    "        def mel_scale(freq: Tensor) -> Tensor:\n",
    "            return 1127.0 * (1.0 + freq / 700.0).log()\n",
    "        \n",
    "        num_fft_bins = 256 # window_length_padded / 2 = 512 / 2\n",
    "        nyquist_freq= 8000.0\n",
    "\n",
    "        low_freq = 20.0\n",
    "        high_freq = nyquist_freq\n",
    "\n",
    "        # fft-bin width [think of it as Nyquist-freq / half-window-length]\n",
    "        fft_bin_width = 31.25 # 16000 / window_length_padded = 16000 / 512\n",
    "        mel_low_freq = mel_scale_scalar(low_freq) # 31.748578341466644\n",
    "        mel_high_freq = mel_scale_scalar(high_freq) # 2840.0377117383778\n",
    "\n",
    "        # divide by num_bins+1 in next line because of end-effects where the bins spread out to the sides.\n",
    "        mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1) # 21.769683204627217\n",
    "\n",
    "        bin = torch.arange(num_bins).unsqueeze(1)\n",
    "        left_mel = mel_low_freq + bin * mel_freq_delta  # size(num_bins, 1) = (128, 1)\n",
    "        center_mel = mel_low_freq + (bin + 1.0) * mel_freq_delta  # size(num_bins, 1) = (128, 1)\n",
    "        right_mel = mel_low_freq + (bin + 2.0) * mel_freq_delta  # size(num_bins, 1) = (128, 1)\n",
    "\n",
    "        center_freqs = inverse_mel_scale(center_mel)  # size (num_bins) = (128)\n",
    "        mel = mel_scale(fft_bin_width * torch.arange(num_fft_bins)).unsqueeze(0) # size(1, num_fft_bins) = size (1, 256)\n",
    "\n",
    "        # size (num_bins, num_fft_bins)\n",
    "        up_slope = (mel - left_mel) / (center_mel - left_mel) # size (128, 256)\n",
    "        down_slope = (right_mel - mel) / (right_mel - center_mel) # size (128, 256)\n",
    "\n",
    "        # left_mel < center_mel < right_mel so we can min the two slopes and clamp negative values\n",
    "        bins = torch.max(torch.zeros(1), torch.min(up_slope, down_slope)) # size (128, 256)\n",
    "        \n",
    "        return bins\n",
    "\n",
    "    # size (num_mel_bins, padded_window_size // 2)\n",
    "    mel_energies = get_mel_banks(128) # torch.Size([128, 256])\n",
    "    mel_energies = mel_energies.to(device=device, dtype=dtype) # torch.Size([128, 256])\n",
    "\n",
    "    # pad right column with zeros and add dimension, size (num_mel_bins, padded_window_size // 2 + 1)\n",
    "    mel_energies = torch.nn.functional.pad(mel_energies, (0, 1), mode=\"constant\", value=0) # torch.Size([128, 257])\n",
    "\n",
    "    # sum with mel fiterbanks over the power spectrum, size (m, num_mel_bins)\n",
    "    mel_energies = torch.mm(spectrum, mel_energies.T) # (998, 256 + 1) x (257, 128) = torch.Size([998, 128])\n",
    "    \n",
    "    # avoid log of zero (which should be prevented anyway by dithering)\n",
    "    mel_energies = torch.max(mel_energies, torch.tensor(torch.finfo(torch.float).eps).to(device=device, dtype=dtype)).log() # torch.Size([998, 128])\n",
    "\n",
    "    return mel_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "# Data type of audio tensor: torch.float32\n",
    "# Shape of audio tensor: torch.Size([160000])\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor_batch = audio_tensor.unsqueeze(0)\n",
    "# Shape of audio tensor: torch.Size([1, 160000])\n",
    "\n",
    "\n",
    "# Call the fbank_own function\n",
    "fbank_features_own = fbank_own(\n",
    "    waveform=audio_tensor_batch,\n",
    ")\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features_torch = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor_batch, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "# Shape of fbank features: torch.Size([998, 128])\n",
    "\n",
    "# Assuming you have already read the audio file into `audio_signal` and it's a 1D array\n",
    "# Initial shape of audio signal: (160000,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot both\n",
    "\n",
    "# plot the fbank features from the own implementation\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_own.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Own fbank Features')\n",
    "\n",
    "# plot the fbank features\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_torch.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Kaldi fbank Features')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# compare the 90th frame\n",
    "plt.plot(fbank_features_torch[90, :], label='Kaldi')\n",
    "plt.plot(fbank_features_own[90, :], label='Own')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to numpy python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fbank_own(waveform, sample_rate=16000, num_mel_bins=128, frame_shift=10, frame_length=25):\n",
    "    # Waveform is now a 1D numpy array: shape = [160000]\n",
    "    \n",
    "    def get_window(waveform, sample_rate, frame_shift, frame_length):\n",
    "        # Stride and size configuration to simulate torch's as_strided\n",
    "        # Assuming waveform length is n = 160000, frame_shift = 10, frame_length = 25\n",
    "        signal_length = waveform.shape[0]\n",
    "        stride = sample_rate*frame_shift // 1000  # 160\n",
    "        window_size = sample_rate*frame_length // 1000  # 400\n",
    "        number_of_frames = (signal_length - window_size) // stride + 1  # 998 frames\n",
    "        \n",
    "        # Create an array of indices for each strided window\n",
    "        indices = np.lib.stride_tricks.as_strided(\n",
    "            np.arange(signal_length),\n",
    "            shape=(number_of_frames, window_size),\n",
    "            strides=(waveform.strides[0]*stride, waveform.strides[0])\n",
    "        )\n",
    "        strided_input = waveform[indices]  # shape = [998, 400]\n",
    "        \n",
    "        # Subtract each row/frame by its mean\n",
    "        row_means = np.mean(strided_input, axis=1, keepdims=True)  # shape = [998, 1]\n",
    "        strided_input -= row_means  # shape = [998, 400]\n",
    "        \n",
    "        # Pre-emphasis filtering\n",
    "        preemphasis_coefficient = 0.97\n",
    "        strided_input[:, 1:] -= preemphasis_coefficient * strided_input[:, :-1]\n",
    "        \n",
    "        # Apply Hanning window to each row/frame\n",
    "        window_function = np.hanning(window_size)  # shape = [400]\n",
    "        strided_input *= window_function  # shape = [998, 400]\n",
    "        \n",
    "        # Zero-pad each frame to the next power of two for FFT\n",
    "        padded_window_size = 1 if window_size == 0 else 2 ** (window_size - 1).bit_length()\n",
    "        strided_input = np.pad(strided_input, ((0, 0), (0, padded_window_size - window_size)), 'constant')  # shape = [998, 512]\n",
    "        \n",
    "        return strided_input, padded_window_size\n",
    "    \n",
    "    strided_input, padded_window_size = get_window(waveform, sample_rate, frame_shift, frame_length)  # shape = [998, 512]\n",
    "    \n",
    "    # Compute the power spectrum\n",
    "    spectrum = np.abs(np.fft.rfft(strided_input, n=padded_window_size))**2  # shape = [998, 257]\n",
    "\n",
    "    def get_mel_banks(num_bins, padded_window_size, sample_rate):\n",
    "        num_fft_bins = padded_window_size // 2\n",
    "        nyquist_freq = sample_rate / 2.0\n",
    "\n",
    "        low_freq = 20.0\n",
    "        high_freq = nyquist_freq\n",
    "\n",
    "        fft_bin_width = nyquist_freq / num_fft_bins\n",
    "        \n",
    "        # Mel scale conversion\n",
    "        def mel_scale(freq):\n",
    "            return 1127.0 * np.log(1.0 + freq / 700.0)\n",
    "        \n",
    "        def inverse_mel_scale(mel_freq):\n",
    "            return 700.0 * (np.exp(mel_freq / 1127.0) - 1.0)\n",
    "        \n",
    "        mel_low_freq = mel_scale(low_freq)\n",
    "        mel_high_freq = mel_scale(high_freq)\n",
    "        mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "        \n",
    "        mel_bins = np.zeros((num_bins, num_fft_bins + 1))\n",
    "        \n",
    "        for i in range(num_bins):\n",
    "            left_mel = mel_low_freq + i * mel_freq_delta\n",
    "            center_mel = left_mel + mel_freq_delta\n",
    "            right_mel = center_mel + mel_freq_delta\n",
    "            \n",
    "            for j in range(num_fft_bins + 1):\n",
    "                freq = j * fft_bin_width\n",
    "                mel_freq = mel_scale(freq)\n",
    "                \n",
    "                if left_mel < mel_freq < right_mel:\n",
    "                    if mel_freq <= center_mel:\n",
    "                        mel_bins[i, j] = (mel_freq - left_mel) / (center_mel - left_mel)\n",
    "                    else:\n",
    "                        mel_bins[i, j] = (right_mel - mel_freq) / (right_mel - center_mel)\n",
    "        \n",
    "        return mel_bins  # shape = [128, 257]\n",
    "\n",
    "    mel_energies = get_mel_banks(num_mel_bins, padded_window_size, sample_rate)  # shape = [128, 257]\n",
    "    \n",
    "    # Filter bank energies\n",
    "    filter_bank_energies = np.dot(spectrum, mel_energies.T)  # shape = [998, 128]\n",
    "    \n",
    "    # Log energies\n",
    "    filter_bank_energies = np.log(np.maximum(filter_bank_energies, 1.19209e-07))\n",
    "    \n",
    "    return filter_bank_energies  # shape = [998, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "# Data type of audio tensor: torch.float32\n",
    "# Shape of audio tensor: torch.Size([160000])\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor_batch = audio_tensor.unsqueeze(0)\n",
    "# Shape of audio tensor: torch.Size([1, 160000])\n",
    "\n",
    "\n",
    "# Call the fbank_own function\n",
    "fbank_features_own = fbank_own(\n",
    "    waveform=audio_signal, num_mel_bins=10, frame_shift=100, frame_length=250\n",
    ")\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features_torch = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor_batch, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=10, \n",
    "    dither=0.0, \n",
    "    frame_shift=100,\n",
    "    frame_length=250\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "# Shape of fbank features: torch.Size([998, 128])\n",
    "\n",
    "# Assuming you have already read the audio file into `audio_signal` and it's a 1D array\n",
    "# Initial shape of audio signal: (160000,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot both\n",
    "\n",
    "# plot the fbank features from the own implementation\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_own.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Own fbank Features')\n",
    "\n",
    "# plot the fbank features\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_torch.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Kaldi fbank Features')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# compare the 90th frame\n",
    "plt.plot(fbank_features_torch[90, :], label='Kaldi')\n",
    "plt.plot(fbank_features_own[90, :], label='Own')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.finfo(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizing the mel frequency filter bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_mel_banks(M, W_pad, f_s):\n",
    "    f_nyq = f_s / 2.0\n",
    "\n",
    "    f_low = 20.0\n",
    "    f_high = f_nyq\n",
    "\n",
    "    fft_bin_width = f_nyq / (W_pad // 2)\n",
    "    \n",
    "    # Mel scale conversion\n",
    "    def m_fun(f):\n",
    "        return 1127.0 * np.log(1.0 + f / 700.0)\n",
    "    \n",
    "    def f_fun(m):\n",
    "        return 700.0 * (np.exp(m / 1127.0) - 1.0)\n",
    "    \n",
    "    m_low = m_fun(f_low)\n",
    "    m_high = m_fun(f_high)\n",
    "    m_delta = (m_high - m_low) / (M + 1)\n",
    "    \n",
    "    H = np.zeros((M, W_pad // 2 + 1))\n",
    "    \n",
    "    for i in range(M):\n",
    "        m_left = m_low + i * m_delta\n",
    "        m_center = m_left + m_delta\n",
    "        m_right = m_center + m_delta\n",
    "        \n",
    "        for j in range(W_pad // 2 + 1):\n",
    "            f = j * fft_bin_width\n",
    "            m = m_fun(f)\n",
    "            \n",
    "            if m_left < m < m_right:\n",
    "                if m <= m_center:\n",
    "                    H[i, j] = (m - m_left) / (m_center - m_left)\n",
    "                else:\n",
    "                    H[i, j] = (m_right - m) / (m_right - m_center)\n",
    "    print(\"shape of the H matrix:\", H.shape)\n",
    "    return H \n",
    "\n",
    "# Parameters for visualization\n",
    "M = 8\n",
    "W_pad = 512\n",
    "f_s = 16000\n",
    "\n",
    "# Get the Mel filter bank matrix\n",
    "H = get_mel_banks(M, W_pad, f_s)\n",
    "\n",
    "# Plotting the matrix\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(H, aspect='auto', origin='lower', cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Filter bank coefficient')\n",
    "plt.xlabel('FFT bins')\n",
    "plt.ylabel('Mel bins')\n",
    "plt.title('Mel Filter Bank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_mel_banks(num_bins, padded_window_size, sample_rate):\n",
    "    num_fft_bins = padded_window_size // 2\n",
    "    nyquist_freq = sample_rate / 2.0\n",
    "\n",
    "    low_freq = 20.0\n",
    "    high_freq = nyquist_freq\n",
    "\n",
    "    fft_bin_width = nyquist_freq / num_fft_bins\n",
    "    \n",
    "    # Mel scale conversion\n",
    "    def mel_scale(freq):\n",
    "        return 1127.0 * np.log(1.0 + freq / 700.0)\n",
    "    \n",
    "    def inverse_mel_scale(mel_freq):\n",
    "        return 700.0 * (np.exp(mel_freq / 1127.0) - 1.0)\n",
    "    \n",
    "    mel_low_freq = mel_scale(low_freq)\n",
    "    mel_high_freq = mel_scale(high_freq)\n",
    "    mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "    \n",
    "    mel_bins = np.zeros((num_bins, num_fft_bins + 1))\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        left_mel = mel_low_freq + i * mel_freq_delta\n",
    "        center_mel = left_mel + mel_freq_delta\n",
    "        right_mel = center_mel + mel_freq_delta\n",
    "        \n",
    "        for j in range(num_fft_bins + 1):\n",
    "            freq = j * fft_bin_width\n",
    "            mel_freq = mel_scale(freq)\n",
    "            \n",
    "            if left_mel < mel_freq < right_mel:\n",
    "                if mel_freq <= center_mel:\n",
    "                    mel_bins[i, j] = (mel_freq - left_mel) / (center_mel - left_mel)\n",
    "                else:\n",
    "                    mel_bins[i, j] = (right_mel - mel_freq) / (right_mel - center_mel)\n",
    "    \n",
    "    return mel_bins  # shape = [128, 257]\n",
    "\n",
    "\n",
    "sample_rate = 16000  # Example sample rate\n",
    "padded_window_size = 512  # Example window size (power of 2 for FFT)\n",
    "num_bins = 8  # Number of Mel bins\n",
    "\n",
    "mel_filters = get_mel_banks(num_bins, padded_window_size, sample_rate)\n",
    "print(\"shape of mel_filters:\", mel_filters.shape)\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "def polygon_under_graph(x, y):\n",
    "    \"\"\"\n",
    "    Construct the vertex list which defines the polygon filling the space under\n",
    "    the (x, y) line graph. This assumes x is in ascending order.\n",
    "    \"\"\"\n",
    "    return [(x[0], 0.), *zip(x, y), (x[-1], 0.)]\n",
    "\n",
    "ax = plt.figure(figsize=(6.2, 3.5)).add_subplot(projection='3d')\n",
    "\n",
    "x = np.linspace(0, padded_window_size // 2, num=padded_window_size // 2 + 1)\n",
    "verts = [polygon_under_graph(x, mel_filters[i]) for i in range(num_bins)]\n",
    "\n",
    "mel_bins = np.arange(0, num_bins, 1)\n",
    "print(\"mel_bins:\", mel_bins)\n",
    "\n",
    "facecolors = plt.get_cmap('hot')(np.linspace(0.2, 0.65, len(verts)))\n",
    "\n",
    "poly = PolyCollection(verts, facecolors=facecolors, alpha=.7)\n",
    "ax.add_collection3d(poly, zs=mel_bins, zdir='y')\n",
    "\n",
    "ax.set(xlim=(0, padded_window_size // 2), ylim=(-0.5, num_bins - 0.5), zlim=(0, 1), xlabel='Frequency Bin', ylabel='Mel Bin')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_mel_banks(num_bins, padded_window_size, sample_rate):\n",
    "    num_fft_bins = padded_window_size // 2\n",
    "    nyquist_freq = sample_rate / 2.0\n",
    "\n",
    "    low_freq = 20.0\n",
    "    high_freq = nyquist_freq\n",
    "\n",
    "    fft_bin_width = nyquist_freq / num_fft_bins\n",
    "    \n",
    "    # Mel scale conversion\n",
    "    def mel_scale(freq):\n",
    "        return 1127.0 * np.log(1.0 + freq / 700.0)\n",
    "    \n",
    "    def inverse_mel_scale(mel_freq):\n",
    "        return 700.0 * (np.exp(mel_freq / 1127.0) - 1.0)\n",
    "    \n",
    "    mel_low_freq = mel_scale(low_freq)\n",
    "    mel_high_freq = mel_scale(high_freq)\n",
    "    mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "    \n",
    "    mel_bins = np.zeros((num_bins, num_fft_bins + 1))\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        left_mel = mel_low_freq + i * mel_freq_delta\n",
    "        center_mel = left_mel + mel_freq_delta\n",
    "        right_mel = center_mel + mel_freq_delta\n",
    "        \n",
    "        for j in range(num_fft_bins + 1):\n",
    "            freq = j * fft_bin_width\n",
    "            mel_freq = mel_scale(freq)\n",
    "            \n",
    "            if left_mel < mel_freq < right_mel:\n",
    "                if mel_freq <= center_mel:\n",
    "                    mel_bins[i, j] = (mel_freq - left_mel) / (center_mel - left_mel)\n",
    "                else:\n",
    "                    mel_bins[i, j] = (right_mel - mel_freq) / (right_mel - center_mel)\n",
    "    \n",
    "    return mel_bins  # shape = [128, 257]\n",
    "\n",
    "\n",
    "sample_rate = 16000  # Example sample rate\n",
    "padded_window_size = 512  # Example window size (power of 2 for FFT)\n",
    "num_bins = 8  # Number of Mel bins\n",
    "\n",
    "mel_filters = get_mel_banks(num_bins, padded_window_size, sample_rate)\n",
    "print(\"shape of mel_filters:\", mel_filters.shape)\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "def polygon_under_graph(x, y):\n",
    "    \"\"\"\n",
    "    Construct the vertex list which defines the polygon filling the space under\n",
    "    the (x, y) line graph. This assumes x is in ascending order.\n",
    "    \"\"\"\n",
    "    return [(x[0], 0.), *zip(x, y), (x[-1], 0.)]\n",
    "\n",
    "ax = plt.figure(figsize=(6.2, 3.5)).add_subplot(projection='3d')\n",
    "\n",
    "frequency_bin = np.linspace(0, padded_window_size // 2, num=padded_window_size // 2 + 1)\n",
    "\n",
    "verts = [polygon_under_graph(frequency_bin, mel_filters[i]) for i in range(num_bins)]\n",
    "\n",
    "mel_bins = np.arange(0, num_bins, 1)\n",
    "print(\"mel_bins:\", mel_bins)\n",
    "\n",
    "edgecolors = plt.get_cmap('hot')(np.linspace(0.2, 0.65, len(verts)))\n",
    "\n",
    "poly = PolyCollection(verts, edgecolors=edgecolors, alpha=0.8, facecolors='white')\n",
    "\n",
    "ax.add_collection3d(poly, zs=mel_bins, zdir='y')\n",
    "\n",
    "ax.set(xlim=(0, padded_window_size // 2), ylim=(-0.5, num_bins - 0.5), zlim=(0, 1), xlabel='Frequency Bin', ylabel='Mel Bin')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
