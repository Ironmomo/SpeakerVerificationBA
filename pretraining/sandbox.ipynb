{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# Navigate up one level to the 'pretraining' directory, where 'dataloader.py' is located\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "import dataloader\n",
    "\n",
    "# Define the base path where your pickle file is located\n",
    "base_path2 = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/mask01-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli'\n",
    "base_path3 = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/mask01-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-20240412-172636'\n",
    "base_path1_at_same_time = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/mask01-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-20240413-164417'\n",
    "base_path_original = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240416-103133'\n",
    "base_path_shuffled = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-shuffled-20240416-102831'\n",
    "path_original_3 = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014'\n",
    "path_original_correctMean = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full path to the 'args.pkl' file\n",
    "args_file_path2 = os.path.join(path_original_3, 'args.pkl')\n",
    "\n",
    "# Load the arguments from the pickle file\n",
    "with open(args_file_path2, 'rb') as f:\n",
    "    args2 = pickle.load(f)\n",
    "\n",
    "# Convert the Namespace to a dictionary if it is of that type\n",
    "if isinstance(args2, argparse.Namespace):\n",
    "    args_dict2 = vars(args2)\n",
    "else:\n",
    "    print(\"The loaded 'args' object is not an argparse.Namespace. Its type is:\", type(args2))\n",
    "    exit()\n",
    "\n",
    "# Determine the maximum width of the argument names for alignment\n",
    "max_key_length = max(len(key) for key in args_dict2.keys())\n",
    "\n",
    "# Print the arguments in a structured table format\n",
    "print(f\"{'Argument':<{max_key_length}} | Value\")\n",
    "print(\"-\" * (max_key_length + 3) + \"+\" + \"-\" * 30)  # Adjust 30 if you expect wider values\n",
    "\n",
    "for key, value in args_dict2.items():\n",
    "    print(f\"{key:<{max_key_length}} | {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs, Iterations and Time Required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the time required for training the model with 2 and 3 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open progress files\n",
    "with open(os.path.join(base_path2, 'progress.pkl'), 'rb') as f:\n",
    "    progress2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(base_path3, 'progress.pkl'), 'rb') as f:\n",
    "    progress3 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(base_path_original, 'progress.pkl'), 'rb') as f:\n",
    "    progress_original = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(base_path_shuffled, 'progress.pkl'), 'rb') as f:\n",
    "    progress_shuffled = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(path_original_3, 'progress.pkl'), 'rb') as f:\n",
    "    progress_original_3 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(path_original_correctMean, 'progress.pkl'), 'rb') as f:\n",
    "    progress_original_correctMean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the iteration (is at the second position in the list of progress)\n",
    "iteration2 = np.array([x[1] for x in progress2])\n",
    "iteration3 = np.array([x[1] for x in progress3])\n",
    "iteration_original = np.array([x[1] for x in progress_original])\n",
    "iteration_shuffled = np.array([x[1] for x in progress_shuffled])\n",
    "iter_original = np.array([x[1] for x in progress_original_3])\n",
    "iter_original_correctMean = np.array([x[1] for x in progress_original_correctMean])*2\n",
    "\n",
    "# get time (is at the fourth position in the list of progress)\n",
    "time2 = np.array([x[3] for x in progress2])\n",
    "time3 = np.array([x[3] for x in progress3])\n",
    "time_original = np.array([x[3] for x in progress_original])\n",
    "time_shuffled = np.array([x[3] for x in progress_shuffled])\n",
    "time_orig = np.array([x[3] for x in progress_original_3])\n",
    "time_original_correctMean = np.array([x[3] for x in progress_original_correctMean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total time required for all 800k iterations based on the number of iterations and the time per iteration\n",
    "n_iterations = 800000\n",
    "time_per_iteration2 = time2[10] / iteration2[10]\n",
    "time_per_iteration3 = time3[10] / iteration3[10]\n",
    "time_per_iteration_original = time_original[-1] / iteration_original[-1]\n",
    "time_per_iteration_shuffled = time_shuffled[-1] / iteration_shuffled[-1]\n",
    "time_per_iter_orig = time_orig[-1] / iter_original[-1]\n",
    "time_per_iter_orig_correctMean = time_original_correctMean[-1] / iter_original_correctMean[-1]\n",
    "\n",
    "total_time2 = time_per_iteration2 * n_iterations\n",
    "total_time3 = time_per_iteration3 * n_iterations\n",
    "total_time_original = time_per_iteration_original * n_iterations\n",
    "total_time_shuffled = time_per_iteration_shuffled * n_iterations\n",
    "total_time_orig = time_per_iter_orig * n_iterations\n",
    "total_time_orig_correctMean = time_per_iter_orig_correctMean * n_iterations/2\n",
    "\n",
    "print(f\"Total time for 2 GPUs: {total_time2/3600:.2f} hours\")\n",
    "print(f\"Total time for 3 GPUs: {total_time3/3600:.2f} hours\")\n",
    "print(f\"Total time for original: {total_time_original/3600:.2f} hours\")\n",
    "print(f\"Total time for shuffled: {total_time_shuffled/3600:.2f} hours\")\n",
    "print(f\"Total time for original 3 PGUs: {total_time_orig/3600:.2f} hours\")\n",
    "print(f\"Total time for original correctMean: {total_time_orig_correctMean/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot iter vs time all in one plot\n",
    "plt.plot(iteration2, time2, label='2 GPUs')\n",
    "plt.plot(iteration3, time3, label='3 GPUs')\n",
    "plt.plot(iteration_original, time_original, label='original')\n",
    "plt.plot(iteration_shuffled, time_shuffled, label='shuffled')\n",
    "plt.plot(iter_original, time_orig, label='original 3 GPUs')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Time [s]')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full path to the 'result.csv' file\n",
    "result_file_path3 = os.path.join(base_path3, 'result.csv')\n",
    "\n",
    "# Load the result from the csv file\n",
    "result3 = np.genfromtxt(result_file_path3, delimiter=',')\n",
    "\n",
    "# Extract the columns from the result\n",
    "acc_train3 = result3[:, 0] # The first column\n",
    "loss_train3 = result3[:, 1] # The second column\n",
    "acc_eval3 = result3[:, 2] # The third column\n",
    "mse_eval3 = result3[:, 3] # The fourth column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the training and evaluation loss for the model trained with original and shuffled spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full path to the 'result.csv' file\n",
    "result_file_path_original = os.path.join(base_path_original, 'result.csv')\n",
    "result_file_path_shuffled = os.path.join(base_path_shuffled, 'result.csv')\n",
    "\n",
    "# Load the result from the csv file\n",
    "result_original = np.genfromtxt(result_file_path_original, delimiter=',')\n",
    "result_shuffled = np.genfromtxt(result_file_path_shuffled, delimiter=',')\n",
    "print(\"shape of result_original: \", result_original.shape)\n",
    "print(\"shape of result_shuffled: \", result_shuffled.shape)\n",
    "\n",
    "# Extract the columns from the result\n",
    "\n",
    "acc_train_original = result_original[:, 0] # The first column\n",
    "loss1_train_original = result_original[:, 1] # The second column\n",
    "loss2_train_original = result_original[:, 2] # The third column\n",
    "acc_eval_original = result_original[:, 3] # The fourth column\n",
    "loss1_eval_original = result_original[:, 4] # The fifth column\n",
    "loss2_eval_original = result_original[:, 5] # The sixth column\n",
    "\n",
    "acc_train_shuffled = result_shuffled[:, 0] # The first column\n",
    "loss1_train_shuffled = result_shuffled[:, 1] # The second column\n",
    "loss2_train_shuffled = result_shuffled[:, 2] # The third column\n",
    "acc_eval_shuffled = result_shuffled[:, 3] # The fourth column\n",
    "loss1_eval_shuffled = result_shuffled[:, 4] # The fifth column\n",
    "loss2_eval_shuffled = result_shuffled[:, 5] # The sixth column\n",
    "\n",
    "learning_rate_original = result_original[:, 6] # The seventh column\n",
    "learning_rate_shuffled = result_shuffled[:, 6] # The seventh column\n",
    "\n",
    "\n",
    "# Define the format for each column\n",
    "header_format = \" {:>5}  | {:<10} | {:<10} | {:<10} | {:<10} | {:<10} | {:<10}\"\n",
    "row_format = \"{:>5}k  | {:<10.5f} | {:<10.5f} | {:<10.5f} | {:<10.5f} | {:<10.5f} | {:<10.5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orignal\n",
    "print(\"Original Spectrograms:\")\n",
    "print(\"-\" * 86)\n",
    "print(header_format.format(\"iter\", \"acc_train\", \"loss1_tr\", \"loss2_tr\", \"acc_ev\", \"loss1_ev\", \"loss2_ev\"))\n",
    "print(\"-\" * 86)  # Adjust the total length to fit your headers and column data\n",
    "for i in range(len(acc_train_original)):\n",
    "    print(row_format.format(iteration_original[i]/1e3, acc_train_original[i], loss1_train_original[i], loss2_train_original[i], acc_eval_original[i], loss1_eval_original[i], loss2_eval_original[i]))\n",
    "print(\"-\" * 86 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffled\n",
    "print(\"Shuffled Spectrograms:\")\n",
    "print(\"-\" * 86)\n",
    "print(header_format.format(\"iter\", \"acc_train\", \"loss1_tr\", \"loss2_tr\", \"acc_ev\", \"loss1_ev\", \"loss2_ev\"))\n",
    "print(\"-\" * 86)  # Adjust the total length to fit your headers and column data\n",
    "for i in range(len(acc_train_shuffled)):\n",
    "    print(row_format.format(iteration_shuffled[i]/1e3, acc_train_shuffled[i], loss1_train_shuffled[i], loss2_train_shuffled[i], acc_eval_shuffled[i], loss1_eval_shuffled[i], loss2_eval_shuffled[i]))\n",
    "print(\"-\" * 86 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gong_path = '/home/bosfab01/SpeakerVerificationBA/pretraining/result_gong.csv'\n",
    "result_original_path = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240416-103133/result.csv'\n",
    "result_shuffled_path = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-shuffled-20240416-102831/result.csv'\n",
    "result_3GPU_path = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/mask01-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-20240412-172636/result.csv'\n",
    "result_original_3GPU_path = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-base-f128-t2-b24-lr1e-4-m400-pretrain_joint-asli-original-20240418-211014/result.csv'\n",
    "result_original_correctMean_path = '/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-20240501-162648-original-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/result.csv'\n",
    "\n",
    "result_gong = np.genfromtxt(result_gong_path, delimiter=',')\n",
    "result_original = np.genfromtxt(result_original_path, delimiter=',')\n",
    "result_shuffled = np.genfromtxt(result_shuffled_path, delimiter=',')\n",
    "result_3GPU = np.genfromtxt(result_3GPU_path, delimiter=',')\n",
    "result_original_3GPU = np.genfromtxt(result_original_3GPU_path, delimiter=',')\n",
    "result_original_correctMean = np.genfromtxt(result_original_correctMean_path, delimiter=',')\n",
    "\n",
    "# print shapes of the arrays\n",
    "print(\"shape of result_gong: \", result_gong.shape)\n",
    "print(\"shape of result_original: \", result_original.shape)\n",
    "print(\"shape of result_shuffled: \", result_shuffled.shape)\n",
    "print(\"shape of result_3GPU: \", result_3GPU.shape)\n",
    "print(\"shape of result_original_3GPU: \", result_original_3GPU.shape)\n",
    "print(\"shape of result_original_correctMean: \", result_original_correctMean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return columns of a numpy array\n",
    "def get_column(array):\n",
    "    for i in range(array.shape[1]):\n",
    "        yield array[:, i]\n",
    "\n",
    "acc_tr_gong, loss_tr_gong, acc_ev_gong, mse_ev_gong, lr_gong = get_column(result_gong)\n",
    "acc_tr_3GPU, loss_tr_3GPU, acc_ev_3GPU, mse_ev_3GPU, _ = get_column(result_3GPU)\n",
    "acc_tr_original, loss1_tr_original, loss2_tr_original, acc_ev_original, loss1_ev_original, loss2_ev_original, _ = get_column(result_original)\n",
    "acc_tr_shuffled, loss1_tr_shuffled, loss2_tr_shuffled, acc_ev_shuffled, loss1_ev_shuffled, loss2_ev_shuffled, _ = get_column(result_shuffled)\n",
    "acc_tr_original_3GPU, loss1_tr_original_3GPU, loss2_tr_original_3GPU, acc_ev_original_3GPU, loss1_ev_original_3GPU, loss2_ev_original_3GPU, lr_original_3GPU = get_column(result_original_3GPU)\n",
    "acc_tr_original_correctMean, loss1_tr_original_correctMean, loss2_tr_original_correctMean, acc_ev_original_correctMean, loss1_ev_original_correctMean, loss2_ev_original_correctMean, lr_original_correctMean = get_column(result_original_correctMean)\n",
    "\n",
    "iteration_gong = np.arange(1, len(acc_tr_gong)+1) * 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axs[0, 0].plot(iteration_gong[:len(acc_tr_original_3GPU)+5]/1e3, acc_tr_gong[:len(acc_tr_original_3GPU)+5], label='gong')\n",
    "axs[0, 0].plot(iteration3[:len(acc_tr_original_3GPU)+5]/1e3, acc_tr_3GPU[:len(acc_tr_original_3GPU)+5], label='original 1024')\n",
    "axs[0, 0].plot(iter_original_correctMean/1e3, acc_tr_original_correctMean, label='original correctMean')\n",
    "# axs[0, 0].plot(iteration_original/1e3, acc_tr_original, label='original')\n",
    "# axs[0, 0].plot(iteration_shuffled/1e3, acc_tr_shuffled, label='shuffled')\n",
    "axs[0, 0].plot(iter_original/1e3, acc_tr_original_3GPU, label='original 998')\n",
    "#axs[0, 0].set_title('Accuracy Training')\n",
    "#axs[0, 0].set_xticks(np.arange(0, len(acc_tr_original)+5, 1))\n",
    "axs[0, 0].set_xlabel('Iteration [k]')\n",
    "axs[0, 0].set_ylabel('Accuracy Training')\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].grid()\n",
    "\n",
    "\n",
    "axs[0, 1].plot(iteration_gong[:len(acc_tr_original_3GPU)+5]/1e3, loss_tr_gong[:len(acc_tr_original_3GPU)+5], label='gong')\n",
    "axs[0, 1].plot(iteration3[:len(acc_tr_original_3GPU)+5]/1e3, loss_tr_3GPU[:len(acc_tr_original_3GPU)+5], label='original 1024')\n",
    "axs[0, 1].plot(iter_original_correctMean/1e3, loss1_tr_original_correctMean+10*loss2_tr_original_correctMean, label='original correctMean')\n",
    "# axs[0, 1].plot(iteration_original/1e3, loss1_tr_original+10*loss2_tr_original, label='original')\n",
    "# axs[0, 1].plot(iteration_shuffled/1e3, loss1_tr_shuffled+10*loss2_tr_shuffled, label='shuffled')\n",
    "axs[0, 1].plot(iter_original/1e3, loss1_tr_original_3GPU+10*loss2_tr_original_3GPU, label='original 998')\n",
    "#axs[0, 1].set_title('Loss Training')\n",
    "#axs[0, 1].set_xticks(np.arange(0, len(acc_tr_original)+5, 1))\n",
    "axs[0, 1].set_xlabel('Iteration [k]')\n",
    "axs[0, 1].set_ylabel('Loss Training')\n",
    "axs[0, 1].legend()\n",
    "axs[0, 1].grid()\n",
    "\n",
    "axs[1, 0].plot(iteration_gong[:len(acc_tr_original_3GPU)+5]/1e3, acc_ev_gong[:len(acc_tr_original_3GPU)+5], label='gong')\n",
    "axs[1, 0].plot(iteration3[:len(acc_tr_original_3GPU)+5]/1e3, acc_ev_3GPU[:len(acc_tr_original_3GPU)+5], label='original 1024')\n",
    "axs[1, 0].plot(iter_original_correctMean/1e3, acc_ev_original_correctMean, label='original correctMean')\n",
    "# axs[1, 0].plot(iteration_original/1e3, acc_ev_original, label='original')\n",
    "# axs[1, 0].plot(iteration_shuffled/1e3, acc_ev_shuffled, label='shuffled')\n",
    "axs[1, 0].plot(iter_original/1e3, acc_ev_original_3GPU, label='original 998')\n",
    "#axs[1, 0].set_title('Accuracy Evaluation')\n",
    "#axs[1, 0].set_xticks(np.arange(0, len(acc_tr_original)+5, 1))\n",
    "axs[1, 0].set_xlabel('Iteration [k]')\n",
    "axs[1, 0].set_ylabel('Accuracy Evaluation')\n",
    "axs[1, 0].legend()\n",
    "axs[1, 0].grid()\n",
    "\n",
    "axs[1, 1].plot(iteration_gong[:len(acc_tr_original_3GPU)+5]/1e3, mse_ev_gong[:len(acc_tr_original_3GPU)+5], label='gong')\n",
    "axs[1, 1].plot(iteration3[:len(acc_tr_original_3GPU)+5]/1e3, mse_ev_3GPU[:len(acc_tr_original_3GPU)+5], label='original 1024')\n",
    "axs[1, 1].plot(iter_original_correctMean/1e3, loss2_ev_original_correctMean, label='original correctMean')\n",
    "# axs[1, 1].plot(iteration_original/1e3, loss2_ev_original, label='original')\n",
    "# axs[1, 1].plot(iteration_shuffled/1e3, loss2_ev_shuffled, label='shuffled')\n",
    "axs[1, 1].plot(iter_original/1e3, loss2_ev_original_3GPU, label='original 998')\n",
    "#axs[1, 1].set_title('MSE Evaluation')\n",
    "#axs[1, 1].set_xticks(np.arange(0, len(acc_tr_original)+5, 1))\n",
    "axs[1, 1].set_xlabel('Iteration [k]')\n",
    "axs[1, 1].set_ylabel('MSE Evaluation')\n",
    "axs[1, 1].legend()\n",
    "axs[1, 1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rate on log scale\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(iteration_original/1e3, learning_rate_original, label='original')\n",
    "ax.plot(iteration_shuffled/1e3, learning_rate_shuffled, label='shuffled')\n",
    "ax.plot(iter_original/1e3, lr_original_3GPU, label='original 998')\n",
    "ax.plot(iteration_gong/1e3, lr_gong, label='gong')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Iteration [k]')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(iteration_shuffled/1e3, loss2_ev_shuffled, label='shuffled')\n",
    "plt.plot(iteration_shuffled/1e3, loss1_ev_shuffled, label='shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demonstration of mismatch between target length and input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudioDataset(\n",
    "        dataset_json_file='/home/bosfab01/SpeakerVerificationBA/data/audioset2M_librispeech960.json',\n",
    "        audio_conf={\n",
    "            'num_mel_bins': 128,\n",
    "            'target_length': 1024,\n",
    "            'freqm': 0,\n",
    "            'timem': 0,\n",
    "            'mixup': 0,\n",
    "            'dataset': 'asli',\n",
    "            'mean': -3.6925695,\n",
    "            'std': 4.020388,\n",
    "            'noise': False,\n",
    "            'mode': 'train',\n",
    "            'shuffle_frames': False\n",
    "        },\n",
    "        label_csv='/home/bosfab01/SpeakerVerificationBA/data/label_information.csv'\n",
    "    ),\n",
    "    batch_size=24,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get one batch of data from the dataloader and display the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "data_iterator = iter(train_loader)\n",
    "\n",
    "# Fetch the first batch\n",
    "audio_input, labels = next(data_iterator)\n",
    "\n",
    "# Print out the details to see what the batch contains\n",
    "print(\"Audio input shape:\", audio_input.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the last 10 spectra of the first sample in the batch\n",
    "# this is to check if the number of frames matches the target_length\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 10))  # Generate 10 colors from the 'viridis' colormap\n",
    "\n",
    "for i in range(10, 0, -1):\n",
    "    markerline, stemlines, baseline = plt.stem(audio_input[0, -i, :], linefmt='-', basefmt=\" \")\n",
    "    plt.setp(stemlines, 'linewidth', 2, 'color', colors[10-i])  # Set the color and line width\n",
    "    plt.setp(markerline, 'marker', '')  # No marker at the end\n",
    "\n",
    "plt.legend([f'Spectrum {-i}' for i in range(10, 0, -1)])\n",
    "plt.xlabel('Frequency Bins')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Spectra of the Last 10 Frames')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the last spectrum of the first sample\n",
    "print(audio_input[0, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted function to plot a spectrogram with the correct orientation\n",
    "def plot_spectrogram(spectrogram, ax, title=\"Spectrogram\"):\n",
    "    # Transpose the spectrogram to align the axes correctly\n",
    "    ax.imshow(spectrogram.T.cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, spectrogram.shape[0])\n",
    "    ax.set_xlabel('Time Frames')\n",
    "    ax.set_ylabel('Mel Frequency Bins')\n",
    "\n",
    "for i in range(3):\n",
    "    fig, ax = plt.subplots(figsize=(10, 1.5))\n",
    "    plot_spectrogram(audio_input[i, :, :], ax, title=f'Spectrogram of sample {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.append(parent_directory)\n",
    "from ssast_model import ASTModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from PIL import Image\n",
    "import qrcode\n",
    "import torchaudio\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")\n",
    "test_input = fbank_features\n",
    "\n",
    "# normalize fbank features\n",
    "dataset_mean=-3.6925695\n",
    "dataset_std=4.020388\n",
    "test_input = (test_input - dataset_mean) / (2 * dataset_std)\n",
    "\n",
    "# add batch dimension\n",
    "test_input = test_input.unsqueeze(0)\n",
    "print(f\"Shape of fbank features: {test_input.shape}\")\n",
    "\n",
    "# # duplicate input tensor to get a batch of 2\n",
    "# test_input = torch.cat((test_input, test_input), 0)\n",
    "# print(f\"Shape of dublicated fbank features: {test_input.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "model = ASTModel(fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=998, model_size='base', pretrain_stage=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('/home/bosfab01/SpeakerVerificationBA/pretraining/exp/pretrained-20240429-112534-shuffled-base-f128-t2-b48-lr1e-4-m390-pretrain_joint-asli/models/audio_model.54.pth'))\n",
    "model = model.module\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "print(next(model.parameters()).device)  # Should print 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "hop_width = 20\n",
    "hop_length = 50 # just for visualization, not the actual hop length used in the data preparation\n",
    "hops = range(hop_length, 998//2 - hop_width//2, hop_length)\n",
    "print(hops)\n",
    "mask_indices = [range(i-hop_width//2, i + hop_width//2) for i in hops]\n",
    "mask_indices = [idx for group in mask_indices for idx in group]\n",
    "print(\"len(mask_indices):\", len(mask_indices))\n",
    "\n",
    "# turn indices from model basis [0, 499] to spectrogram basis [0, 998]\n",
    "expanded_mask_indices = []\n",
    "for idx in mask_indices:\n",
    "    expanded_mask_indices.extend([2 * idx, 2 * idx + 1])  # Expanding indice\n",
    "\n",
    "# Create a mask for the spectrogram\n",
    "mask = torch.ones_like(test_input)\n",
    "for idx in expanded_mask_indices:\n",
    "    mask[0, idx, :] = 0  # Set the specific patches to 0\n",
    "\n",
    "# Apply the mask to the input spectrogram\n",
    "masked_spectrogram = test_input * mask\n",
    "\n",
    "# turn into tensor\n",
    "mask_indices = torch.tensor(mask_indices)\n",
    "\n",
    "print(\"shape of mask_indices:\", mask_indices.shape)\n",
    "\n",
    "# Call the model\n",
    "with torch.no_grad():\n",
    "    c_vec, x_vec, prob, nce = model(test_input, task='show_classification_head', mask_indices=mask_indices)\n",
    "\n",
    "# compare input and output\n",
    "print(test_input.shape)\n",
    "print(c_vec.shape)\n",
    "print(x_vec.shape)\n",
    "print(prob.shape)\n",
    "print(\"expected probability:\", 1 / len(mask_indices))\n",
    "print(\"actual probabilities:\", prob.cpu().numpy())\n",
    "\n",
    "# plot the first 3 vectors from c_vec and x_vec\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(c_vec[0, 0, :].numpy(), label='c_vec', color='blue')\n",
    "plt.plot(x_vec[0, 0, :].numpy(), label='x_vec', color='blue', linestyle='dotted')\n",
    "plt.plot(c_vec[0, 1, :].numpy(), label='c_vec', color='green')\n",
    "plt.plot(x_vec[0, 1, :].numpy(), label='x_vec', color='green', linestyle='dotted')\n",
    "plt.plot(c_vec[0, 2, :].numpy(), label='c_vec', color='yellow')\n",
    "plt.plot(x_vec[0, 2, :].numpy(), label='x_vec', color='yellow', linestyle='dotted')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# probability matrix \n",
    "probabilities = c_vec[0].cpu().numpy() @ x_vec[0].cpu().numpy().T\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(probabilities, cmap='viridis', aspect='auto')\n",
    "plt.xlabel('c_vec')\n",
    "plt.ylabel('x_vec')\n",
    "plt.title('Pre-Probability Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## own function for fbank (to ensure I understand the process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "print(f\"Shape of fbank features: {fbank_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### own function for fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# numeric_limits<float>::epsilon() 1.1920928955078125e-07\n",
    "EPSILON = torch.tensor(torch.finfo(torch.float).eps)\n",
    "# 1 milliseconds = 0.001 seconds\n",
    "MILLISECONDS_TO_SECONDS = 0.001\n",
    "\n",
    "\n",
    "def _get_epsilon(device, dtype):\n",
    "    return EPSILON.to(device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def _next_power_of_2(x: int) -> int:\n",
    "    r\"\"\"Returns the smallest power of 2 that is greater than x\"\"\"\n",
    "    return 1 if x == 0 else 2 ** (x - 1).bit_length()\n",
    "\n",
    "\n",
    "def _get_strided(waveform: Tensor, window_size: int, window_shift: int, snip_edges: bool) -> Tensor:\n",
    "    r\"\"\"Given a waveform (1D tensor of size ``num_samples``), it returns a 2D tensor (m, ``window_size``)\n",
    "    representing how the window is shifted along the waveform. Each row is a frame.\n",
    "\n",
    "    Args:\n",
    "        waveform (Tensor): Tensor of size ``num_samples``\n",
    "        window_size (int): Frame length\n",
    "        window_shift (int): Frame shift\n",
    "        snip_edges (bool): If True, end effects will be handled by outputting only frames that completely fit\n",
    "            in the file, and the number of frames depends on the frame_length.  If False, the number of frames\n",
    "            depends only on the frame_shift, and we reflect the data at the ends.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: 2D tensor of size (m, ``window_size``) where each row is a frame\n",
    "    \"\"\"\n",
    "    assert waveform.dim() == 1\n",
    "    num_samples = waveform.size(0)\n",
    "    strides = (window_shift * waveform.stride(0), waveform.stride(0))\n",
    "\n",
    "    if snip_edges:\n",
    "        if num_samples < window_size:\n",
    "            return torch.empty((0, 0), dtype=waveform.dtype, device=waveform.device)\n",
    "        else:\n",
    "            m = 1 + (num_samples - window_size) // window_shift\n",
    "    else:\n",
    "        reversed_waveform = torch.flip(waveform, [0])\n",
    "        m = (num_samples + (window_shift // 2)) // window_shift\n",
    "        pad = window_size // 2 - window_shift // 2\n",
    "        pad_right = reversed_waveform\n",
    "        if pad > 0:\n",
    "            # torch.nn.functional.pad returns [2,1,0,1,2] for 'reflect'\n",
    "            # but we want [2, 1, 0, 0, 1, 2]\n",
    "            pad_left = reversed_waveform[-pad:]\n",
    "            waveform = torch.cat((pad_left, waveform, pad_right), dim=0)\n",
    "        else:\n",
    "            # pad is negative so we want to trim the waveform at the front\n",
    "            waveform = torch.cat((waveform[-pad:], pad_right), dim=0)\n",
    "\n",
    "    sizes = (m, window_size)\n",
    "    return waveform.as_strided(sizes, strides)\n",
    "\n",
    "\n",
    "def _feature_window_function(\n",
    "    window_size: int,\n",
    "    device: torch.device,\n",
    "    dtype: int,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Returns a window function with the given type and size\"\"\"\n",
    "    return torch.hann_window(window_size, periodic=False, device=device, dtype=dtype)\n",
    "\n",
    "def _get_log_energy(strided_input: Tensor, epsilon: Tensor, energy_floor: float) -> Tensor:\n",
    "    r\"\"\"Returns the log energy of size (m) for a strided_input (m,*)\"\"\"\n",
    "    device, dtype = strided_input.device, strided_input.dtype\n",
    "    log_energy = torch.max(strided_input.pow(2).sum(1), epsilon).log()  # size (m)\n",
    "    if energy_floor == 0.0:\n",
    "        return log_energy\n",
    "    return torch.max(log_energy, torch.tensor(math.log(energy_floor), device=device, dtype=dtype))\n",
    "\n",
    "\n",
    "def _get_waveform_and_window_properties(\n",
    "    waveform: Tensor,\n",
    "    channel: int,\n",
    "    sample_frequency: float,\n",
    "    frame_shift: float,\n",
    "    frame_length: float,\n",
    "    round_to_power_of_two: bool,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, int, int, int]:\n",
    "    r\"\"\"Gets the waveform and window properties\"\"\"\n",
    "    channel = max(channel, 0)\n",
    "    assert channel < waveform.size(0), \"Invalid channel {} for size {}\".format(channel, waveform.size(0))\n",
    "    waveform = waveform[channel, :]  # size (n)\n",
    "    window_shift = int(sample_frequency * frame_shift * MILLISECONDS_TO_SECONDS)\n",
    "    window_size = int(sample_frequency * frame_length * MILLISECONDS_TO_SECONDS)\n",
    "    padded_window_size = _next_power_of_2(window_size) if round_to_power_of_two else window_size\n",
    "\n",
    "    assert 2 <= window_size <= len(waveform), \"choose a window size {} that is [2, {}]\".format(\n",
    "        window_size, len(waveform)\n",
    "    )\n",
    "    assert 0 < window_shift, \"`window_shift` must be greater than 0\"\n",
    "    assert padded_window_size % 2 == 0, (\n",
    "        \"the padded `window_size` must be divisible by two.\" \" use `round_to_power_of_two` or change `frame_length`\"\n",
    "    )\n",
    "    assert 0.0 <= preemphasis_coefficient <= 1.0, \"`preemphasis_coefficient` must be between [0,1]\"\n",
    "    assert sample_frequency > 0, \"`sample_frequency` must be greater than zero\"\n",
    "    return waveform, window_shift, window_size, padded_window_size\n",
    "\n",
    "\n",
    "def _get_window(\n",
    "    waveform: Tensor,\n",
    "    padded_window_size: int,\n",
    "    window_size: int,\n",
    "    window_shift: int,\n",
    "    snip_edges: bool,\n",
    "    raw_energy: bool,\n",
    "    energy_floor: float,\n",
    "    remove_dc_offset: bool,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Gets a window and its log energy\n",
    "\n",
    "    Returns:\n",
    "        (Tensor, Tensor): strided_input of size (m, ``padded_window_size``) and signal_log_energy of size (m)\n",
    "    \"\"\"\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "    epsilon = _get_epsilon(device, dtype)\n",
    "\n",
    "    # size (m, window_size)\n",
    "    strided_input = _get_strided(waveform, window_size, window_shift, snip_edges)\n",
    "\n",
    "    if remove_dc_offset:\n",
    "        # Subtract each row/frame by its mean\n",
    "        row_means = torch.mean(strided_input, dim=1).unsqueeze(1)  # size (m, 1)\n",
    "        strided_input = strided_input - row_means\n",
    "\n",
    "    if raw_energy:\n",
    "        # Compute the log energy of each row/frame before applying preemphasis and\n",
    "        # window function\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    if preemphasis_coefficient != 0.0:\n",
    "        # strided_input[i,j] -= preemphasis_coefficient * strided_input[i, max(0, j-1)] for all i,j\n",
    "        offset_strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (1, 0), mode=\"replicate\").squeeze(\n",
    "            0\n",
    "        )  # size (m, window_size + 1)\n",
    "        strided_input = strided_input - preemphasis_coefficient * offset_strided_input[:, :-1]\n",
    "\n",
    "    # Apply window_function to each row/frame\n",
    "    window_function = _feature_window_function(window_size, device, dtype).unsqueeze(\n",
    "        0\n",
    "    )  # size (1, window_size)\n",
    "    strided_input = strided_input * window_function  # size (m, window_size)\n",
    "\n",
    "    # Pad columns with zero until we reach size (m, padded_window_size)\n",
    "    if padded_window_size != window_size:\n",
    "        padding_right = padded_window_size - window_size\n",
    "        strided_input = torch.nn.functional.pad(\n",
    "            strided_input.unsqueeze(0), (0, padding_right), mode=\"constant\", value=0\n",
    "        ).squeeze(0)\n",
    "\n",
    "    # Compute energy after window function (not the raw one)\n",
    "    if not raw_energy:\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    return strided_input, signal_log_energy\n",
    "\n",
    "\n",
    "def _subtract_column_mean(tensor: Tensor, subtract_mean: bool) -> Tensor:\n",
    "    # subtracts the column mean of the tensor size (m, n) if subtract_mean=True\n",
    "    # it returns size (m, n)\n",
    "    if subtract_mean:\n",
    "        col_means = torch.mean(tensor, dim=0).unsqueeze(0)\n",
    "        tensor = tensor - col_means\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\n",
    "def inverse_mel_scale_scalar(mel_freq: float) -> float:\n",
    "    return 700.0 * (math.exp(mel_freq / 1127.0) - 1.0)\n",
    "\n",
    "\n",
    "def inverse_mel_scale(mel_freq: Tensor) -> Tensor:\n",
    "    return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)\n",
    "\n",
    "\n",
    "def mel_scale_scalar(freq: float) -> float:\n",
    "    return 1127.0 * math.log(1.0 + freq / 700.0)\n",
    "\n",
    "\n",
    "def mel_scale(freq: Tensor) -> Tensor:\n",
    "    return 1127.0 * (1.0 + freq / 700.0).log()\n",
    "\n",
    "\n",
    "def get_mel_banks(\n",
    "    num_bins: int,\n",
    "    window_length_padded: int,\n",
    "    sample_freq: float,\n",
    "    low_freq: float,\n",
    "    high_freq: float,\n",
    "    vtln_low: float,\n",
    "    vtln_high: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        (Tensor, Tensor): The tuple consists of ``bins`` (which is\n",
    "        melbank of size (``num_bins``, ``num_fft_bins``)) and ``center_freqs`` (which is\n",
    "        center frequencies of bins of size (``num_bins``)).\n",
    "    \"\"\"\n",
    "    assert num_bins > 3, \"Must have at least 3 mel bins\"\n",
    "    assert window_length_padded % 2 == 0\n",
    "    num_fft_bins = window_length_padded / 2\n",
    "    nyquist = 0.5 * sample_freq\n",
    "\n",
    "    if high_freq <= 0.0:\n",
    "        high_freq += nyquist\n",
    "\n",
    "    assert (\n",
    "        (0.0 <= low_freq < nyquist) and (0.0 < high_freq <= nyquist) and (low_freq < high_freq)\n",
    "    ), \"Bad values in options: low-freq {} and high-freq {} vs. nyquist {}\".format(low_freq, high_freq, nyquist)\n",
    "\n",
    "    # fft-bin width [think of it as Nyquist-freq / half-window-length]\n",
    "    fft_bin_width = sample_freq / window_length_padded\n",
    "    mel_low_freq = mel_scale_scalar(low_freq)\n",
    "    mel_high_freq = mel_scale_scalar(high_freq)\n",
    "\n",
    "    # divide by num_bins+1 in next line because of end-effects where the bins\n",
    "    # spread out to the sides.\n",
    "    mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "\n",
    "    if vtln_high < 0.0:\n",
    "        vtln_high += nyquist\n",
    "\n",
    "    bin = torch.arange(num_bins).unsqueeze(1)\n",
    "    left_mel = mel_low_freq + bin * mel_freq_delta  # size(num_bins, 1)\n",
    "    center_mel = mel_low_freq + (bin + 1.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "    right_mel = mel_low_freq + (bin + 2.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "\n",
    "    center_freqs = inverse_mel_scale(center_mel)  # size (num_bins)\n",
    "    # size(1, num_fft_bins)\n",
    "    mel = mel_scale(fft_bin_width * torch.arange(num_fft_bins)).unsqueeze(0)\n",
    "\n",
    "    # size (num_bins, num_fft_bins)\n",
    "    up_slope = (mel - left_mel) / (center_mel - left_mel)\n",
    "    down_slope = (right_mel - mel) / (right_mel - center_mel)\n",
    "\n",
    "    # left_mel < center_mel < right_mel so we can min the two slopes and clamp negative values\n",
    "    bins = torch.max(torch.zeros(1), torch.min(up_slope, down_slope))\n",
    "    \n",
    "    return bins, center_freqs\n",
    "\n",
    "\n",
    "def fbank_own(\n",
    "    waveform: Tensor,\n",
    "    channel: int = -1,\n",
    "    energy_floor: float = 1.0,\n",
    "    frame_length: float = 25.0,\n",
    "    frame_shift: float = 10.0,\n",
    "    high_freq: float = 0.0,\n",
    "    low_freq: float = 20.0,\n",
    "    min_duration: float = 0.0,\n",
    "    num_mel_bins: int = 23,\n",
    "    preemphasis_coefficient: float = 0.97,\n",
    "    raw_energy: bool = True,\n",
    "    remove_dc_offset: bool = True,\n",
    "    round_to_power_of_two: bool = True,\n",
    "    sample_frequency: float = 16000.0,\n",
    "    snip_edges: bool = True,\n",
    "    subtract_mean: bool = False,\n",
    "    use_log_fbank: bool = True,\n",
    "    use_power: bool = True,\n",
    "    vtln_high: float = -500.0,\n",
    "    vtln_low: float = 100.0,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Create a fbank from a raw audio signal. This matches the input/output of Kaldi's\n",
    "    compute-fbank-feats.\n",
    "\n",
    "    Args:\n",
    "        waveform (Tensor): Tensor of audio of size (c, n) where c is in the range [0,2)\n",
    "        blackman_coeff (float, optional): Constant coefficient for generalized Blackman window. (Default: ``0.42``)\n",
    "        channel (int, optional): Channel to extract (-1 -> expect mono, 0 -> left, 1 -> right) (Default: ``-1``)\n",
    "        dither (float, optional): Dithering constant (0.0 means no dither). If you turn this off, you should set\n",
    "            the energy_floor option, e.g. to 1.0 or 0.1 (Default: ``0.0``)\n",
    "        energy_floor (float, optional): Floor on energy (absolute, not relative) in Spectrogram computation.  Caution:\n",
    "            this floor is applied to the zeroth component, representing the total signal energy.  The floor on the\n",
    "            individual spectrogram elements is fixed at std::numeric_limits<float>::epsilon(). (Default: ``1.0``)\n",
    "        frame_length (float, optional): Frame length in milliseconds (Default: ``25.0``)\n",
    "        frame_shift (float, optional): Frame shift in milliseconds (Default: ``10.0``)\n",
    "        high_freq (float, optional): High cutoff frequency for mel bins (if <= 0, offset from Nyquist)\n",
    "         (Default: ``0.0``)\n",
    "        htk_compat (bool, optional): If true, put energy last.  Warning: not sufficient to get HTK compatible features\n",
    "         (need to change other parameters). (Default: ``False``)\n",
    "        low_freq (float, optional): Low cutoff frequency for mel bins (Default: ``20.0``)\n",
    "        min_duration (float, optional): Minimum duration of segments to process (in seconds). (Default: ``0.0``)\n",
    "        num_mel_bins (int, optional): Number of triangular mel-frequency bins (Default: ``23``)\n",
    "        preemphasis_coefficient (float, optional): Coefficient for use in signal preemphasis (Default: ``0.97``)\n",
    "        raw_energy (bool, optional): If True, compute energy before preemphasis and windowing (Default: ``True``)\n",
    "        remove_dc_offset (bool, optional): Subtract mean from waveform on each frame (Default: ``True``)\n",
    "        round_to_power_of_two (bool, optional): If True, round window size to power of two by zero-padding input\n",
    "            to FFT. (Default: ``True``)\n",
    "        sample_frequency (float, optional): Waveform data sample frequency (must match the waveform file, if\n",
    "            specified there) (Default: ``16000.0``)\n",
    "        snip_edges (bool, optional): If True, end effects will be handled by outputting only frames that completely fit\n",
    "            in the file, and the number of frames depends on the frame_length.  If False, the number of frames\n",
    "            depends only on the frame_shift, and we reflect the data at the ends. (Default: ``True``)\n",
    "        subtract_mean (bool, optional): Subtract mean of each feature file [CMS]; not recommended to do\n",
    "            it this way.  (Default: ``False``)\n",
    "        use_energy (bool, optional): Add an extra dimension with energy to the FBANK output. (Default: ``False``)\n",
    "        use_log_fbank (bool, optional):If true, produce log-filterbank, else produce linear. (Default: ``True``)\n",
    "        use_power (bool, optional): If true, use power, else use magnitude. (Default: ``True``)\n",
    "        vtln_high (float, optional): High inflection point in piecewise linear VTLN warping function (if\n",
    "            negative, offset from high-mel-freq (Default: ``-500.0``)\n",
    "        vtln_low (float, optional): Low inflection point in piecewise linear VTLN warping function (Default: ``100.0``)\n",
    "        vtln_warp (float, optional): Vtln warp factor (only applicable if vtln_map not specified) (Default: ``1.0``)\n",
    "        window_type (str, optional): Type of window ('hamming'|'hanning'|'povey'|'rectangular'|'blackman')\n",
    "         (Default: ``'povey'``)\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A fbank identical to what Kaldi would output. The shape is (m, ``num_mel_bins + use_energy``)\n",
    "        where m is calculated in _get_strided\n",
    "    \"\"\"\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "    waveform, window_shift, window_size, padded_window_size = _get_waveform_and_window_properties(\n",
    "        waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient\n",
    "    )\n",
    "\n",
    "    if len(waveform) < min_duration * sample_frequency:\n",
    "        # signal is too short\n",
    "        return torch.empty(0, device=device, dtype=dtype)\n",
    "\n",
    "    # strided_input, size (m, padded_window_size) and signal_log_energy, size (m)\n",
    "    strided_input, signal_log_energy = _get_window(\n",
    "        waveform,\n",
    "        padded_window_size,\n",
    "        window_size,\n",
    "        window_shift,\n",
    "        snip_edges,\n",
    "        raw_energy,\n",
    "        energy_floor,\n",
    "        remove_dc_offset,\n",
    "        preemphasis_coefficient,\n",
    "    )\n",
    "\n",
    "    # size (m, padded_window_size // 2 + 1)\n",
    "    spectrum = torch.fft.rfft(strided_input).abs()\n",
    "    if use_power:\n",
    "        spectrum = spectrum.pow(2.0)\n",
    "\n",
    "    # size (num_mel_bins, padded_window_size // 2)\n",
    "    mel_energies, _ = get_mel_banks(\n",
    "        num_mel_bins, padded_window_size, sample_frequency, low_freq, high_freq, vtln_low, vtln_high\n",
    "    )\n",
    "    mel_energies = mel_energies.to(device=device, dtype=dtype)\n",
    "\n",
    "    # pad right column with zeros and add dimension, size (num_mel_bins, padded_window_size // 2 + 1)\n",
    "    mel_energies = torch.nn.functional.pad(mel_energies, (0, 1), mode=\"constant\", value=0)\n",
    "\n",
    "    # sum with mel fiterbanks over the power spectrum, size (m, num_mel_bins)\n",
    "    mel_energies = torch.mm(spectrum, mel_energies.T)\n",
    "    if use_log_fbank:\n",
    "        # avoid log of zero (which should be prevented anyway by dithering)\n",
    "        mel_energies = torch.max(mel_energies, _get_epsilon(device, dtype)).log()\n",
    "\n",
    "    mel_energies = _subtract_column_mean(mel_energies, subtract_mean)\n",
    "    return mel_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call fbank_own\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "print(\"Data type of audio tensor:\", audio_tensor.dtype)\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor = audio_tensor.unsqueeze(0)\n",
    "print(\"Shape of audio tensor:\", audio_tensor.shape)\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features_own = fbank_own(\n",
    "    audio_tensor, \n",
    "    sample_frequency=sample_rate, \n",
    "    num_mel_bins=128, \n",
    "    frame_shift=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both\n",
    "\n",
    "# plot the fbank features from the own implementation\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_own.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Own fbank Features')\n",
    "\n",
    "# plot the fbank features\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Kaldi fbank Features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the 90th frame\n",
    "plt.plot(fbank_features[90, :], label='Kaldi')\n",
    "plt.plot(fbank_features_own[90, :], label='Own')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# numeric_limits<float>::epsilon() 1.1920928955078125e-07\n",
    "EPSILON = torch.tensor(torch.finfo(torch.float).eps)\n",
    "# 1 milliseconds = 0.001 seconds\n",
    "MILLISECONDS_TO_SECONDS = 0.001\n",
    "\n",
    "\n",
    "def _get_epsilon(device, dtype):\n",
    "    return EPSILON.to(device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def _next_power_of_2(x: int) -> int:\n",
    "    return 1 if x == 0 else 2 ** (x - 1).bit_length()\n",
    "\n",
    "\n",
    "def _get_strided(waveform: Tensor, window_size: int, window_shift: int, snip_edges: bool) -> Tensor:\n",
    "    assert waveform.dim() == 1\n",
    "    num_samples = waveform.size(0)\n",
    "    strides = (window_shift * waveform.stride(0), waveform.stride(0))\n",
    "\n",
    "    if snip_edges:\n",
    "        if num_samples < window_size:\n",
    "            return torch.empty((0, 0), dtype=waveform.dtype, device=waveform.device)\n",
    "        else:\n",
    "            m = 1 + (num_samples - window_size) // window_shift\n",
    "    else:\n",
    "        reversed_waveform = torch.flip(waveform, [0])\n",
    "        m = (num_samples + (window_shift // 2)) // window_shift\n",
    "        pad = window_size // 2 - window_shift // 2\n",
    "        pad_right = reversed_waveform\n",
    "        if pad > 0:\n",
    "            # torch.nn.functional.pad returns [2,1,0,1,2] for 'reflect'\n",
    "            # but we want [2, 1, 0, 0, 1, 2]\n",
    "            pad_left = reversed_waveform[-pad:]\n",
    "            waveform = torch.cat((pad_left, waveform, pad_right), dim=0)\n",
    "        else:\n",
    "            # pad is negative so we want to trim the waveform at the front\n",
    "            waveform = torch.cat((waveform[-pad:], pad_right), dim=0)\n",
    "\n",
    "    sizes = (m, window_size)\n",
    "    return waveform.as_strided(sizes, strides)\n",
    "\n",
    "\n",
    "def _feature_window_function(\n",
    "    window_size: int,\n",
    "    device: torch.device,\n",
    "    dtype: int,\n",
    ") -> Tensor:\n",
    "    return torch.hann_window(window_size, periodic=False, device=device, dtype=dtype)\n",
    "\n",
    "def _get_log_energy(strided_input: Tensor, epsilon: Tensor, energy_floor: float) -> Tensor:\n",
    "    device, dtype = strided_input.device, strided_input.dtype\n",
    "    log_energy = torch.max(strided_input.pow(2).sum(1), epsilon).log()  # size (m)\n",
    "    if energy_floor == 0.0:\n",
    "        return log_energy\n",
    "    return torch.max(log_energy, torch.tensor(math.log(energy_floor), device=device, dtype=dtype))\n",
    "\n",
    "\n",
    "def _get_waveform_and_window_properties(\n",
    "    waveform: Tensor,\n",
    "    channel: int,\n",
    "    sample_frequency: float,\n",
    "    frame_shift: float,\n",
    "    frame_length: float,\n",
    "    round_to_power_of_two: bool,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, int, int, int]:\n",
    "    r\"\"\"Gets the waveform and window properties\"\"\"\n",
    "    channel = max(channel, 0)\n",
    "    assert channel < waveform.size(0), \"Invalid channel {} for size {}\".format(channel, waveform.size(0))\n",
    "    waveform = waveform[channel, :]  # size (n)\n",
    "    window_shift = int(sample_frequency * frame_shift * MILLISECONDS_TO_SECONDS)\n",
    "    window_size = int(sample_frequency * frame_length * MILLISECONDS_TO_SECONDS)\n",
    "    padded_window_size = _next_power_of_2(window_size) if round_to_power_of_two else window_size\n",
    "\n",
    "    assert 2 <= window_size <= len(waveform), \"choose a window size {} that is [2, {}]\".format(\n",
    "        window_size, len(waveform)\n",
    "    )\n",
    "    assert 0 < window_shift, \"`window_shift` must be greater than 0\"\n",
    "    assert padded_window_size % 2 == 0, (\n",
    "        \"the padded `window_size` must be divisible by two.\" \" use `round_to_power_of_two` or change `frame_length`\"\n",
    "    )\n",
    "    assert 0.0 <= preemphasis_coefficient <= 1.0, \"`preemphasis_coefficient` must be between [0,1]\"\n",
    "    assert sample_frequency > 0, \"`sample_frequency` must be greater than zero\"\n",
    "    return waveform, window_shift, window_size, padded_window_size\n",
    "\n",
    "\n",
    "def _get_window(\n",
    "    waveform: Tensor,\n",
    "    padded_window_size: int,\n",
    "    window_size: int,\n",
    "    window_shift: int,\n",
    "    snip_edges: bool,\n",
    "    raw_energy: bool,\n",
    "    energy_floor: float,\n",
    "    preemphasis_coefficient: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "    epsilon = _get_epsilon(device, dtype)\n",
    "\n",
    "    # size (m, window_size)\n",
    "    strided_input = _get_strided(waveform, window_size, window_shift, snip_edges)\n",
    "\n",
    "    # Subtract each row/frame by its mean\n",
    "    row_means = torch.mean(strided_input, dim=1).unsqueeze(1)  # size (m, 1)\n",
    "    strided_input = strided_input - row_means\n",
    "\n",
    "    if raw_energy:\n",
    "        # Compute the log energy of each row/frame before applying preemphasis and window function\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    if preemphasis_coefficient != 0.0:\n",
    "        # strided_input[i,j] -= preemphasis_coefficient * strided_input[i, max(0, j-1)] for all i,j\n",
    "        offset_strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (1, 0), mode=\"replicate\").squeeze(\n",
    "            0\n",
    "        )  # size (m, window_size + 1)\n",
    "        strided_input = strided_input - preemphasis_coefficient * offset_strided_input[:, :-1]\n",
    "\n",
    "    # Apply window_function to each row/frame\n",
    "    window_function = _feature_window_function(window_size, device, dtype).unsqueeze(\n",
    "        0\n",
    "    )  # size (1, window_size)\n",
    "    strided_input = strided_input * window_function  # size (m, window_size)\n",
    "\n",
    "    # Pad columns with zero until we reach size (m, padded_window_size)\n",
    "    if padded_window_size != window_size:\n",
    "        padding_right = padded_window_size - window_size\n",
    "        strided_input = torch.nn.functional.pad(\n",
    "            strided_input.unsqueeze(0), (0, padding_right), mode=\"constant\", value=0\n",
    "        ).squeeze(0)\n",
    "\n",
    "    # Compute energy after window function (not the raw one)\n",
    "    if not raw_energy:\n",
    "        signal_log_energy = _get_log_energy(strided_input, epsilon, energy_floor)  # size (m)\n",
    "\n",
    "    return strided_input, signal_log_energy\n",
    "\n",
    "\n",
    "def _subtract_column_mean(tensor: Tensor, subtract_mean: bool) -> Tensor:\n",
    "    # subtracts the column mean of the tensor size (m, n) if subtract_mean=True\n",
    "    # it returns size (m, n)\n",
    "    if subtract_mean:\n",
    "        col_means = torch.mean(tensor, dim=0).unsqueeze(0)\n",
    "        tensor = tensor - col_means\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\n",
    "def inverse_mel_scale_scalar(mel_freq: float) -> float:\n",
    "    return 700.0 * (math.exp(mel_freq / 1127.0) - 1.0)\n",
    "\n",
    "\n",
    "def inverse_mel_scale(mel_freq: Tensor) -> Tensor:\n",
    "    return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)\n",
    "\n",
    "\n",
    "def mel_scale_scalar(freq: float) -> float:\n",
    "    return 1127.0 * math.log(1.0 + freq / 700.0)\n",
    "\n",
    "\n",
    "def mel_scale(freq: Tensor) -> Tensor:\n",
    "    return 1127.0 * (1.0 + freq / 700.0).log()\n",
    "\n",
    "\n",
    "def get_mel_banks(\n",
    "    num_bins: int,\n",
    "    window_length_padded: int,\n",
    "    sample_freq: float,\n",
    "    low_freq: float,\n",
    "    high_freq: float,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \n",
    "    num_fft_bins = window_length_padded / 2\n",
    "    nyquist = 0.5 * sample_freq\n",
    "\n",
    "    if high_freq <= 0.0:\n",
    "        high_freq += nyquist\n",
    "\n",
    "    # fft-bin width [think of it as Nyquist-freq / half-window-length]\n",
    "    fft_bin_width = sample_freq / window_length_padded\n",
    "    mel_low_freq = mel_scale_scalar(low_freq)\n",
    "    mel_high_freq = mel_scale_scalar(high_freq)\n",
    "\n",
    "    # divide by num_bins+1 in next line because of end-effects where the bins\n",
    "    # spread out to the sides.\n",
    "    mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "\n",
    "    bin = torch.arange(num_bins).unsqueeze(1)\n",
    "    left_mel = mel_low_freq + bin * mel_freq_delta  # size(num_bins, 1)\n",
    "    center_mel = mel_low_freq + (bin + 1.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "    right_mel = mel_low_freq + (bin + 2.0) * mel_freq_delta  # size(num_bins, 1)\n",
    "\n",
    "    center_freqs = inverse_mel_scale(center_mel)  # size (num_bins)\n",
    "    # size(1, num_fft_bins)\n",
    "    mel = mel_scale(fft_bin_width * torch.arange(num_fft_bins)).unsqueeze(0)\n",
    "\n",
    "    # size (num_bins, num_fft_bins)\n",
    "    up_slope = (mel - left_mel) / (center_mel - left_mel)\n",
    "    down_slope = (right_mel - mel) / (right_mel - center_mel)\n",
    "\n",
    "    # left_mel < center_mel < right_mel so we can min the two slopes and clamp negative values\n",
    "    bins = torch.max(torch.zeros(1), torch.min(up_slope, down_slope))\n",
    "    \n",
    "    return bins, center_freqs\n",
    "\n",
    "\n",
    "def fbank_own(\n",
    "    waveform: Tensor,\n",
    "    channel: int = -1,\n",
    "    energy_floor: float = 1.0,\n",
    "    frame_length: float = 25.0,\n",
    "    frame_shift: float = 10.0,\n",
    "    high_freq: float = 0.0,\n",
    "    low_freq: float = 20.0,\n",
    "    min_duration: float = 0.0,\n",
    "    num_mel_bins: int = 128,\n",
    "    preemphasis_coefficient: float = 0.97,\n",
    "    raw_energy: bool = True,\n",
    "    round_to_power_of_two: bool = True,\n",
    "    sample_frequency: float = 16000.0,\n",
    "    snip_edges: bool = True,\n",
    "    subtract_mean: bool = False,\n",
    "    use_log_fbank: bool = True,\n",
    "    use_power: bool = True,\n",
    ") -> Tensor:\n",
    "\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "    waveform, window_shift, window_size, padded_window_size = _get_waveform_and_window_properties(\n",
    "        waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient\n",
    "    )\n",
    "\n",
    "    if len(waveform) < min_duration * sample_frequency:\n",
    "        # signal is too short\n",
    "        return torch.empty(0, device=device, dtype=dtype)\n",
    "\n",
    "    # strided_input, size (m, padded_window_size) and signal_log_energy, size (m)\n",
    "    strided_input, signal_log_energy = _get_window(\n",
    "        waveform,\n",
    "        padded_window_size,\n",
    "        window_size,\n",
    "        window_shift,\n",
    "        snip_edges,\n",
    "        raw_energy,\n",
    "        energy_floor,\n",
    "        preemphasis_coefficient,\n",
    "    )\n",
    "\n",
    "    # size (m, padded_window_size // 2 + 1)\n",
    "    spectrum = torch.fft.rfft(strided_input).abs()\n",
    "    if use_power:\n",
    "        spectrum = spectrum.pow(2.0)\n",
    "\n",
    "    # size (num_mel_bins, padded_window_size // 2)\n",
    "    mel_energies, _ = get_mel_banks(\n",
    "        num_mel_bins, padded_window_size, sample_frequency, low_freq, high_freq\n",
    "    )\n",
    "    mel_energies = mel_energies.to(device=device, dtype=dtype)\n",
    "\n",
    "    # pad right column with zeros and add dimension, size (num_mel_bins, padded_window_size // 2 + 1)\n",
    "    mel_energies = torch.nn.functional.pad(mel_energies, (0, 1), mode=\"constant\", value=0)\n",
    "\n",
    "    # sum with mel fiterbanks over the power spectrum, size (m, num_mel_bins)\n",
    "    mel_energies = torch.mm(spectrum, mel_energies.T)\n",
    "    if use_log_fbank:\n",
    "        # avoid log of zero (which should be prevented anyway by dithering)\n",
    "        mel_energies = torch.max(mel_energies, _get_epsilon(device, dtype)).log()\n",
    "\n",
    "    mel_energies = _subtract_column_mean(mel_energies, subtract_mean)\n",
    "    return mel_energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def fbank_own(\n",
    "    waveform: Tensor,\n",
    ") -> Tensor:\n",
    "    device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "    # shape is [c, n] (=[1, n] in case of mono) = [1, 160000] in our case\n",
    "    waveform = torch.squeeze(waveform)\n",
    "    # now shape is [n] = [160000] in our case\n",
    "\n",
    "\n",
    "    def get_window(\n",
    "        waveform: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        device, dtype = waveform.device, waveform.dtype\n",
    "\n",
    "        strides = (160, 1)\n",
    "        sizes = (998, 400)\n",
    "\n",
    "        strided_input = waveform.as_strided(sizes, strides) # size (998, 400)\n",
    "\n",
    "        # Subtract each row/frame by its mean\n",
    "        row_means = torch.mean(strided_input, dim=1).unsqueeze(1)  # size (998, 1)\n",
    "        strided_input = strided_input - row_means # size (998, 400)\n",
    "\n",
    "        # strided_input[i,j] -= preemphasis_coefficient * strided_input[i, max(0, j-1)] for all i,j\n",
    "        offset_strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (1, 0), mode=\"replicate\").squeeze(0)  # size (998, 400 + 1)\n",
    "        strided_input = strided_input - 0.97 * offset_strided_input[:, :-1] # size (998, 400)\n",
    "\n",
    "        # Apply window_function to each row/frame\n",
    "        window_function = torch.hann_window(400, periodic=False, device=device, dtype=dtype).unsqueeze(0)  # size (1, 400)\n",
    "        strided_input = strided_input * window_function  # size (998, 400)\n",
    "\n",
    "        strided_input = torch.nn.functional.pad(strided_input.unsqueeze(0), (0, 112), mode=\"constant\", value=0).squeeze(0) # 512 - 400 = 112\n",
    "\n",
    "        return strided_input\n",
    "\n",
    "    # strided_input, size (m, padded_window_size) and signal_log_energy, size (m)\n",
    "    strided_input = get_window(waveform) # size (998, 512)\n",
    "\n",
    "    # size (m, padded_window_size // 2 + 1)\n",
    "    spectrum = torch.fft.rfft(strided_input).abs() # size (998, 256 + 1)\n",
    "\n",
    "    spectrum = spectrum.pow(2.0)\n",
    "\n",
    "\n",
    "    def get_mel_banks(\n",
    "        num_bins: int\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "        def inverse_mel_scale_scalar(mel_freq: float) -> float:\n",
    "            return 700.0 * (math.exp(mel_freq / 1127.0) - 1.0)\n",
    "\n",
    "        def inverse_mel_scale(mel_freq: Tensor) -> Tensor:\n",
    "            return 700.0 * ((mel_freq / 1127.0).exp() - 1.0)\n",
    "\n",
    "        def mel_scale_scalar(freq: float) -> float:\n",
    "            return 1127.0 * math.log(1.0 + freq / 700.0)\n",
    "\n",
    "        def mel_scale(freq: Tensor) -> Tensor:\n",
    "            return 1127.0 * (1.0 + freq / 700.0).log()\n",
    "        \n",
    "        num_fft_bins = 256 # window_length_padded / 2 = 512 / 2\n",
    "        nyquist_freq= 8000.0\n",
    "\n",
    "        low_freq = 20.0\n",
    "        high_freq = nyquist_freq\n",
    "\n",
    "        # fft-bin width [think of it as Nyquist-freq / half-window-length]\n",
    "        fft_bin_width = 31.25 # 16000 / window_length_padded = 16000 / 512\n",
    "        mel_low_freq = mel_scale_scalar(low_freq) # 31.748578341466644\n",
    "        mel_high_freq = mel_scale_scalar(high_freq) # 2840.0377117383778\n",
    "\n",
    "        # divide by num_bins+1 in next line because of end-effects where the bins spread out to the sides.\n",
    "        mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1) # 21.769683204627217\n",
    "\n",
    "        bin = torch.arange(num_bins).unsqueeze(1)\n",
    "        left_mel = mel_low_freq + bin * mel_freq_delta  # size(num_bins, 1) = (128, 1)\n",
    "        center_mel = mel_low_freq + (bin + 1.0) * mel_freq_delta  # size(num_bins, 1) = (128, 1)\n",
    "        right_mel = mel_low_freq + (bin + 2.0) * mel_freq_delta  # size(num_bins, 1) = (128, 1)\n",
    "\n",
    "        center_freqs = inverse_mel_scale(center_mel)  # size (num_bins) = (128)\n",
    "        mel = mel_scale(fft_bin_width * torch.arange(num_fft_bins)).unsqueeze(0) # size(1, num_fft_bins) = size (1, 256)\n",
    "\n",
    "        # size (num_bins, num_fft_bins)\n",
    "        up_slope = (mel - left_mel) / (center_mel - left_mel) # size (128, 256)\n",
    "        down_slope = (right_mel - mel) / (right_mel - center_mel) # size (128, 256)\n",
    "\n",
    "        # left_mel < center_mel < right_mel so we can min the two slopes and clamp negative values\n",
    "        bins = torch.max(torch.zeros(1), torch.min(up_slope, down_slope)) # size (128, 256)\n",
    "        \n",
    "        return bins\n",
    "\n",
    "    # size (num_mel_bins, padded_window_size // 2)\n",
    "    mel_energies = get_mel_banks(128) # torch.Size([128, 256])\n",
    "    mel_energies = mel_energies.to(device=device, dtype=dtype) # torch.Size([128, 256])\n",
    "\n",
    "    # pad right column with zeros and add dimension, size (num_mel_bins, padded_window_size // 2 + 1)\n",
    "    mel_energies = torch.nn.functional.pad(mel_energies, (0, 1), mode=\"constant\", value=0) # torch.Size([128, 257])\n",
    "\n",
    "    # sum with mel fiterbanks over the power spectrum, size (m, num_mel_bins)\n",
    "    mel_energies = torch.mm(spectrum, mel_energies.T) # (998, 256 + 1) x (257, 128) = torch.Size([998, 128])\n",
    "    \n",
    "    # avoid log of zero (which should be prevented anyway by dithering)\n",
    "    mel_energies = torch.max(mel_energies, torch.tensor(torch.finfo(torch.float).eps).to(device=device, dtype=dtype)).log() # torch.Size([998, 128])\n",
    "\n",
    "    return mel_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "# Data type of audio tensor: torch.float32\n",
    "# Shape of audio tensor: torch.Size([160000])\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor_batch = audio_tensor.unsqueeze(0)\n",
    "# Shape of audio tensor: torch.Size([1, 160000])\n",
    "\n",
    "\n",
    "# Call the fbank_own function\n",
    "fbank_features_own = fbank_own(\n",
    "    waveform=audio_tensor_batch,\n",
    ")\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features_torch = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor_batch, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "# Shape of fbank features: torch.Size([998, 128])\n",
    "\n",
    "# Assuming you have already read the audio file into `audio_signal` and it's a 1D array\n",
    "# Initial shape of audio signal: (160000,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot both\n",
    "\n",
    "# plot the fbank features from the own implementation\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_own.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Own fbank Features')\n",
    "\n",
    "# plot the fbank features\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_torch.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Kaldi fbank Features')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# compare the 90th frame\n",
    "plt.plot(fbank_features_torch[90, :], label='Kaldi')\n",
    "plt.plot(fbank_features_own[90, :], label='Own')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to raw python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fbank_own(waveform):\n",
    "    # Waveform is now a 1D numpy array: shape = [160000]\n",
    "    \n",
    "    def get_window(waveform):\n",
    "        # Stride and size configuration to simulate torch's as_strided\n",
    "        # Assuming waveform length is n = 160000\n",
    "        n = waveform.shape[0]\n",
    "        stride = 160\n",
    "        window_length = 400\n",
    "        number_of_frames = (n - window_length) // stride + 1  # 998 frames\n",
    "        \n",
    "        # Create an array of indices for each strided window\n",
    "        indices = np.lib.stride_tricks.as_strided(\n",
    "            np.arange(n),\n",
    "            shape=(number_of_frames, window_length),\n",
    "            strides=(waveform.strides[0]*stride, waveform.strides[0])\n",
    "        )\n",
    "        strided_input = waveform[indices]  # shape = [998, 400]\n",
    "        \n",
    "        # Subtract each row/frame by its mean\n",
    "        row_means = np.mean(strided_input, axis=1, keepdims=True)  # shape = [998, 1]\n",
    "        strided_input -= row_means  # shape = [998, 400]\n",
    "        \n",
    "        # Pre-emphasis filtering\n",
    "        preemphasis_coefficient = 0.97\n",
    "        strided_input[:, 1:] -= preemphasis_coefficient * strided_input[:, :-1]\n",
    "        \n",
    "        # Apply Hanning window to each row/frame\n",
    "        window_function = np.hanning(window_length)  # shape = [400]\n",
    "        strided_input *= window_function  # shape = [998, 400]\n",
    "        \n",
    "        # Zero-pad each frame to the next power of two for FFT\n",
    "        padded_window_size = 512\n",
    "        strided_input = np.pad(strided_input, ((0, 0), (0, padded_window_size - window_length)), 'constant')  # shape = [998, 512]\n",
    "        \n",
    "        return strided_input  # shape = [998, 512]\n",
    "    \n",
    "    strided_input = get_window(waveform)  # shape = [998, 512]\n",
    "    \n",
    "    # Compute the power spectrum\n",
    "    spectrum = np.abs(np.fft.rfft(strided_input, n=512))**2  # shape = [998, 257]\n",
    "\n",
    "    def get_mel_banks(num_bins):\n",
    "        num_fft_bins = 256  # Half the padded window size\n",
    "        nyquist_freq = 8000.0\n",
    "        low_freq = 20.0\n",
    "        high_freq = nyquist_freq\n",
    "        fft_bin_width = nyquist_freq / num_fft_bins\n",
    "        \n",
    "        # Mel scale conversion\n",
    "        def mel_scale(freq):\n",
    "            return 1127.0 * np.log(1.0 + freq / 700.0)\n",
    "        \n",
    "        def inverse_mel_scale(mel_freq):\n",
    "            return 700.0 * (np.exp(mel_freq / 1127.0) - 1.0)\n",
    "        \n",
    "        mel_low_freq = mel_scale(low_freq)\n",
    "        mel_high_freq = mel_scale(high_freq)\n",
    "        mel_freq_delta = (mel_high_freq - mel_low_freq) / (num_bins + 1)\n",
    "        \n",
    "        mel_bins = np.zeros((num_bins, num_fft_bins + 1))\n",
    "        \n",
    "        for i in range(num_bins):\n",
    "            left_mel = mel_low_freq + i * mel_freq_delta\n",
    "            center_mel = left_mel + mel_freq_delta\n",
    "            right_mel = center_mel + mel_freq_delta\n",
    "            \n",
    "            for j in range(num_fft_bins + 1):\n",
    "                freq = j * fft_bin_width\n",
    "                mel_freq = mel_scale(freq)\n",
    "                \n",
    "                if left_mel < mel_freq < right_mel:\n",
    "                    if mel_freq <= center_mel:\n",
    "                        mel_bins[i, j] = (mel_freq - left_mel) / (center_mel - left_mel)\n",
    "                    else:\n",
    "                        mel_bins[i, j] = (right_mel - mel_freq) / (right_mel - center_mel)\n",
    "        \n",
    "        return mel_bins  # shape = [128, 257]\n",
    "\n",
    "    mel_energies = get_mel_banks(128)  # shape = [128, 257]\n",
    "    \n",
    "    # Filter bank energies\n",
    "    filter_bank_energies = np.dot(spectrum, mel_energies.T)  # shape = [998, 128]\n",
    "    \n",
    "    # Log energies\n",
    "    filter_bank_energies = np.log(np.maximum(filter_bank_energies, 1.19209e-07))\n",
    "    \n",
    "    return filter_bank_energies  # shape = [998, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# Verify the file path is correct\n",
    "file_path = '/home/bosfab01/SpeakerVerificationBA/data/preprocessed/0a4b5c0f-facc-4d3b-8a41-bc9148d62d95/0_segment_0.flac'\n",
    "try:\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create time array for plotting\n",
    "time = np.arange(len(audio_signal)) / sample_rate\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "audio_tensor = torch.from_numpy(audio_signal)\n",
    "\n",
    "# Ensure the tensor is in float32 format (required for most torchaudio operations)\n",
    "audio_tensor = audio_tensor.float()\n",
    "# Data type of audio tensor: torch.float32\n",
    "# Shape of audio tensor: torch.Size([160000])\n",
    "\n",
    "# If your array is not in batch x channels x time format, adjust accordingly\n",
    "# Assuming the audio signal is single-channel and not batched:\n",
    "audio_tensor_batch = audio_tensor.unsqueeze(0)\n",
    "# Shape of audio tensor: torch.Size([1, 160000])\n",
    "\n",
    "\n",
    "# Call the fbank_own function\n",
    "fbank_features_own = fbank_own(\n",
    "    waveform=audio_signal,\n",
    ")\n",
    "\n",
    "# Now call the fbank function\n",
    "fbank_features_torch = torchaudio.compliance.kaldi.fbank(\n",
    "    audio_tensor_batch, \n",
    "    sample_frequency=sample_rate, \n",
    "    htk_compat=True, \n",
    "    use_energy=False, \n",
    "    window_type='hanning', \n",
    "    num_mel_bins=128, \n",
    "    dither=0.0, \n",
    "    frame_shift=10\n",
    ")\n",
    "\n",
    "# Output the shape of the fbank features to confirm\n",
    "# Shape of fbank features: torch.Size([998, 128])\n",
    "\n",
    "# Assuming you have already read the audio file into `audio_signal` and it's a 1D array\n",
    "# Initial shape of audio signal: (160000,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot both\n",
    "\n",
    "# plot the fbank features from the own implementation\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_own.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Own fbank Features')\n",
    "\n",
    "# plot the fbank features\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "plt.imshow(fbank_features_torch.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Kaldi fbank Features')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# compare the 90th frame\n",
    "plt.plot(fbank_features_torch[90, :], label='Kaldi')\n",
    "plt.plot(fbank_features_own[90, :], label='Own')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.finfo(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
